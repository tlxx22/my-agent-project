"""
ä»ªè¡¨è¡¨æ ¼è§£æå·¥å…· - ä¸»è¡Œ/è¡¥å……è¡ŒæŠ˜å ç®—æ³•å®ç°
æ”¯æŒå¤æ‚çš„è¡¨æ ¼ç»“æ„ï¼šä¸»è¡Œ+å¤šä¸ªè¡¥å……è¡Œçš„çµæ´»ç»„åˆ
å¢å¼ºç‰ˆï¼šæ”¯æŒè¯†åˆ«åˆ†ç±»æ ‡é¢˜è¡Œå¹¶æŒ‰åŒºåŸŸåˆ†é…ä»ªè¡¨ç±»åˆ«
"""
import pandas as pd
import re
from typing import Union, Optional, Dict, Any, List, Tuple
import logging

logger = logging.getLogger(__name__)

def parse_quantity_field(quantity_str: Union[str, int, float]) -> int:
    """
    è§£ææ•°é‡å­—æ®µï¼Œå¤„ç†å„ç§æ ¼å¼å¹¶è¿”å›æ•´æ•°æ€»æ•°é‡
    
    æ”¯æŒçš„æ ¼å¼:
    - ç®€å•æ•°å­—: "5", 5, 5.0
    - ä¹˜æ³•è¡¨è¾¾å¼: "1Ã—2", "2X2", "3x1", "2*3"
    - å¸¦å•ä½: "5å°", "3ä¸ª"
    - å¤åˆè¡¨è¾¾å¼: "2Ã—3+1", "1+2Ã—3"
    
    Args:
        quantity_str: æ•°é‡å­—ç¬¦ä¸²æˆ–æ•°å­—
    
    Returns:
        è§£æåçš„æ•´æ•°æ•°é‡
    """
    if pd.isna(quantity_str):
        return 0
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    quantity_str = str(quantity_str).strip()
    
    if not quantity_str:
        return 0
    
    try:
        # ç§»é™¤å¸¸è§çš„å•ä½è¯
        units_to_remove = ['å°', 'ä¸ª', 'å¥—', 'ä»¶', 'åª', 'æ ¹', 'æ”¯', 'ç‰‡', 'å—']
        for unit in units_to_remove:
            quantity_str = quantity_str.replace(unit, '')
        
        # æ›¿æ¢å„ç§ä¹˜å·ä¸ºæ ‡å‡†æ ¼å¼
        quantity_str = re.sub(r'[Ã—Xx*]', '*', quantity_str)
        
        # ç§»é™¤ç©ºæ ¼
        quantity_str = quantity_str.replace(' ', '')
        
        # å¤„ç†ç®€å•æ•°å­—
        if re.match(r'^\d+(\.\d+)?$', quantity_str):
            return int(float(quantity_str))
        
        # å¤„ç†ä¹˜æ³•è¡¨è¾¾å¼ (å¦‚: 2*3, 1*2*3)
        if re.match(r'^\d+(\*\d+)+$', quantity_str):
            parts = quantity_str.split('*')
            result = 1
            for part in parts:
                result *= int(part)
            return result
        
        # å¤„ç†å¤åˆè¡¨è¾¾å¼ (å¦‚: 2*3+1, 1+2*3)
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«åŠ æ³•
        if '+' in quantity_str:
            # æŒ‰åŠ å·åˆ†å‰²
            terms = quantity_str.split('+')
            total = 0
            for term in terms:
                term = term.strip()
                if '*' in term:
                    # å¤„ç†ä¹˜æ³•é¡¹
                    parts = term.split('*')
                    product = 1
                    for part in parts:
                        product *= int(part)
                    total += product
                else:
                    # ç®€å•æ•°å­—é¡¹
                    total += int(term)
            return total
        
        # å¤„ç†å‡æ³•è¡¨è¾¾å¼
        if '-' in quantity_str and not quantity_str.startswith('-'):
            terms = quantity_str.split('-')
            total = 0
            for i, term in enumerate(terms):
                term = term.strip()
                if '*' in term:
                    parts = term.split('*')
                    product = 1
                    for part in parts:
                        product *= int(part)
                    if i == 0:
                        total += product
                    else:
                        total -= product
                else:
                    if i == 0:
                        total += int(term)
                    else:
                        total -= int(term)
            return max(0, total)  # ç¡®ä¿ä¸è¿”å›è´Ÿæ•°
        
        # å°è¯•ç›´æ¥è®¡ç®—è¡¨è¾¾å¼ (è°¨æ…ä½¿ç”¨eval)
        # ä»…å½“è¡¨è¾¾å¼åªåŒ…å«æ•°å­—ã€åŸºæœ¬è¿ç®—ç¬¦æ—¶æ‰ä½¿ç”¨
        if re.match(r'^[\d+\-*/().]+$', quantity_str):
            try:
                result = eval(quantity_str)
                return int(result) if result >= 0 else 0
            except:
                pass
        
        # å¦‚æœä»¥ä¸Šéƒ½ä¸åŒ¹é…ï¼Œå°è¯•æå–æ•°å­—
        numbers = re.findall(r'\d+', quantity_str)
        if numbers:
            # å–ç¬¬ä¸€ä¸ªæ•°å­—
            return int(numbers[0])
        
        # æ— æ³•è§£æï¼Œè¿”å›0
        logger.warning(f"æ— æ³•è§£ææ•°é‡å­—æ®µ: {quantity_str}")
        return 0
        
    except Exception as e:
        logger.error(f"è§£ææ•°é‡å­—æ®µæ—¶å‡ºé”™: {quantity_str}, é”™è¯¯: {str(e)}")
        return 0

def find_category_sections(df: pd.DataFrame) -> List[Tuple[str, int, int]]:
    """
    è¯†åˆ«è¡¨æ ¼ä¸­çš„åˆ†ç±»æ ‡é¢˜è¡Œï¼Œç²¾ç¡®åŒ¹é…åˆ†ç±»æ ¼å¼
    
    Args:
        df: åŸå§‹DataFrame
    
    Returns:
        List of (category_name, start_row, end_row)
    """
    sections = []
    
    logger.info("ğŸ” ç²¾ç¡®è¯†åˆ«è¡¨æ ¼åˆ†ç±»æ ‡é¢˜")
    
    for idx in range(len(df)):
        row_text = " ".join([str(val) for val in df.iloc[idx].values if pd.notna(val)])
        row_text_clean = row_text.strip()
        
        if not row_text_clean:
            continue
            
        logger.debug(f"æ£€æŸ¥ç¬¬{idx+1}è¡Œ: {row_text_clean}")
        
        # ç²¾ç¡®åŒ¹é…ä¸­æ–‡æ•°å­—åˆ†ç±»æ ‡é¢˜ï¼ˆæ”¯æŒå¤šç§æ ‡ç‚¹ç¬¦å·ï¼šã€ï¼šã€.ç­‰ï¼‰
        chinese_num_patterns = [
            (r'^ä¸€[ï¼š:\s\.\,\ã€\.]+(.+)', 'ä¸€'),
            (r'^äºŒ[ï¼š:\s\.\,\ã€\.]+(.+)', 'äºŒ'),
            (r'^ä¸‰[ï¼š:\s\.\,\ã€\.]+(.+)', 'ä¸‰'),
            (r'^å››[ï¼š:\s\.\,\ã€\.]+(.+)', 'å››'),
            (r'^äº”[ï¼š:\s\.\,\ã€\.]+(.+)', 'äº”'),
            (r'^å…­[ï¼š:\s\.\,\ã€\.]+(.+)', 'å…­'),
            (r'^ä¸ƒ[ï¼š:\s\.\,\ã€\.]+(.+)', 'ä¸ƒ'),
            (r'^å…«[ï¼š:\s\.\,\ã€\.]+(.+)', 'å…«'),
            (r'^ä¹[ï¼š:\s\.\,\ã€\.]+(.+)', 'ä¹'),
            (r'^å[ï¼š:\s\.\,\ã€\.]+(.+)', 'å')
        ]
        
        # åŒ¹é…ç‰¹æ®Šæƒ…å†µï¼šç´§è·Ÿåˆ†ç±»åçš„æƒ…å†µ
        special_patterns = [
            (r'^ä¸€ã€(.+)', 'ä¸€'),
            (r'^äºŒã€(.+)', 'äºŒ'),
            (r'^ä¸‰ã€(.+)', 'ä¸‰'),
            (r'^å››ã€(.+)', 'å››'),
            (r'^äº”ã€(.+)', 'äº”'),
            (r'^å…­ã€(.+)', 'å…­'),
            (r'^ä¸ƒã€(.+)', 'ä¸ƒ'),
            (r'^å…«ã€(.+)', 'å…«'),
            (r'^ä¹ã€(.+)', 'ä¹'),
            (r'^åã€(.+)', 'å')
        ]
        
        # ç»„åˆæ‰€æœ‰æ¨¡å¼
        all_patterns = chinese_num_patterns + special_patterns
        
        found = False
        for pattern, num_char in all_patterns:
            match = re.search(pattern, row_text_clean)
            if match:
                # æå–åˆ†ç±»åç§°
                category_name = match.group(1).strip().split()[0] if match.group(1).strip().split() else 'æœªçŸ¥åˆ†ç±»'
                
                # è¿›ä¸€æ­¥éªŒè¯ï¼šç¡®ä¿è¿™çœŸçš„æ˜¯åˆ†ç±»æ ‡é¢˜
                # 1. åˆ†ç±»åç§°ä¸èƒ½å¤ªé•¿
                if len(category_name) > 50:  # æ”¾å®½é•¿åº¦é™åˆ¶
                    continue
                    
                # 2. ä¸èƒ½åŒ…å«æ˜æ˜¾çš„æŠ€æœ¯å‚æ•°ï¼ˆæ”¾å®½é™åˆ¶ï¼‰
                tech_params = ['MPa', 'çº§', 'â„ƒ', 'Â°C', 'mm', 'Ğ¤', 'DN', 'PN']
                if any(param in category_name for param in tech_params):
                    continue
                
                # 3. ä¸èƒ½åŒ…å«å¤æ‚ç¬¦å·ï¼ˆå…è®¸æ‹¬å·ï¼Œå› ä¸ºåˆ†ç±»ä¸­å¯èƒ½åŒ…å«ï¼‰
                complex_chars = ['=']
                if any(char in category_name for char in complex_chars):
                    continue
                
                # 4. åº”è¯¥åŒ…å«ä»ªè¡¨ç›¸å…³è¯æ±‡ï¼ˆæ”¾å®½è¦æ±‚ï¼Œæ”¯æŒæ›´å¤šç±»å‹ï¼‰
                instrument_keywords = ['ä»ªè¡¨', 'è®¾å¤‡', 'ç³»ç»Ÿ', 'æ§åˆ¶', 'è£…ç½®', 'å˜é€å™¨', 'ä¼ æ„Ÿå™¨', 'è®¡', 'è¡¨', 'é˜€', 'ç®±', 'é—¨', 'é£é—¨', 'å¤´', 'åˆ†æ']
                if not any(keyword in category_name for keyword in instrument_keywords):
                    # å¯¹äºæ˜ç¡®çš„æ•°å­—åˆ†ç±»ï¼Œå³ä½¿ä¸åŒ…å«å…³é”®è¯ä¹Ÿå¯èƒ½æ˜¯æœ‰æ•ˆåˆ†ç±»
                    logger.warning(f"åˆ†ç±» '{category_name}' ä¸åŒ…å«ä»ªè¡¨å…³é”®è¯ï¼Œä½†ç»§ç»­å¤„ç†")
                
                sections.append((category_name, idx, -1))  # -1è¡¨ç¤ºç»“æŸè¡Œå¾…ç¡®å®š
                logger.info(f"âœ… æ‰¾åˆ°åˆ†ç±»æ ‡é¢˜: {num_char}ï¼š{category_name} åœ¨ç¬¬{idx+1}è¡Œ")
                found = True
                break
        
        if found:
            continue  # æ‰¾åˆ°åŒ¹é…åè·³è¿‡å…¶ä»–æ£€æŸ¥
    
    # ç¡®å®šæ¯ä¸ªåˆ†ç±»çš„ç»“æŸè¡Œ
    for i in range(len(sections)):
        current_start = sections[i][1]
        if i < len(sections) - 1:
            next_start = sections[i+1][1]
            sections[i] = (sections[i][0], current_start, next_start - 1)
        else:
            # æœ€åä¸€ä¸ªåˆ†ç±»åˆ°è¡¨æ ¼ç»“æŸ
            sections[i] = (sections[i][0], current_start, len(df) - 1)
    
    logger.info(f"ğŸ¯ æˆåŠŸè¯†åˆ«åˆ° {len(sections)} ä¸ªåˆ†ç±»åŒºåŸŸ")
    for category_name, start_row, end_row in sections:
        logger.info(f"   åˆ†ç±»: {category_name}, èµ·å§‹è¡Œ: {start_row+1}, ç»“æŸè¡Œ: {end_row+1}")
    
    return sections

def find_header_row(df: pd.DataFrame) -> int:
    """
    å®šä½æ ‡é¢˜è¡Œï¼šåŒæ—¶åŒ…å«"ä½å·"+"æµ‹ç‚¹"æˆ–"å‹å·"
    
    Args:
        df: åŸå§‹DataFrame
    
    Returns:
        æ ‡é¢˜è¡Œç´¢å¼•ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›-1
    """
    key_patterns = [
        ["ä½å·", "å‹å·"],
        ["ä½å·", "æµ‹ç‚¹"], 
        ["tag", "model"],
        ["å·¥ä½å·", "è®¾å¤‡å‹å·"],
        ["ä»ªè¡¨ä½å·", "ä»ªè¡¨å‹å·"]
    ]
    
    for idx in range(min(10, len(df))):  # åªæ£€æŸ¥å‰10è¡Œ
        row_str = ' '.join(str(cell).lower() for cell in df.iloc[idx] if pd.notna(cell))
        
        for patterns in key_patterns:
            if all(pattern.lower() in row_str for pattern in patterns):
                logger.info(f"æ‰¾åˆ°æ ‡é¢˜è¡Œåœ¨ç¬¬ {idx+1} è¡Œ: {patterns}")
                return idx
    
    logger.warning("æœªæ‰¾åˆ°åŒ…å«ä½å·+å‹å·çš„æ ‡é¢˜è¡Œ")
    return -1

def assign_categories_by_sections(df: pd.DataFrame, category_sections: List[Tuple[str, int, int]]) -> pd.DataFrame:
    """
    æ ¹æ®åˆ†ç±»åŒºåŸŸä¸ºæ¯è¡Œæ•°æ®åˆ†é…ç±»åˆ«
    
    Args:
        df: è§£æåçš„ä»ªè¡¨æ•°æ®DataFrame
        category_sections: åˆ†ç±»åŒºåŸŸåˆ—è¡¨
    
    Returns:
        æ·»åŠ äº†'ä»ªè¡¨ç±»å‹'åˆ—çš„DataFrame
    """
    df = df.copy()
    df['ä»ªè¡¨ç±»å‹'] = "æœªåˆ†ç±»"
    df['åŸå§‹è¡Œå·'] = range(len(df))
    
    # ä¸ºæ¯è¡Œåˆ†é…ç±»åˆ«
    for category_name, start_row, end_row in category_sections:
        # æ‰¾åˆ°å±äºæ­¤åŒºåŸŸçš„æ•°æ®è¡Œ
        for idx, row in df.iterrows():
            original_row = row.get('åŸå§‹è¡Œå·', idx)
            if start_row <= original_row <= end_row:
                df.at[idx, 'ä»ªè¡¨ç±»å‹'] = category_name
    
    # åˆ é™¤ä¸´æ—¶çš„åŸå§‹è¡Œå·åˆ—
    df = df.drop('åŸå§‹è¡Œå·', axis=1)
    
    return df

def extract_and_parse_instrument_table(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ ¸å¿ƒå‡½æ•°ï¼šå®ç°ä¸»è¡Œ/è¡¥å……è¡ŒæŠ˜å ç®—æ³• + åˆ†ç±»åŒºåŸŸè¯†åˆ«
    
    è§„åˆ™æ¡†æ¶ï¼š
    a. è¯†åˆ«åˆ†ç±»æ ‡é¢˜è¡Œå’ŒåŒºåŸŸè¾¹ç•Œ
    b. å®šä½æ•°æ®æ ‡é¢˜è¡Œ - åŒæ—¶åŒ…å«"ä½å·"+"å‹å·"  
    c. åˆ‡åˆ†æ•°æ®åŒº - æ ‡é¢˜è¡Œä»¥ä¸‹
    d. è¯†åˆ«"ä¸»è¡Œ" - ä½å·â‰ ç©ºä¸”å‹å·â‰ ç©º
    e. å‘ä¸‹æ”¶é›†è¡¥å……è¡Œ - åœ¨ä¸‹ä¸€æ¡ä¸»è¡Œå‡ºç°å‰ï¼ŒæŠŠè¡Œå·ç´¯è¿›åˆ°å½“å‰ä¸»è¡Œçš„"æ¡¶"é‡Œ
    f. ä¸»è¡Œforward-fillç©ºåˆ— - ä½å·forward-fillï¼›å‹å·åªä¿ç•™ä¸»è¡Œçš„å€¼
    g. è¡¥å……åˆ—æŠ˜å  - å¯¹åŒä¸€group_idï¼šå­—ç¬¦ä¸²åˆ—â†’";".joinï¼Œæ•°é‡åˆ—â†’ç¬¬ä¸€æ¡éç©º
    h. æ ¹æ®åˆ†ç±»åŒºåŸŸåˆ†é…ä»ªè¡¨ç±»å‹
    i. åˆ é™¤ç©ºç™½æ¡¶ - ä¸»é”®åˆ—ä¸ºç©ºçš„è¡Œåˆ é™¤
    
    Args:
        df: åŸå§‹Excel DataFrameï¼ˆå¯èƒ½æœ‰headeræˆ–æ²¡æœ‰headerï¼‰
    
    Returns:
        æŠ˜å åçš„æ ‡å‡†åŒ–DataFrameï¼ŒåŒ…å«'ä»ªè¡¨ç±»å‹'åˆ—
    """
    if df.empty:
        logger.warning("è¾“å…¥DataFrameä¸ºç©º")
        return pd.DataFrame()
    
    # ä¿å­˜åŸå§‹è¡Œç´¢å¼•ç”¨äºç±»åˆ«åˆ†é…
    df_original = df.copy()
    df_original['original_index'] = range(len(df_original))
    
    # Step a: è¯†åˆ«åˆ†ç±»æ ‡é¢˜è¡Œå’ŒåŒºåŸŸè¾¹ç•Œ
    category_sections = find_category_sections(df_original)
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰åˆé€‚çš„åˆ—åï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
    if 'ä½å·' in df.columns and 'å‹å·' in df.columns:
        logger.info("æ£€æµ‹åˆ°å·²è®¾ç½®åˆ—åï¼Œè·³è¿‡æ ‡é¢˜è¡ŒæŸ¥æ‰¾")
        df_data = df.copy()
        idx_header = -1  # è¡¨ç¤ºæ— éœ€å¤„ç†æ ‡é¢˜è¡Œ
    else:
        # Step b: å®šä½æ•°æ®æ ‡é¢˜è¡Œ
        idx_header = find_header_row(df)
        if idx_header == -1:
            logger.error("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„æ ‡é¢˜è¡Œ")
            return pd.DataFrame()
        
        # Step c: åˆ‡åˆ†æ•°æ®åŒº 
        df_data = df.iloc[idx_header+1:].copy()
        df_data.columns = df.iloc[idx_header]  # ä½¿ç”¨æ ‡é¢˜è¡Œä½œä¸ºåˆ—å
        df_data = df_data.reset_index(drop=True)
    
    if df_data.empty:
        logger.warning("æ ‡é¢˜è¡Œåæ— æ•°æ®")
        return pd.DataFrame()
    
    # ä¿å­˜åŸå§‹ç´¢å¼•æ˜ å°„
    if idx_header != -1:
        original_indices = list(range(idx_header + 1, idx_header + 1 + len(df_data)))
    else:
        original_indices = list(range(len(df_data)))
    
    df_data['_original_row'] = original_indices
    
    # æ™ºèƒ½è¯†åˆ«åˆ—åæ˜ å°„
    column_mapping = {}
    for col in df_data.columns:
        col_str = str(col).lower().strip()
        if any(keyword in col_str for keyword in ['ä½å·', 'tag', 'å·¥ä½å·']):
            column_mapping['ä½å·'] = col
        elif any(keyword in col_str for keyword in ['å‹å·', 'model', 'è®¾å¤‡å‹å·', 'ä»ªè¡¨å‹å·']):
            column_mapping['å‹å·'] = col
        elif any(keyword in col_str for keyword in ['æ•°é‡', 'qty', 'quantity', 'å°æ•°']):
            column_mapping['æ•°é‡'] = col
        elif any(keyword in col_str for keyword in ['è§„æ ¼', 'spec', 'å‹å¼', 'æè¿°']):
            column_mapping['è§„æ ¼'] = col
        elif any(keyword in col_str for keyword in ['å¤‡æ³¨', 'remark', 'è¯´æ˜', 'note']):
            column_mapping['å¤‡æ³¨'] = col
    
    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
    if 'ä½å·' not in column_mapping or 'å‹å·' not in column_mapping:
        logger.error(f"ç¼ºå°‘å¿…è¦çš„åˆ—ï¼šä½å·æˆ–å‹å·ã€‚å½“å‰åˆ—ï¼š{list(df_data.columns)}")
        return pd.DataFrame()
    
    logger.info(f"åˆ—æ˜ å°„: {column_mapping}")
    
    # Step d: è¯†åˆ«"ä¸»è¡Œ" - ä½å·â‰ ç©ºä¸”å‹å·â‰ ç©º
    tag_col = column_mapping['ä½å·']
    model_col = column_mapping['å‹å·']
    
    # æ¸…ç†ç©ºå€¼å’Œå­—ç¬¦ä¸²
    df_data[tag_col] = df_data[tag_col].astype(str).str.strip()
    df_data[model_col] = df_data[model_col].astype(str).str.strip()
    
    # ä¸»è¡Œmaskï¼šè¯†åˆ«çœŸæ­£çš„ä»ªè¡¨è¡Œï¼Œæ’é™¤åˆ†ç±»æ ‡é¢˜å’Œè¯´æ˜è¡Œ
    # 1. ä½å·ä¸ä¸ºç©º
    # 2. æ’é™¤çº¯æ•°å­—ç¼–å·ï¼ˆå¦‚"ä¸€ï¼š", "äºŒï¼š"ç­‰åˆ†ç±»æ ‡é¢˜ï¼‰
    # 3. æ’é™¤è¯´æ˜è¡Œ
    def is_valid_instrument_row(tag_value):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä»ªè¡¨è¡Œ"""
        if not tag_value or str(tag_value).strip() in ['', 'nan', 'None']:
            return False
        
        tag_str = str(tag_value).strip()
        
        # æ’é™¤åˆ†ç±»æ ‡é¢˜è¡Œï¼ˆå¦‚"ä¸€ï¼š"ã€"äºŒï¼š"ã€"ä¸‰ï¼š"ã€"ä¸€ã€"ç­‰ï¼‰
        if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d+][:ï¼š\s\.\,\ã€\.]*$', tag_str):
            return False
        
        # æ’é™¤å®Œæ•´çš„åˆ†ç±»æ ‡é¢˜è¡Œï¼ˆå¦‚"ä¸€ã€æ¸©åº¦ä»ªè¡¨"ï¼‰
        if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d+][ï¼š:\s\.\,\ã€\.ã€]+.+', tag_str):
            return False
        
        # æ’é™¤è¯´æ˜è¡Œ
        if tag_str.startswith('è¯´æ˜'):
            return False
        
        # ä»ªè¡¨ä½å·é€šå¸¸åŒ…å«å­—æ¯å’Œæ•°å­—çš„ç»„åˆ
        if re.match(r'^[A-Z]+[A-Z0-9\-]*\d+$', tag_str, re.IGNORECASE):
            return True
        
        # æˆ–è€…æ˜¯ç‰¹æ®Šæ ¼å¼çš„ä½å·ï¼ˆå¦‚MB-101ç­‰ï¼‰
        if re.match(r'^[A-Z]{1,4}-\d+$', tag_str, re.IGNORECASE):
            return True
        
        return False
    
    mask_main = df_data[tag_col].apply(is_valid_instrument_row)
    
    if not mask_main.any():
        logger.error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¸»è¡Œï¼ˆä½å·+å‹å·åŒæ—¶éç©ºï¼‰")
        return pd.DataFrame()
    
    logger.info(f"æ‰¾åˆ° {mask_main.sum()} ä¸ªä¸»è¡Œ")
    
    # Step e: å‘ä¸‹æ”¶é›†è¡¥å……è¡Œ - group_idåˆ†ç»„
    df_data["group_id"] = mask_main.cumsum()
    
    # è¿‡æ»¤æ‰group_idä¸º0çš„è¡Œï¼ˆæ ‡é¢˜è¡Œä¹‹å‰çš„åƒåœ¾è¡Œï¼‰
    df_data = df_data[df_data["group_id"] > 0].copy()
    
    if df_data.empty:
        logger.error("è¿‡æ»¤åæ— æœ‰æ•ˆæ•°æ®")
        return pd.DataFrame()
    
    # Step f: ä¸»è¡Œforward-fillç©ºåˆ—
    # ä½å·forward-fillï¼ˆæ¯ä¸ªç»„å†…çš„è¡¥å……è¡Œç»§æ‰¿ä¸»è¡Œçš„ä½å·ï¼‰
    df_data[tag_col] = df_data.groupby("group_id")[tag_col].ffill()
    
    # å‹å·åªä¿ç•™ä¸»è¡Œçš„å€¼ï¼ˆè¡¥å……è¡Œçš„å‹å·æ¸…ç©ºï¼‰
    # é‡å»ºmask_mainä»¥åŒ¹é…æ–°çš„ç´¢å¼•
    mask_main_aligned = mask_main.reindex(df_data.index, fill_value=False)
    
    # æ‰¾åˆ°æ¯ä¸ªç»„çš„ä¸»è¡Œä½ç½®å¹¶è®°å½•å‹å·å€¼
    model_values = {}
    for group_id in df_data["group_id"].unique():
        group_data = df_data[df_data["group_id"] == group_id]
        group_main_mask = mask_main_aligned[group_data.index]
        group_main_rows = group_data[group_main_mask]
        
        if not group_main_rows.empty:
            model_values[group_id] = group_main_rows.iloc[0][model_col]
        else:
            model_values[group_id] = ""
    
    # é‡å»ºå‹å·åˆ—ï¼šä¿æŒåŸå§‹å‹å·å€¼ï¼ŒåŒ…æ‹¬ç©ºå€¼
    def get_model_value(row):
        if mask_main_aligned.loc[row.name]:
            original_model = model_values.get(row["group_id"], "")
            # å¯¹äºç”µåŠ¨é—¨æ§åˆ¶ç®±ç­‰ç‰¹æ®Šæƒ…å†µï¼Œä½¿ç”¨æè¿°æ€§åç§°
            if original_model in ["", "nan", None] or pd.isna(original_model):
                tag_value = str(row.get(tag_col, ""))
                if tag_value.startswith("MB-"):
                    # ç”µåŠ¨é—¨æ§åˆ¶ç®±ä½¿ç”¨æµ‹ç‚¹åç§°ä½œä¸ºå‹å·
                    test_point_col = None
                    for col in df_data.columns:
                        if any(keyword in str(col).lower() for keyword in ['æµ‹ç‚¹', 'åç§°', 'è¯´æ˜']):
                            test_point_col = col
                            break
                    if test_point_col and pd.notna(row.get(test_point_col)):
                        return str(row.get(test_point_col, "")).strip()
                    else:
                        return "ç”µåŠ¨é—¨æ§åˆ¶ç®±"  # é»˜è®¤å‹å·
                elif tag_value.startswith(("CV-", "FV-", "HV-", "LV-", "PV-", "TV-")):
                    return "æ§åˆ¶é˜€"  # é»˜è®¤å‹å·
                else:
                    # å¯¹äºå…¶ä»–æƒ…å†µï¼Œä¿æŒå‹å·ä¸ºç©º
                    return ""
            return original_model
        else:
            return ""
    
    df_data[model_col] = df_data.apply(get_model_value, axis=1)
    
    # Step g: è¡¥å……åˆ—æŠ˜å 
    # å‡†å¤‡èšåˆå‡½æ•°
    def safe_join(series):
        """å®‰å…¨åœ°è¿æ¥å­—ç¬¦ä¸²ï¼Œè¿‡æ»¤ç©ºå€¼"""
        valid_values = [str(v).strip() for v in series if pd.notna(v) and str(v).strip() != '' and str(v).strip() != 'nan']
        return "; ".join(unique_preserve_order(valid_values)) if valid_values else ""
    
    def first_valid(series):
        """è¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼"""
        for v in series:
            if pd.notna(v) and str(v).strip() != '' and str(v).strip() != 'nan':
                return v
        return None
    
    def sum_quantities(series):
        """æ±‚å’Œæ•°é‡ï¼Œä½¿ç”¨è§£æå‡½æ•°"""
        total = 0
        for v in series:
            if pd.notna(v):
                parsed = parse_quantity_field(v)
                total += parsed
        return total if total > 0 else 1  # é»˜è®¤æœ€å°‘ä¸º1
    
    def first_original_row(series):
        """è·å–ç»„å†…ç¬¬ä¸€ä¸ªåŸå§‹è¡Œå·ï¼Œç”¨äºç±»åˆ«åˆ†é…"""
        return series.iloc[0] if len(series) > 0 else -1
    
    # æ„å»ºèšåˆå­—å…¸
    agg_dict = {
        tag_col: 'first',  # ä½å·å–ç¬¬ä¸€ä¸ªï¼ˆå·²ç»forward-fillè¿‡ï¼‰
        model_col: first_valid,  # å‹å·å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼ï¼ˆä¸»è¡Œçš„å€¼ï¼‰
        '_original_row': first_original_row  # ä¿å­˜åŸå§‹è¡Œå·ç”¨äºç±»åˆ«åˆ†é…
    }
    
    # å¤„ç†å…¶ä»–åˆ—
    for col_name, col in column_mapping.items():
        if col_name in ['ä½å·', 'å‹å·']:
            continue
        elif col_name == 'æ•°é‡':
            agg_dict[col] = sum_quantities
        else:
            agg_dict[col] = safe_join
    
    # å¯¹äºæœªæ˜ å°„çš„åˆ—ï¼Œä½¿ç”¨é»˜è®¤èšåˆç­–ç•¥
    for col in df_data.columns:
        if col not in agg_dict and col not in ["group_id", "_original_row"]:
            # åˆ¤æ–­åˆ—çš„ç±»å‹
            if df_data[col].dtype in ['int64', 'float64'] or col_name_suggests_numeric(col):
                agg_dict[col] = sum_quantities
            else:
                agg_dict[col] = safe_join
    
    # æŒ‰group_idåˆ†ç»„èšåˆ
    df_final = df_data.groupby("group_id").agg(agg_dict).reset_index(drop=True)
    
    # Step h: æ ¹æ®åˆ†ç±»åŒºåŸŸåˆ†é…ä»ªè¡¨ç±»å‹
    df_final['ä»ªè¡¨ç±»å‹'] = "æœªåˆ†ç±»"
    
    if category_sections:
        # æƒ…å†µ1ï¼šè¡¨æ ¼æœ‰æ˜ç¡®åˆ†ç±»ï¼ŒæŒ‰åˆ†ç±»åŒºåŸŸåˆ†é…
        logger.info("ğŸ“‹ è¡¨æ ¼æœ‰æ˜ç¡®åˆ†ç±»ï¼Œä½¿ç”¨åˆ†ç±»åŒºåŸŸåˆ†é…")
        for category_name, start_row, end_row in category_sections:
            # æ‰¾åˆ°å±äºæ­¤åŒºåŸŸçš„æ•°æ®è¡Œ
            mask_in_section = (df_final['_original_row'] >= start_row) & (df_final['_original_row'] <= end_row)
            df_final.loc[mask_in_section, 'ä»ªè¡¨ç±»å‹'] = category_name
            logger.info(f"åˆ†é… {mask_in_section.sum()} è¡Œåˆ°ç±»åˆ«: {category_name}")
    else:
        # æƒ…å†µ2ï¼šè¡¨æ ¼æ²¡æœ‰åˆ†ç±»ï¼Œä½¿ç”¨LLMåˆ¤æ–­ä»ªè¡¨ç±»å‹
        logger.info("ğŸ¤– è¡¨æ ¼æ²¡æœ‰æ˜ç¡®åˆ†ç±»ï¼Œä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­ä»ªè¡¨ç±»å‹")
        df_final = _classify_instruments_with_llm(df_final, tag_col, model_col)
    
    # åˆ é™¤ä¸´æ—¶åˆ—
    df_final = df_final.drop('_original_row', axis=1)
    
    # Step i: åˆ é™¤ç©ºç™½æ¡¶ - åªè¦æ±‚ä½å·ä¸ä¸ºç©º
    # å…è®¸å‹å·ä¸ºç©ºï¼ˆå¦‚åŒè‰²æ°´ä½è®¡ç­‰æ²¡æœ‰æ ‡å‡†å‹å·çš„ä»ªè¡¨ï¼‰
    df_final = df_final[
        (df_final[tag_col].notna()) & 
        (df_final[tag_col].astype(str).str.strip() != '') &
        (df_final[tag_col].astype(str).str.strip() != 'nan')
    ].copy()
    
    # æ ‡å‡†åŒ–åˆ—å - ç¡®ä¿æ ¼å¼ä¸€è‡´æ€§
    standard_columns = {'ä½å·': tag_col, 'å‹å·': model_col}
    if 'æ•°é‡' in column_mapping:
        standard_columns['æ•°é‡'] = column_mapping['æ•°é‡']
    if 'è§„æ ¼' in column_mapping:
        standard_columns['è§„æ ¼'] = column_mapping['è§„æ ¼']  
    if 'å¤‡æ³¨' in column_mapping:
        standard_columns['å¤‡æ³¨'] = column_mapping['å¤‡æ³¨']
    
    # é‡å‘½åä¸ºæ ‡å‡†åˆ—å
    rename_dict = {v: k for k, v in standard_columns.items()}
    df_final = df_final.rename(columns=rename_dict)
    
    # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨ - æ— è®ºæœ‰åˆ†ç±»è¿˜æ˜¯æ— åˆ†ç±»ï¼Œæ ¼å¼å®Œå…¨ä¸€è‡´
    required_cols = ['ä½å·', 'å‹å·', 'æ•°é‡', 'è§„æ ¼', 'å¤‡æ³¨', 'ä»ªè¡¨ç±»å‹']
    for col in required_cols:
        if col not in df_final.columns:
            if col == 'æ•°é‡':
                df_final[col] = 1
            elif col == 'ä»ªè¡¨ç±»å‹':
                df_final[col] = "æœªåˆ†ç±»"
            else:
                df_final[col] = ""
    
    # é€‰æ‹©æœ€ç»ˆåˆ— - ç¡®ä¿ä¸¤ç§æƒ…å†µæ ¼å¼å®Œå…¨ä¸€è‡´
    final_cols = ['ä½å·', 'å‹å·', 'æ•°é‡', 'è§„æ ¼', 'å¤‡æ³¨', 'ä»ªè¡¨ç±»å‹']
    df_final = df_final[final_cols].copy()
    
    logger.info(f"âœ… æˆåŠŸè§£æ {len(df_final)} è¡Œä»ªè¡¨æ•°æ®")
    logger.info(f"ğŸ“Š åŸå§‹è¡¨æ ¼è¡Œæ•°: {len(df_original)} è¡Œï¼ˆåŒ…å«æ‰€æœ‰è¡Œå’Œç©ºè¡Œï¼‰")
    logger.info(f"ğŸ” æœ‰æ•ˆä»ªè¡¨æ•°æ®: {len(df_final)} è¡Œï¼ˆå»é™¤åˆ†ç±»æ ‡é¢˜å’Œç©ºè¡Œåï¼‰")
    
    # è¾“å‡ºåˆ†ç±»ç»Ÿè®¡
    category_stats = df_final['ä»ªè¡¨ç±»å‹'].value_counts()
    logger.info("ğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
    for category, count in category_stats.items():
        logger.info(f"   â€¢ {category}: {count}å°")
    
    return df_final

def unique_preserve_order(lst):
    """ä¿æŒé¡ºåºçš„å»é‡"""
    seen = set()
    result = []
    for item in lst:
        if item not in seen:
            seen.add(item)
            result.append(item)
    return result

def col_name_suggests_numeric(col_name):
    """æ ¹æ®åˆ—ååˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯æ•°å€¼åˆ—"""
    numeric_keywords = ['æ•°é‡', 'å°æ•°', 'ä¸ªæ•°', 'ä»·æ ¼', 'é‡‘é¢', 'qty', 'quantity', 'price', 'amount']
    col_str = str(col_name).lower()
    return any(keyword in col_str for keyword in numeric_keywords)

def validate_parsed_data(df: pd.DataFrame) -> bool:
    """
    éªŒè¯è§£æåçš„æ•°æ®æ˜¯å¦æœ‰æ•ˆ
    
    Args:
        df: è§£æåçš„DataFrame
    
    Returns:
        æ•°æ®æ˜¯å¦æœ‰æ•ˆ
    """
    if df.empty:
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ä½å·å’Œå‹å·
    valid_tags = df['ä½å·'].str.strip().str.len() > 0
    valid_models = df['å‹å·'].str.strip().str.len() > 0
    
    # æ£€æŸ¥æ•°é‡æ˜¯å¦ä¸ºæ­£æ•°
    valid_quantities = df['æ•°é‡'] > 0
    
    return valid_tags.any() and valid_models.any() and valid_quantities.any()

# å‘åå…¼å®¹çš„æ¥å£
def extract_instrument_info(df: pd.DataFrame) -> pd.DataFrame:
    """
    å‘åå…¼å®¹çš„æ¥å£å‡½æ•°
    """
    return extract_and_parse_instrument_table(df)

def _classify_instruments_with_llm(df: pd.DataFrame, tag_col: str, model_col: str) -> pd.DataFrame:
    """
    å½“è¡¨æ ¼æ²¡æœ‰æ˜ç¡®åˆ†ç±»æ—¶ï¼Œä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­æ¯ä¸ªä»ªè¡¨çš„ç±»å‹
    ç¡®ä¿è¿”å›çš„DataFrameæ ¼å¼ä¸æœ‰åˆ†ç±»æƒ…å†µå®Œå…¨ä¸€è‡´
    
    Args:
        df: ä»ªè¡¨æ•°æ®DataFrame
        tag_col: ä½å·åˆ—å
        model_col: å‹å·åˆ—å
    
    Returns:
        æ·»åŠ äº†æ™ºèƒ½åˆ†ç±»çš„DataFrameï¼ˆæ ¼å¼ä¸æœ‰åˆ†ç±»æƒ…å†µä¸€è‡´ï¼‰
    """
    try:
        from config.settings import get_openai_config
        
        llm_config = get_openai_config()
        if not llm_config.get('api_key'):
            logger.warning("âš ï¸ æ²¡æœ‰LLMé…ç½®ï¼Œæ— æ³•æ™ºèƒ½åˆ†ç±»ï¼Œä¿æŒæœªåˆ†ç±»çŠ¶æ€")
            # ç¡®ä¿è¿”å›æ ¼å¼ä¸€è‡´ï¼šæ‰€æœ‰ä»ªè¡¨æ ‡è®°ä¸º"æœªåˆ†ç±»"
            df_result = df.copy()
            df_result['ä»ªè¡¨ç±»å‹'] = "æœªåˆ†ç±»"
            return df_result
        
        logger.info(f"ğŸ¤– å¼€å§‹LLMæ™ºèƒ½åˆ†ç±» {len(df)} ä¸ªä»ªè¡¨")
        
        # å‡†å¤‡åˆ†ææ•°æ®
        instruments_for_analysis = []
        for idx, row in df.iterrows():
            instrument_info = {
                'index': idx,
                'tag': str(row.get(tag_col, '')).strip(),
                'model': str(row.get(model_col, '')).strip(),
                'spec': str(row.get('è§„æ ¼', '')).strip(),
                'remark': str(row.get('å¤‡æ³¨', '')).strip()
            }
            instruments_for_analysis.append(instrument_info)
        
        # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ¬¡æœ€å¤š10ä¸ªä»ªè¡¨ï¼‰
        batch_size = 10
        df_result = df.copy()
        
        # åˆå§‹åŒ–æ‰€æœ‰ä»ªè¡¨ä¸ºæœªåˆ†ç±»ï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´
        df_result['ä»ªè¡¨ç±»å‹'] = "æœªåˆ†ç±»"
        
        for i in range(0, len(instruments_for_analysis), batch_size):
            batch = instruments_for_analysis[i:i+batch_size]
            logger.info(f"ğŸ” å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªä»ªè¡¨")
            
            # æ„å»ºLLMåˆ†ææç¤º
            instruments_text = ""
            for j, inst in enumerate(batch):
                instruments_text += f"ä»ªè¡¨{j+1}: ä½å·={inst['tag']}, å‹å·={inst['model']}, è§„æ ¼={inst['spec']}, å¤‡æ³¨={inst['remark']}\n"
            
            prompt = f"""è¯·åˆ†æä»¥ä¸‹ä»ªè¡¨æ•°æ®ï¼Œä¸ºæ¯ä¸ªä»ªè¡¨åˆ¤æ–­å…¶ç±»å‹ã€‚

ä»ªè¡¨ä¿¡æ¯ï¼š
{instruments_text}

è¯·åŸºäºä»¥ä¸‹å¸¸è§ä»ªè¡¨ç±»å‹è¿›è¡Œåˆ†ç±»ï¼š
- æ¸©åº¦ä»ªè¡¨ï¼šçƒ­ç”µå¶ã€çƒ­ç”µé˜»ã€æ¸©åº¦è®¡ã€æ¸©åº¦å˜é€å™¨
- å‹åŠ›ä»ªè¡¨ï¼šå‹åŠ›è¡¨ã€å‹åŠ›å˜é€å™¨ã€å·®å‹å˜é€å™¨
- æµé‡ä»ªè¡¨ï¼šæµé‡è®¡ã€æµé‡å˜é€å™¨ã€å­”æ¿ã€å–·å˜´
- æ¶²ä½ä»ªè¡¨ï¼šæ¶²ä½è®¡ã€æ¶²ä½å˜é€å™¨ã€æµ®çƒæ¶²ä½è®¡
- æ§åˆ¶è®¾å¤‡ï¼šè°ƒèŠ‚é˜€ã€æ§åˆ¶é˜€ã€ç”µåŠ¨é˜€ã€æ°”åŠ¨é˜€
- ç”µæ°”è®¾å¤‡ï¼šæ§åˆ¶ç®±ã€é…ç”µç®±ã€æ“ä½œå°
- åˆ†æä»ªè¡¨ï¼šåˆ†æä»ªã€æ£€æµ‹ä»ª
- å…¶ä»–ä»ªè¡¨ï¼šå¦‚æœä¸å±äºä»¥ä¸Šç±»å‹

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "classifications": [
        {{
            "instrument_index": 1,
            "category": "æ¸©åº¦ä»ªè¡¨",
            "confidence": 0.9,
            "reason": "ä½å·TEå¼€å¤´ï¼Œå‹å·ä¸ºçƒ­ç”µé˜»"
        }},
        ...
    ]
}}

æ³¨æ„ï¼š
- ä½å·å‰ç¼€å«ä¹‰ï¼šTE/TT=æ¸©åº¦, PT/PI=å‹åŠ›, FT/FI=æµé‡, LT/LI=æ¶²ä½, CV/FV=æ§åˆ¶é˜€
- æ ¹æ®å‹å·ã€è§„æ ¼ã€å¤‡æ³¨ç»¼åˆåˆ¤æ–­
- ç½®ä¿¡åº¦èŒƒå›´0-1"""

            # è°ƒç”¨LLM
            classifications = _call_llm_for_classification(prompt, llm_config)
            
            if classifications and 'classifications' in classifications:
                # åº”ç”¨åˆ†ç±»ç»“æœ
                for result in classifications['classifications']:
                    instrument_index = result.get('instrument_index', 0) - 1  # è½¬æ¢ä¸º0ç´¢å¼•
                    category = result.get('category', 'å…¶ä»–ä»ªè¡¨')
                    confidence = result.get('confidence', 0.5)
                    
                    if 0 <= instrument_index < len(batch) and confidence >= 0.5:
                        original_idx = batch[instrument_index]['index']
                        df_result.at[original_idx, 'ä»ªè¡¨ç±»å‹'] = category
                        logger.info(f"âœ… ä»ªè¡¨ {batch[instrument_index]['tag']} åˆ†ç±»ä¸º: {category} (ç½®ä¿¡åº¦: {confidence})")
                    else:
                        logger.warning(f"âš ï¸ è·³è¿‡ä½ç½®ä¿¡åº¦åˆ†ç±»: ç½®ä¿¡åº¦ {confidence}")
            else:
                logger.warning(f"âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} LLMåˆ†ç±»å¤±è´¥")
        
        # ç»Ÿè®¡åˆ†ç±»ç»“æœ
        classification_stats = df_result['ä»ªè¡¨ç±»å‹'].value_counts()
        logger.info("ğŸ¯ LLMæ™ºèƒ½åˆ†ç±»å®Œæˆ:")
        for category, count in classification_stats.items():
            logger.info(f"   â€¢ {category}: {count}å°")
        
        # ç¡®ä¿è¿”å›çš„DataFrameæ ¼å¼ä¸æœ‰åˆ†ç±»æƒ…å†µå®Œå…¨ä¸€è‡´
        # åŒ…å«ç›¸åŒçš„åˆ—ï¼š['ä½å·', 'å‹å·', 'æ•°é‡', 'è§„æ ¼', 'å¤‡æ³¨', 'ä»ªè¡¨ç±»å‹']
        return df_result
        
    except Exception as e:
        logger.error(f"âŒ LLMæ™ºèƒ½åˆ†ç±»å¤±è´¥: {str(e)}")
        logger.info("ğŸ“‹ ä¿æŒåŸæœ‰çš„æœªåˆ†ç±»çŠ¶æ€")
        # ç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿè¿”å›ä¸€è‡´çš„æ ¼å¼
        df_result = df.copy()
        df_result['ä»ªè¡¨ç±»å‹'] = "æœªåˆ†ç±»"
        return df_result

def _call_llm_for_classification(prompt: str, config: dict) -> dict:
    """è°ƒç”¨LLMè¿›è¡Œä»ªè¡¨åˆ†ç±»"""
    try:
        from openai import OpenAI
        import json
        import re
        
        client = OpenAI(
            api_key=config.get('api_key'),
            base_url=config.get('base_url', 'https://api.openai.com/v1')
        )
        
        response = client.chat.completions.create(
            model=config.get('model', 'gpt-4o-mini'),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»ªè¡¨åˆ†ç±»ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®ä½å·ã€å‹å·ã€è§„æ ¼ç­‰ä¿¡æ¯åˆ¤æ–­ä»ªè¡¨ç±»å‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=2000
        )
        
        result_text = response.choices[0].message.content
        
        # è§£æJSONå“åº”
        try:
            return json.loads(result_text)
        except json.JSONDecodeError:
            # å°è¯•æå–JSONå†…å®¹
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                logger.warning(f"æ— æ³•è§£æLLMåˆ†ç±»å“åº”: {result_text[:200]}...")
                return {}
                
    except Exception as e:
        logger.error(f"LLMåˆ†ç±»è°ƒç”¨å¤±è´¥: {str(e)}")
        return {}

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•æ•°é‡è§£æåŠŸèƒ½
    test_quantities = ["5", "1Ã—2", "2X3", "3x2", "2*3+1", "5å°", "3ä¸ª", "2Ã—3-1"]
    
    print("æ•°é‡è§£ææµ‹è¯•:")
    for qty in test_quantities:
        result = parse_quantity_field(qty)
        print(f"{qty} -> {result}")
    
    print("\nä»ªè¡¨è¡¨æ ¼è§£æå·¥å…·å·²å°±ç»ª") 