"""
å‘é‡ç´¢å¼•æ„å»ºå·¥å…·
ç”¨äºæ„å»ºä»ªè¡¨å®‰è£…è§„èŒƒçš„å‘é‡æ•°æ®åº“ç´¢å¼•
"""
import os
import sys
import pickle
from typing import List, Dict, Optional
import logging
import PyPDF2
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# é…ç½®å¸¸é‡
FAISS_INDEX_PATH = "./data/indexes/instrument_standards.index"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®é¡¹ç°åœ¨ç”±DocumentIndexerç±»å†…éƒ¨ç®¡ç†

logger = logging.getLogger(__name__)

class DocumentIndexer:
    """æ–‡æ¡£å‘é‡ç´¢å¼•æ„å»ºå™¨"""
    
    def __init__(self, model_name: str = None, dimension: int = None):
        """
        åˆå§‹åŒ–æ–‡æ¡£ç´¢å¼•å™¨
        
        Args:
            model_name: åµŒå…¥æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
            dimension: å‘é‡ç»´åº¦ï¼ˆè‡ªåŠ¨ä»æ¨¡å‹è·å–ï¼‰
        """
        # ä½¿ç”¨å¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½çš„embeddingæ¨¡å‹
        self.model_name = model_name or "shibing624/text2vec-base-chinese"
        self.model = None
        self.index = None
        self.documents = []
        self.metadata = []
        
    def _load_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        if self.model is None:
            try:
                logger.info(f"åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
                self.model = SentenceTransformer(self.model_name)
                logger.info("åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {str(e)}")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ›´é€šç”¨çš„æ¨¡å‹
                try:
                    self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                    logger.info("ä½¿ç”¨å¤‡ç”¨åµŒå…¥æ¨¡å‹")
                except Exception as e2:
                    logger.error(f"å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {str(e2)}")
                    raise
    
    def extract_text_from_pdf(self, pdf_path: str) -> List[str]:
        """
        ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼Œæ”¯æŒå¤šç§æå–æ–¹æ³•
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
        
        Returns:
            æå–çš„æ–‡æœ¬æ®µè½åˆ—è¡¨
        """
        text_chunks = []
        
        try:
            # æ–¹æ³•1ï¼šä½¿ç”¨PyPDF2æå–
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        text = page.extract_text()
                        if text and text.strip():
                            # æŒ‰ç”¨æˆ·è¦æ±‚ï¼šä»¥\nä¸ºåˆ†æ®µæ ‡å‡†ï¼Œä¿æŒåŸå§‹æ®µè½ç»“æ„
                            paragraphs = text.split('\n')
                            
                            for paragraph in paragraphs:
                                paragraph = paragraph.strip()
                                # åªè¦ä¸æ˜¯ç©ºè¡Œï¼Œä¸”æœ‰ä¸€å®šé•¿åº¦ï¼Œå°±ä½œä¸ºä¸€ä¸ªæ–‡æœ¬å—
                                if paragraph and len(paragraph) > 10:  # æœ€å°é•¿åº¦è¦æ±‚é™ä½
                                    text_chunks.append(paragraph)
                                    
                    except Exception as e:
                        logger.warning(f"æå–ç¬¬{page_num+1}é¡µæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
                        continue
                
            # å¦‚æœPyPDF2æå–æ•ˆæœä¸å¥½ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if len(text_chunks) < 5:  # å¦‚æœæå–çš„æ–‡æœ¬å—å¤ªå°‘
                logger.warning(f"PyPDF2æå–æ•ˆæœä¸ä½³ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                text_chunks = self._try_alternative_pdf_extraction(pdf_path)
                
                logger.info(f"ä»PDFæ–‡ä»¶ {pdf_path} æå–äº† {len(text_chunks)} ä¸ªæ–‡æœ¬å—")
                return text_chunks
                
        except Exception as e:
            logger.error(f"è¯»å–PDFæ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def _try_alternative_pdf_extraction(self, pdf_path: str) -> List[str]:
        """
        å°è¯•å…¶ä»–PDFæ–‡æœ¬æå–æ–¹æ³•
        """
        text_chunks = []
        
        try:
            # å°è¯•ä½¿ç”¨pdfplumberï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import pdfplumber
                with pdfplumber.open(pdf_path) as pdf:
                    for page in pdf.pages:
                        text = page.extract_text()
                        if text:
                            # æŒ‰ç”¨æˆ·è¦æ±‚ï¼šä»¥\nä¸ºåˆ†æ®µæ ‡å‡†
                            paragraphs = text.split('\n')
                            for para in paragraphs:
                                para = para.strip()
                                if para and len(para) > 10:
                                    text_chunks.append(para)
                logger.info(f"ä½¿ç”¨pdfplumberæå–äº† {len(text_chunks)} ä¸ªæ–‡æœ¬å—")
                return text_chunks
            except ImportError:
                logger.info("pdfplumberæœªå®‰è£…ï¼Œè·³è¿‡")
            
            # å°è¯•ä½¿ç”¨pymupdfï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import fitz  # pymupdf
                doc = fitz.open(pdf_path)
                for page_num in range(doc.page_count):
                    page = doc[page_num]
                    text = page.get_text()
                    if text:
                        # æŒ‰ç”¨æˆ·è¦æ±‚ï¼šä»¥\nä¸ºåˆ†æ®µæ ‡å‡†
                        paragraphs = text.split('\n')
                        for para in paragraphs:
                            para = para.strip()
                            if para and len(para) > 10:
                                text_chunks.append(para)
                doc.close()
                logger.info(f"ä½¿ç”¨pymupdfæå–äº† {len(text_chunks)} ä¸ªæ–‡æœ¬å—")
                return text_chunks
            except ImportError:
                logger.info("pymupdfæœªå®‰è£…ï¼Œè·³è¿‡")
                
        except Exception as e:
            logger.warning(f"å¤‡ç”¨PDFæå–æ–¹æ³•å¤±è´¥: {str(e)}")
        
        return text_chunks
    
    def extract_text_from_txt(self, file_path: str) -> List[str]:
        """ä»TXTæ–‡ä»¶ä¸­æå–æ–‡æœ¬å—"""
        text_chunks = []
        
        try:
            # å°è¯•ä¸åŒç¼–ç è¯»å–æ–‡ä»¶
            encodings = ['utf-8', 'gbk', 'gb2312']
            content = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as file:
                        content = file.read().strip()
                        break
                except UnicodeDecodeError:
                    continue
            
            if content is None:
                logger.error(f"æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è¯»å–æ–‡ä»¶: {file_path}")
                return []
            
            if not content:
                return []
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»ªè¡¨å®‰è£…è§„èŒƒæ–‡æ¡£ï¼ˆé€šè¿‡æ–‡ä»¶åå’Œå†…å®¹ç‰¹å¾åˆ¤æ–­ï¼‰
            is_instrument_standard = (
                'ä»ªè¡¨å®‰è£…è§„èŒƒ' in os.path.basename(file_path) or 
                'ç¬¬ 1.0.1 æ¡æœ¬è§„èŒƒé€‚ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–ä»ªè¡¨' in content[:200]
            )
            
            if is_instrument_standard:
                # å¯¹äºä»ªè¡¨å®‰è£…è§„èŒƒï¼ŒæŒ‰æ¡æ¬¾å·åˆ†å‰²
                import re
                
                # æ”¯æŒå¤šç§ç¼–å·æ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼
                patterns = [
                    r'ç¬¬\s*\d+\.\d+\.\d+\s*æ¡',  # ç¬¬ 1.0.1 æ¡
                    r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,2}\.?\s*',  # ä¸€. æˆ– ä¸€
                    r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,2}ã€\s*',  # ä¸€ã€
                    r'\d+\.?\s*',  # 1. æˆ– 1
                    r'\d+ã€\s*',  # 1ã€
                    r'\(\d+\)\s*',  # (1)
                    r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,2}\)\s*',  # ä¸€)
                    r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,2}\)ã€\s*',  # ä¸€)ã€
                    r'â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³',  # â‘ â‘¡â‘¢ç­‰
                ]
                
                # å…ˆå°è¯•æœ€ç²¾ç¡®çš„"ç¬¬ X.X.X æ¡"æ ¼å¼
                clause_pattern = r'ç¬¬\s*\d+\.\d+\.\d+\s*æ¡'
                clause_matches = re.findall(clause_pattern, content)
                
                if len(clause_matches) > 10:  # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿå¤šçš„æ¡æ¬¾ï¼Œä½¿ç”¨æ¡æ¬¾åˆ†å‰²
                    # åˆ†å‰²æ–‡æ¡£
                    clauses = re.split(clause_pattern, content)
                    
                    # é‡æ–°ç»„åˆï¼Œæ¯ä¸ªæ¡æ¬¾åŒ…å«æ¡æ¬¾å·å’Œå†…å®¹
                    for i, clause_content in enumerate(clauses):
                        if i == 0:
                            # ç¬¬ä¸€éƒ¨åˆ†æ˜¯æ¡æ¬¾å·ä¹‹å‰çš„å†…å®¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                            if clause_content.strip():
                                text_chunks.append(clause_content.strip())
                        else:
                            # ç»„åˆæ¡æ¬¾å·å’Œå†…å®¹
                            if i-1 < len(clause_matches):
                                clause_title = clause_matches[i-1]
                                full_clause = f"{clause_title} {clause_content.strip()}"
                                if len(full_clause) > 20:  # æœ€å°é•¿åº¦è¿‡æ»¤
                                    text_chunks.append(full_clause)
                    
                    logger.info(f"æŒ‰æ¡æ¬¾åˆ†å‰²ä»ªè¡¨è§„èŒƒæ–‡æ¡£ {file_path}ï¼Œæå–äº† {len(text_chunks)} ä¸ªæ¡æ¬¾")
                else:
                    # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„"ç¬¬ X.X.X æ¡"æ ¼å¼ï¼Œå°è¯•å…¶ä»–ç¼–å·æ ¼å¼
                    logger.info(f"æœªæ‰¾åˆ°è¶³å¤Ÿçš„æ ‡å‡†æ¡æ¬¾æ ¼å¼ï¼Œå°è¯•å…¶ä»–ç¼–å·æ ¼å¼")
                    
                    # å°è¯•æŒ‰å…¶ä»–ç¼–å·æ ¼å¼åˆ†å‰²
                    best_split = []
                    best_pattern = None
                    
                    for pattern in patterns[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªå·²ç»è¯•è¿‡çš„æ¨¡å¼
                        try:
                            matches = re.findall(pattern, content)
                            if len(matches) > 5:  # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿå¤šçš„åŒ¹é…
                                splits = re.split(pattern, content)
                                if len(splits) > len(best_split):
                                    best_split = splits
                                    best_pattern = pattern
                                    best_matches = matches
                        except:
                            continue
                    
                    if best_split and len(best_split) > 3:
                        # ä½¿ç”¨æ‰¾åˆ°çš„æœ€ä½³åˆ†å‰²æ–¹å¼
                        for i, section_content in enumerate(best_split):
                            if i == 0:
                                if section_content.strip():
                                    text_chunks.append(section_content.strip())
                            else:
                                if i-1 < len(best_matches):
                                    section_title = best_matches[i-1]
                                    full_section = f"{section_title} {section_content.strip()}"
                                    if len(full_section) > 20:
                                        text_chunks.append(full_section)
                        
                        logger.info(f"æŒ‰ç¼–å·æ ¼å¼ '{best_pattern}' åˆ†å‰²æ–‡æ¡£ {file_path}ï¼Œæå–äº† {len(text_chunks)} ä¸ªæ®µè½")
                    else:
                        # å¦‚æœæ‰€æœ‰ç¼–å·æ ¼å¼éƒ½ä¸é€‚ç”¨ï¼Œå›é€€åˆ°æ®µè½åˆ†å‰²
                        paragraphs = content.split('\n')
                        for paragraph in paragraphs:
                            paragraph = paragraph.strip()
                            if paragraph and len(paragraph) > 50:  # å¯¹äºå¤æ‚æ–‡æ¡£æé«˜æœ€å°é•¿åº¦
                                text_chunks.append(paragraph)
                        
                        logger.info(f"ä½¿ç”¨æ®µè½åˆ†å‰²æ–‡æ¡£ {file_path}ï¼Œæå–äº† {len(text_chunks)} ä¸ªæ®µè½")
            else:
                # å¯¹äºå…¶ä»–æ–‡æ¡£ï¼Œä¿æŒåŸæœ‰çš„æŒ‰æ®µè½åˆ†å‰²é€»è¾‘
                paragraphs = content.split('\n')
                for paragraph in paragraphs:
                    paragraph = paragraph.strip()
                    if paragraph and len(paragraph) > 10:
                        text_chunks.append(paragraph)
                        
                logger.info(f"ä»æ–‡æœ¬æ–‡ä»¶ {file_path} æå–äº† {len(text_chunks)} ä¸ªæ–‡æœ¬å—")
            
            return text_chunks
            
        except Exception as e:
            logger.error(f"è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def process_documents(self, file_paths: List[str]) -> List[Dict]:
        """
        å¤„ç†å¤šä¸ªæ–‡æ¡£æ–‡ä»¶
        
        Args:
            file_paths: æ–‡æ¡£æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
        Returns:
            å¤„ç†åçš„æ–‡æ¡£ä¿¡æ¯åˆ—è¡¨
        """
        all_documents = []
        
        for file_path in file_paths:
            if not os.path.exists(file_path):
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext == '.pdf':
                text_chunks = self.extract_text_from_pdf(file_path)
            elif file_ext in ['.txt', '.md']:
                text_chunks = self.extract_text_from_txt(file_path)
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                continue
            
            # ä¸ºæ¯ä¸ªæ–‡æœ¬å—æ·»åŠ å…ƒæ•°æ®
            for i, chunk in enumerate(text_chunks):
                doc_info = {
                    'content': chunk,
                    'source_file': file_path,
                    'chunk_id': i,
                    'file_type': file_ext
                }
                all_documents.append(doc_info)
        
        logger.info(f"æ€»å…±å¤„ç†äº† {len(all_documents)} ä¸ªæ–‡æ¡£å—")
        return all_documents
    
    def build_index(self, file_paths: List[str], index_path: str = None) -> bool:
        """
        æ„å»ºå‘é‡ç´¢å¼•
        
        Args:
            file_paths: æ–‡æ¡£æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            index_path: ç´¢å¼•ä¿å­˜è·¯å¾„
        
        Returns:
            æ˜¯å¦æ„å»ºæˆåŠŸ
        """
        try:
            # åŠ è½½æ¨¡å‹
            self._load_model()
            
            # å¤„ç†æ–‡æ¡£
            documents = self.process_documents(file_paths)
            if not documents:
                logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹")
                return False
            
            # æå–æ–‡æœ¬å†…å®¹
            texts = [doc['content'] for doc in documents]
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            logger.info("å¼€å§‹ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡...")
            embeddings = self.model.encode(texts, show_progress_bar=True)
            
            # åˆ›å»ºFAISSç´¢å¼•
            logger.info("åˆ›å»ºFAISSç´¢å¼•...")
            self.index = faiss.IndexFlatIP(embeddings.shape[1])  # ä½¿ç”¨å†…ç§¯ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡
            
            # æ ‡å‡†åŒ–å‘é‡ä»¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            faiss.normalize_L2(embeddings)
            
            # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
            self.index.add(embeddings.astype(np.float32))
            
            # ä¿å­˜æ–‡æ¡£å’Œå…ƒæ•°æ®
            self.documents = texts
            self.metadata = documents
            
            # ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶
            if index_path is None:
                index_path = FAISS_INDEX_PATH
            
            self.save_index(index_path)
            
            logger.info(f"æˆåŠŸæ„å»ºå‘é‡ç´¢å¼•ï¼ŒåŒ…å« {len(texts)} ä¸ªæ–‡æ¡£å—")
            return True
            
        except Exception as e:
            logger.error(f"æ„å»ºå‘é‡ç´¢å¼•å¤±è´¥: {str(e)}")
            return False
    
    def save_index(self, index_path: str):
        """
        ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶
        
        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(index_path), exist_ok=True)
            
            # ä¿å­˜FAISSç´¢å¼•
            faiss.write_index(self.index, index_path)
            
            # ä¿å­˜æ–‡æ¡£å’Œå…ƒæ•°æ®
            metadata_path = index_path.replace('.index', '_metadata.pkl')
            with open(metadata_path, 'wb') as f:
                pickle.dump({
                    'documents': self.documents,
                    'metadata': self.metadata,
                    'model_name': self.model_name
                }, f)
            
            logger.info(f"ç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {str(e)}")
            raise
    
    def load_index(self, index_path: str) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•
        
        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            if not os.path.exists(index_path):
                logger.error(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
                return False
            
            # åŠ è½½FAISSç´¢å¼•
            self.index = faiss.read_index(index_path)
            
            # åŠ è½½æ–‡æ¡£å’Œå…ƒæ•°æ®
            metadata_path = index_path.replace('.index', '_metadata.pkl')
            if os.path.exists(metadata_path):
                with open(metadata_path, 'rb') as f:
                    data = pickle.load(f)
                    self.documents = data['documents']
                    self.metadata = data['metadata']
                    self.model_name = data.get('model_name', self.model_name)
            
            # åŠ è½½æ¨¡å‹
            self._load_model()
            
            logger.info(f"æˆåŠŸåŠ è½½ç´¢å¼•ï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£å—")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½ç´¢å¼•å¤±è´¥: {str(e)}")
            return False

def build_index_from_files(file_paths: List[str], index_path: str = None) -> bool:
    """
    ä»æ–‡ä»¶åˆ—è¡¨æ„å»ºå‘é‡ç´¢å¼•çš„ä¾¿æ·å‡½æ•°
    
    Args:
        file_paths: æ–‡æ¡£æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        index_path: ç´¢å¼•ä¿å­˜è·¯å¾„
    
    Returns:
        æ˜¯å¦æ„å»ºæˆåŠŸ
    """
    indexer = DocumentIndexer()
    return indexer.build_index(file_paths, index_path)

def scan_and_list_documents():
    """
    æ‰«æå¹¶åˆ—å‡ºdata/standardsç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
    ä¸åˆ›å»ºä»»ä½•ç¡¬ç¼–ç çš„ç¤ºä¾‹æ–‡ä»¶
    """
    import glob
    
    # åŠ¨æ€ç¡®å®šé¡¹ç›®æ ¹ç›®å½•å’Œdata/standardsè·¯å¾„
    current_file = os.path.abspath(__file__)
    current_dir = os.path.dirname(current_file)  # toolsç›®å½•
    
    # ä»toolsç›®å½•å‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
    project_root = os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•
    standards_dir = os.path.join(project_root, "data", "standards")
    
    # å¦‚æœä¸Šé¢çš„è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„ä½ç½®
    if not os.path.exists(standards_dir):
        # å°è¯•å½“å‰å·¥ä½œç›®å½•ä¸‹çš„data/standards
        cwd_standards = os.path.join(os.getcwd(), "data", "standards")
        if os.path.exists(cwd_standards):
            standards_dir = cwd_standards
        else:
            # å‘ä¸Šæœç´¢åŒ…å«dataç›®å½•çš„ç›®å½•
            search_dir = current_dir
            while search_dir != os.path.dirname(search_dir):
                test_data_dir = os.path.join(search_dir, "data", "standards")
                if os.path.exists(test_data_dir):
                    standards_dir = test_data_dir
                    break
                search_dir = os.path.dirname(search_dir)
    
    print(f"æ‰«æç›®å½•: {standards_dir}")
    
    if not os.path.exists(standards_dir):
        print(f"ç›®å½•ä¸å­˜åœ¨: {standards_dir}")
        return []
    
    # æ‰«ææ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶
    supported_extensions = ['*.pdf', '*.PDF', '*.txt', '*.TXT', '*.md', '*.MD']
    all_files = []
    
    for extension in supported_extensions:
        pattern = os.path.join(standards_dir, extension)
        found_files = glob.glob(pattern)
        all_files.extend(found_files)
    
    all_files = sorted(list(set(all_files)))
    
    if all_files:
        print(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶:")
        for file_path in all_files:
            try:
                file_size = os.path.getsize(file_path) / 1024
                print(f"  - {os.path.basename(file_path)} ({file_size:.1f} KB)")
            except Exception as e:
                print(f"  - {os.path.basename(file_path)} (æ— æ³•è·å–å¤§å°)")
    else:
        print("æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£æ–‡ä»¶")
        
        # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        try:
            all_items = os.listdir(standards_dir)
            if all_items:
                print("ç›®å½•ä¸­ç°æœ‰çš„æ–‡ä»¶:")
                for item in all_items:
                    print(f"  - {item}")
            else:
                print("ç›®å½•ä¸ºç©º")
        except Exception as e:
            print(f"æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {str(e)}")
    
    return all_files

def rebuild_rag_index():
    """é‡æ–°æ„å»ºRAGå‘é‡ç´¢å¼•çš„ä¾¿æ·å‡½æ•° - åŠ¨æ€æœç´¢æ‰€æœ‰PDFæ–‡ä»¶"""
    import glob
    
    # åŠ¨æ€ç¡®å®šé¡¹ç›®æ ¹ç›®å½•å’Œdata/standardsè·¯å¾„
    current_file = os.path.abspath(__file__)
    current_dir = os.path.dirname(current_file)  # toolsç›®å½•
    
    # ä»toolsç›®å½•å‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŒ…å«contest1ç­‰ï¼‰
    project_root = os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•
    standards_dir = os.path.join(project_root, "data", "standards")
    
    # å¦‚æœä¸Šé¢çš„è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„ä½ç½®
    if not os.path.exists(standards_dir):
        # å°è¯•å½“å‰å·¥ä½œç›®å½•ä¸‹çš„data/standards
        cwd_standards = os.path.join(os.getcwd(), "data", "standards")
        if os.path.exists(cwd_standards):
            standards_dir = cwd_standards
        else:
            # å‘ä¸Šæœç´¢åŒ…å«dataç›®å½•çš„ç›®å½•
            search_dir = current_dir
            while search_dir != os.path.dirname(search_dir):
                test_data_dir = os.path.join(search_dir, "data", "standards")
                if os.path.exists(test_data_dir):
                    standards_dir = test_data_dir
                    break
                search_dir = os.path.dirname(search_dir)
    
    print(f"æ‰«æç›®å½•: {standards_dir}")
    
    if not os.path.exists(standards_dir):
        print(f"é”™è¯¯: æ ‡å‡†æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {standards_dir}")
        print(f"è¯·ç¡®ä¿ data/standards/ ç›®å½•å­˜åœ¨ä¸”åŒ…å«PDFæ–‡ä»¶")
        return False
    
    # åŠ¨æ€æœç´¢æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶ï¼ˆç»ä¸ç¡¬ç¼–ç æ–‡ä»¶åï¼‰
    supported_extensions = ['*.pdf', '*.PDF', '*.txt', '*.TXT', '*.md', '*.MD']
    all_files = []
    
    print("æ­£åœ¨æ‰«ææ‰€æœ‰æ–‡æ¡£æ–‡ä»¶...")
    for extension in supported_extensions:
        pattern = os.path.join(standards_dir, extension)
        found_files = glob.glob(pattern)
        all_files.extend(found_files)
    
    # å»é‡å¹¶æ’åº
    all_files = sorted(list(set(all_files)))
    
    if not all_files:
        print(f"é”™è¯¯: åœ¨ç›®å½• {standards_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£æ–‡ä»¶")
        print("æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: PDF, TXT, MD")
        
        # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        try:
            all_items = os.listdir(standards_dir)
            if all_items:
                print(f"ç›®å½•ä¸­ç°æœ‰çš„æ–‡ä»¶:")
                for item in all_items:
                    item_path = os.path.join(standards_dir, item)
                    if os.path.isfile(item_path):
                        file_size = os.path.getsize(item_path) / 1024
                        print(f"  - {item} ({file_size:.1f} KB)")
            else:
                print("ç›®å½•ä¸ºç©º")
        except Exception as e:
            print(f"æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {str(e)}")
        
        return False
    
    print(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶:")
    total_size = 0
    for file_path in all_files:
        try:
            file_size = os.path.getsize(file_path) / 1024  # KB
            total_size += file_size
            print(f"  - {os.path.basename(file_path)} ({file_size:.1f} KB)")
        except Exception as e:
            print(f"  - {os.path.basename(file_path)} (æ— æ³•è·å–å¤§å°: {str(e)})")
    
    print(f"æ€»è®¡: {total_size:.1f} KB")
    
    # æ„å»ºå‘é‡ç´¢å¼•
    print(f"\nå¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
    indexer = DocumentIndexer()
    success = indexer.build_index(all_files)
    
    if success:
        print(f"âœ… æˆåŠŸ: å‘é‡ç´¢å¼•æ„å»ºæˆåŠŸï¼")
        print(f"ğŸ“Š ç»Ÿè®¡: ç´¢å¼•åŒ…å« {len(indexer.documents)} ä¸ªæ–‡æ¡£å—")
        print(f"ğŸ’¾ ä¿å­˜: ç´¢å¼•å·²ä¿å­˜åˆ°: data/indexes/instrument_standards.index")
        print(f"\nğŸ‰ å®Œæˆ: æ™ºèƒ½ä½“ç°åœ¨å¯ä»¥è‡ªåŠ¨æ£€ç´¢è¿™äº›æ–‡æ¡£å†…å®¹äº†ï¼")
        return True
    else:
        print(f"âŒ å¤±è´¥: å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥")
        return False

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='å‘é‡ç´¢å¼•æ„å»ºå·¥å…·')
    parser.add_argument('--list', action='store_true', 
                        help='ä»…åˆ—å‡ºdata/standardsç›®å½•ä¸­çš„æ–‡æ¡£æ–‡ä»¶ï¼Œä¸æ„å»ºç´¢å¼•')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    if args.list:
        # ä»…æ‰«æå¹¶åˆ—å‡ºæ–‡æ¡£æ–‡ä»¶
        print("æ‰«ædata/standardsç›®å½•ä¸­çš„æ–‡æ¡£æ–‡ä»¶...")
        files = scan_and_list_documents()
        if files:
            print(f"\nå¯ç”¨äºæ„å»ºç´¢å¼•çš„æ–‡ä»¶: {len(files)} ä¸ª")
        else:
            print("\næœªæ‰¾åˆ°å¯ç”¨çš„æ–‡æ¡£æ–‡ä»¶")
            print("è¯·å°†PDFã€TXTæˆ–MDæ ¼å¼çš„æ–‡æ¡£æ”¾å…¥data/standardsç›®å½•")
    else:
        # é»˜è®¤æ¨¡å¼ï¼šæ„å»ºå‘é‡ç´¢å¼• - ä½¿ç”¨ç”¨æˆ·çš„çœŸå®PDFæ–‡ä»¶
        print("ğŸ” æ­£åœ¨æ‰«ædata/standardsç›®å½•ä¸­çš„æ–‡æ¡£æ–‡ä»¶...")
        success = rebuild_rag_index()
        if not success:
            print("\nâŒ æ„å»ºå¤±è´¥çš„å¯èƒ½åŸå› :")
            print("1. data/standards/ ç›®å½•ä¸å­˜åœ¨")
            print("2. ç›®å½•ä¸­æ²¡æœ‰PDFã€TXTæˆ–MDæ–‡ä»¶")
            print("3. æ–‡ä»¶æ— æ³•è¯»å–æˆ–æ ¼å¼ä¸æ”¯æŒ")
            print("\nğŸ’¡ å»ºè®®:")
            print("- å°†æ‚¨çš„PDFæ–‡æ¡£æ”¾å…¥data/standardsç›®å½•")
            print("- è¿è¡Œ 'python tools/build_index.py --list' æŸ¥çœ‹å½“å‰æ–‡ä»¶")
            sys.exit(1) 