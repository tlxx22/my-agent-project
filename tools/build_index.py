"""
å‘é‡ç´¢å¼•æ„å»ºå·¥å…·
ç”¨äºæ„å»ºä»ªè¡¨å®‰è£…è§„èŒƒçš„å‘é‡æ•°æ®åº“ç´¢å¼•
"""
import os
import sys
import pickle
import json
import re
from typing import List, Dict, Optional
import logging
import PyPDF2
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from datetime import datetime
from pathlib import Path
import argparse
from collections import Counter
import fitz  # PyMuPDF

# é…ç½®å¸¸é‡
FAISS_INDEX_PATH = "./data/indexes/instrument_standards.index"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®é¡¹ç°åœ¨ç”±DocumentIndexerç±»å†…éƒ¨ç®¡ç†

logger = logging.getLogger(__name__)

class InstrumentTypeClassifier:
    """LLMé©±åŠ¨çš„ä»ªè¡¨ç±»å‹æ™ºèƒ½è¯†åˆ«å™¨"""
    
    def __init__(self):
        self.identified_types = {}
        self.classification_cache = {}
    
    def analyze_documents_with_llm(self, documents: List[str]) -> Dict[str, Dict]:
        """
        ä½¿ç”¨LLMæ™ºèƒ½è¯†åˆ«æ–‡æ¡£ä¸­çš„ä»ªè¡¨ç±»å‹ï¼ˆäºŒçº§åˆ†ç±»ç»“æ„ï¼‰
        
        Args:
            documents: æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨
        
        Returns:
            è¯†åˆ«å‡ºçš„ä»ªè¡¨ç±»å‹å­—å…¸ï¼ˆåŒ…å«å¤§åˆ†ç±»å’Œå…·ä½“ä»ªè¡¨çš„å±‚æ¬¡ç»“æ„ï¼‰
        """
        logger.info("ğŸ¤– å¯åŠ¨LLMæ™ºèƒ½è¯†åˆ«ä»ªè¡¨ç±»å‹ï¼ˆäºŒçº§åˆ†ç±»ï¼‰...")
        logger.info(f"ğŸ“š åˆ†ææ–‡æ¡£æ•°é‡: {len(documents)} ä¸ªæ–‡æ¡£å—")
        
        # åˆå¹¶æ‰€æœ‰æ–‡æ¡£æ–‡æœ¬è¿›è¡Œåˆ†æ
        combined_text = "\n".join(documents)
        
        # è®¾è®¡å¢å¼ºçš„äºŒçº§åˆ†ç±»LLM prompt
        analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ä»ªè¡¨å®‰è£…è§„èŒƒæ–‡æ¡£ï¼Œè¯†åˆ«å‡ºæ–‡æ¡£ä¸­æåˆ°çš„**å…·ä½“ä»ªè¡¨ç±»å‹**ï¼Œå¹¶æŒ‰ç…§äºŒçº§åˆ†ç±»ç»“æ„ç»„ç»‡ã€‚

## åˆ†ç±»è¦æ±‚ï¼š
1. **å¤§åˆ†ç±»**ï¼šæ¸©åº¦ä»ªè¡¨ã€å‹åŠ›ä»ªè¡¨ã€æµé‡ä»ªè¡¨ã€æ¶²ä½ä»ªè¡¨ã€æ§åˆ¶è®¾å¤‡ã€ç”µæ°”è®¾å¤‡ã€åˆ†æä»ªè¡¨ç­‰
2. **å…·ä½“ä»ªè¡¨**ï¼šæ¯ä¸ªå…·ä½“ä»ªè¡¨éƒ½å¿…é¡»æ ‡æ˜æ‰€å±çš„å¤§åˆ†ç±»
3. **åªè¯†åˆ«å…·ä½“è®¾å¤‡åç§°**ï¼Œæ’é™¤"ä»ªè¡¨"ã€"è®¾å¤‡"ç­‰é€šç”¨è¯æ±‡
4. **æ¯ç§ä»ªè¡¨åœ¨æ–‡æ¡£ä¸­åº”è¯¥æœ‰å®é™…æåŠ**

## ç¤ºä¾‹æ ¼å¼ï¼š
```
çƒ­ç”µå¶ (type: æ¸©åº¦ä»ªè¡¨)
çƒ­ç”µé˜» (type: æ¸©åº¦ä»ªè¡¨)  
åŒé‡‘å±æ¸©åº¦è®¡ (type: æ¸©åº¦ä»ªè¡¨)
å‹åŠ›å˜é€å™¨ (type: å‹åŠ›ä»ªè¡¨)
å·®å‹å˜é€å™¨ (type: å‹åŠ›ä»ªè¡¨)
ç”µç£æµé‡è®¡ (type: æµé‡ä»ªè¡¨)
æ¶¡è½®æµé‡è®¡ (type: æµé‡ä»ªè¡¨)
ç£ç¿»æ¿æ¶²ä½è®¡ (type: æ¶²ä½ä»ªè¡¨)
æµ®çƒæ¶²ä½è®¡ (type: æ¶²ä½ä»ªè¡¨)
è°ƒèŠ‚é˜€ (type: æ§åˆ¶è®¾å¤‡)
ç”µåŠ¨æ‰§è¡Œæœºæ„ (type: æ§åˆ¶è®¾å¤‡)
é…ç”µç®± (type: ç”µæ°”è®¾å¤‡)
æ§åˆ¶æŸœ (type: ç”µæ°”è®¾å¤‡)
PHè®¡ (type: åˆ†æä»ªè¡¨)
æº¶æ°§ä»ª (type: åˆ†æä»ªè¡¨)
```

## æ–‡æ¡£å†…å®¹ï¼š
{combined_text}

## è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œç»“æ„å¦‚ä¸‹ï¼š
```json
{{
    "instrument_categories": {{
        "æ¸©åº¦ä»ªè¡¨": {{
            "description": "ç”¨äºæ¸©åº¦æµ‹é‡å’Œæ§åˆ¶çš„ä»ªè¡¨è®¾å¤‡",
            "instruments": {{
                "çƒ­ç”µå¶": {{
                    "frequency": 15,
                    "description": "ç”¨äºé«˜æ¸©æµ‹é‡çš„æ¸©åº¦ä¼ æ„Ÿå™¨",
                    "typical_models": ["WRN-630", "Kå‹çƒ­ç”µå¶"],
                    "main_applications": ["é”…ç‚‰æ¸©åº¦", "ç®¡é“æ¸©åº¦"]
                }},
                "çƒ­ç”µé˜»": {{
                    "frequency": 12,
                    "description": "ç”¨äºä¸­ä½æ¸©æµ‹é‡çš„æ¸©åº¦ä¼ æ„Ÿå™¨", 
                    "typical_models": ["WZP-630", "Pt100"],
                    "main_applications": ["ç»™æ°´æ¸©åº¦", "ç¯å¢ƒæ¸©åº¦"]
                }}
            }}
        }},
        "å‹åŠ›ä»ªè¡¨": {{
            "description": "ç”¨äºå‹åŠ›æµ‹é‡å’Œæ§åˆ¶çš„ä»ªè¡¨è®¾å¤‡",
            "instruments": {{
                "å‹åŠ›å˜é€å™¨": {{
                    "frequency": 20,
                    "description": "å°†å‹åŠ›ä¿¡å·è½¬æ¢ä¸ºæ ‡å‡†ä¿¡å·çš„è®¾å¤‡",
                    "typical_models": ["EJA430A", "3051"],
                    "main_applications": ["ç®¡é“å‹åŠ›ç›‘æµ‹", "å®¹å™¨å‹åŠ›æ§åˆ¶"]
                }},
                "å·®å‹å˜é€å™¨": {{
                    "frequency": 8,
                    "description": "æµ‹é‡ä¸¤ç‚¹é—´å‹åŠ›å·®çš„å˜é€å™¨",
                    "typical_models": ["EJA110A", "3051DP"],
                    "main_applications": ["æµé‡æµ‹é‡", "æ¶²ä½æµ‹é‡"]
                }}
            }}
        }}
    }},
    "summary": {{
        "total_categories": 6,
        "total_instruments": 25,
        "analysis_method": "llm_hierarchical_analysis"
    }}
}}
```

æ³¨æ„ï¼š
- è¯†åˆ«å°½å¯èƒ½å¤šçš„å…·ä½“ä»ªè¡¨ç±»å‹
- æ¯ä¸ªä»ªè¡¨éƒ½è¦å½’ç±»åˆ°åˆé€‚çš„å¤§åˆ†ç±»ä¸‹
- æä¾›å…¸å‹å‹å·å’Œä¸»è¦åº”ç”¨åœºæ™¯
- ä¼°ç®—åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°é¢‘æ¬¡
"""
        
        try:
            # è°ƒç”¨LLMè¿›è¡ŒäºŒçº§åˆ†ç±»åˆ†æ
            result = self._call_llm_for_analysis(analysis_prompt)
            
            if result and "instrument_categories" in result:
                # è½¬æ¢ä¸ºæ‰å¹³åŒ–çš„æ ¼å¼ï¼Œä¾¿äºåç»­å¤„ç†
                flattened_types = self._flatten_hierarchical_results(result['instrument_categories'])
                
                logger.info(f"âœ… LLMæˆåŠŸè¯†åˆ«äº† {len(flattened_types)} ç§å…·ä½“ä»ªè¡¨ç±»å‹")
                logger.info(f"ğŸ“Š è¦†ç›–äº† {len(result['instrument_categories'])} ä¸ªå¤§åˆ†ç±»")
                
                # æ˜¾ç¤ºè¯†åˆ«ç»“æœæ¦‚è§ˆ
                self._display_hierarchical_results(result['instrument_categories'])
                
                return flattened_types
            else:
                logger.warning("âš ï¸ LLMåˆ†æç»“æœæ ¼å¼ä¸æ­£ç¡®ï¼Œæ— æ³•è¯†åˆ«ä»ªè¡¨ç±»å‹")
                return {}
                
        except Exception as e:
            logger.warning(f"âš ï¸ LLMåˆ†æå¤±è´¥: {str(e)}ï¼Œæ— æ³•è¯†åˆ«ä»ªè¡¨ç±»å‹")
            return {}
    
    def _call_llm_for_analysis(self, prompt: str) -> Dict:
        """
        è°ƒç”¨LLMè¿›è¡Œåˆ†æ
        
        Args:
            prompt: åˆ†ææç¤ºè¯
        
        Returns:
            LLMåˆ†æç»“æœ
        """
        # è¿™é‡Œå¯ä»¥é›†æˆä¸åŒçš„LLMæœåŠ¡
        # æ¯”å¦‚OpenAI APIã€Azure OpenAIã€æœ¬åœ°LLMç­‰
        
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨configä¸­é…ç½®çš„LLM
            from config.settings import get_openai_config
            llm_config = get_openai_config()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„API key
            if llm_config.get('api_key'):
                logger.info(f"ğŸ¤– ä½¿ç”¨OpenAI API: {llm_config.get('model', 'gpt-4o-mini')}")
                return self._call_openai_llm(prompt, llm_config)
            else:
                logger.warning("âš ï¸ OpenAI API keyæœªé…ç½®ï¼Œä½¿ç”¨æœ¬åœ°åˆ†æ")
                return self._call_local_llm(prompt)
                
        except ImportError:
            logger.info("æœªæ‰¾åˆ°LLMé…ç½®ï¼Œå°è¯•æœ¬åœ°åˆ†æ")
            return self._call_local_llm(prompt)
    
    def _call_openai_llm(self, prompt: str, config: Dict) -> Dict:
        """è°ƒç”¨OpenAI API"""
        try:
            from openai import OpenAI
            
            # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
            client = OpenAI(
                api_key=config.get('api_key'),
                base_url=config.get('base_url', 'https://api.openai.com/v1')
            )
            
            logger.info(f"ğŸ“¡ è°ƒç”¨OpenAI API - æ¨¡å‹: {config.get('model', 'gpt-4o-mini')}")
            
            response = client.chat.completions.create(
                model=config.get('model', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥ä¸šä»ªè¡¨è¯†åˆ«ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†ææ–‡æ¡£å†…å®¹ï¼Œè¯†åˆ«å‡ºå…·ä½“çš„ä»ªè¡¨ç±»å‹ï¼Œé¿å…é€šç”¨è¯æ±‡å¦‚'ä»ªè¡¨'ã€'è®¾å¤‡'ç­‰ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )
            
            result_text = response.choices[0].message.content
            logger.info(f"âœ… OpenAI APIè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            # è§£æJSONå“åº”
            import json
            try:
                return json.loads(result_text)
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                logger.info(f"åŸå§‹å“åº”: {result_text[:500]}...")
                # å°è¯•æå–JSONå†…å®¹
                json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
                else:
                    raise
            
        except Exception as e:
            logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def _call_local_llm(self, prompt: str) -> Dict:
        """æœ¬åœ°LLMå¤‡ç”¨æ–¹æ¡ˆï¼ˆç°åœ¨ç›´æ¥è¿”å›ç©ºï¼Œå› ä¸ºæˆ‘ä»¬æœ‰OpenAI APIï¼‰"""
        logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„LLMé…ç½®ï¼Œæ— æ³•è¿›è¡Œä»ªè¡¨ç±»å‹è¯†åˆ«")
        return {
            'instrument_types': {}
        }
    
    def _flatten_hierarchical_results(self, categories: Dict) -> Dict[str, Dict]:
        """
        å°†å±‚æ¬¡åŒ–ç»“æœè½¬æ¢ä¸ºæ‰å¹³åŒ–æ ¼å¼ï¼Œä¾¿äºåç»­ä½¿ç”¨
        
        Args:
            categories: å±‚æ¬¡åŒ–çš„åˆ†ç±»ç»“æœ
        
        Returns:
            æ‰å¹³åŒ–çš„ä»ªè¡¨ç±»å‹å­—å…¸ï¼Œæ ¼å¼: "ä»ªè¡¨åç§° (type: å¤§åˆ†ç±»)"
        """
        flattened = {}
        
        for category_name, category_info in categories.items():
            instruments = category_info.get('instruments', {})
            
            for instrument_name, instrument_info in instruments.items():
                # åˆ›å»ºåŒ…å«ç±»å‹ä¿¡æ¯çš„é”®å
                full_name = f"{instrument_name} (type: {category_name})"
                
                flattened[full_name] = {
                    'category': category_name,
                    'instrument_name': instrument_name,
                    'frequency': instrument_info.get('frequency', 1),
                    'description': instrument_info.get('description', ''),
                    'typical_models': instrument_info.get('typical_models', []),
                    'main_applications': instrument_info.get('main_applications', []),
                    'llm_confidence': instrument_info.get('frequency', 0)
                }
        
        return flattened
    
    def _display_hierarchical_results(self, categories: Dict):
        """
        æ˜¾ç¤ºå±‚æ¬¡åŒ–è¯†åˆ«ç»“æœ
        
        Args:
            categories: å±‚æ¬¡åŒ–çš„åˆ†ç±»ç»“æœ
        """
        print(f"\nğŸ¯ LLMè¯†åˆ«çš„ä»ªè¡¨ç±»å‹ï¼ˆäºŒçº§åˆ†ç±»ï¼‰:")
        print("=" * 60)
        
        for category_name, category_info in categories.items():
            instruments = category_info.get('instruments', {})
            print(f"\nğŸ“‚ {category_name} ({len(instruments)}ç§ä»ªè¡¨)")
            print(f"   ğŸ“ {category_info.get('description', '')}")
            
            for instrument_name, instrument_info in instruments.items():
                frequency = instrument_info.get('frequency', 0)
                models = instrument_info.get('typical_models', [])
                models_str = f" | å‹å·: {', '.join(models[:2])}" if models else ""
                print(f"   â€¢ {instrument_name} (é¢‘æ¬¡: {frequency}){models_str}")
        
        total_instruments = sum(len(cat.get('instruments', {})) for cat in categories.values())
        print(f"\nğŸ“Š æ€»è®¡: {len(categories)}ä¸ªå¤§åˆ†ç±», {total_instruments}ç§å…·ä½“ä»ªè¡¨")
        print("=" * 60)
    
    def save_classification_results(self, types_dict: Dict, save_path: str = "./data/llm_instrument_types.json"):
        """
        ä¿å­˜LLMè¯†åˆ«çš„ä»ªè¡¨ç±»å‹ç»“æœï¼ˆæ”¯æŒå±‚æ¬¡åŒ–ç»“æ„ï¼‰
        
        Args:
            types_dict: è¯†åˆ«ç»“æœå­—å…¸ï¼ˆæ‰å¹³åŒ–æ ¼å¼ï¼‰
            save_path: ä¿å­˜è·¯å¾„
        """
        try:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ·»åŠ å…ƒæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
            categories = set()
            for instrument_key, instrument_info in types_dict.items():
                categories.add(instrument_info.get('category', 'å…¶ä»–'))
            
            result_data = {
                'instrument_types': types_dict,
                'metadata': {
                    'total_types': len(types_dict),
                    'total_categories': len(categories),
                    'categories': list(categories),
                    'generation_time': str(datetime.now()),
                    'method': 'llm_hierarchical_analysis',
                    'format_explanation': 'ä»ªè¡¨åç§°æ ¼å¼: "å…·ä½“ä»ªè¡¨ (type: å¤§åˆ†ç±»)"'
                }
            }
            
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“ å±‚æ¬¡åŒ–ä»ªè¡¨ç±»å‹è¯†åˆ«ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
            logger.info(f"ğŸ“Š æ€»è®¡: {len(categories)}ä¸ªå¤§åˆ†ç±», {len(types_dict)}ç§å…·ä½“ä»ªè¡¨")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¯†åˆ«ç»“æœå¤±è´¥: {str(e)}")

class DocumentIndexer:
    """æ–‡æ¡£å‘é‡ç´¢å¼•æ„å»ºå™¨"""
    
    def __init__(self, model_name: str = None, dimension: int = None):
        """
        åˆå§‹åŒ–æ–‡æ¡£ç´¢å¼•å™¨
        
        Args:
            model_name: åµŒå…¥æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
            dimension: å‘é‡ç»´åº¦ï¼ˆè‡ªåŠ¨ä»æ¨¡å‹è·å–ï¼‰
        """
        # ä½¿ç”¨å¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½çš„embeddingæ¨¡å‹
        self.model_name = model_name or "shibing624/text2vec-base-chinese"
        self.model = None
        self.index = None
        self.documents = []
        self.metadata = []
        
        # æ·»åŠ ä»ªè¡¨ç±»å‹åˆ†ç±»å™¨
        self.instrument_classifier = InstrumentTypeClassifier()
        
    def _load_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        if self.model is None:
            try:
                logger.info(f"åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
                self.model = SentenceTransformer(self.model_name)
                logger.info("åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {str(e)}")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ›´é€šç”¨çš„æ¨¡å‹
                try:
                    self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                    logger.info("ä½¿ç”¨å¤‡ç”¨åµŒå…¥æ¨¡å‹")
                except Exception as e2:
                    logger.error(f"å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {str(e2)}")
                    raise
    
    def extract_text_from_pdf(self, pdf_path: str) -> List[str]:
        """
        ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼Œæ”¯æŒå¤šç§æå–æ–¹æ³•
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
        
        Returns:
            æå–çš„æ–‡æœ¬æ®µè½åˆ—è¡¨
        """
        text_chunks = []
        
        try:
            # æ–¹æ³•1ï¼šä½¿ç”¨PyPDF2æå–
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        text = page.extract_text()
                        if text and text.strip():
                            # æŒ‰ç”¨æˆ·è¦æ±‚ï¼šä»¥\nä¸ºåˆ†æ®µæ ‡å‡†ï¼Œä¿æŒåŸå§‹æ®µè½ç»“æ„
                            paragraphs = text.split('\n')
                            
                            for paragraph in paragraphs:
                                paragraph = paragraph.strip()
                                # åªè¦ä¸æ˜¯ç©ºè¡Œï¼Œä¸”æœ‰ä¸€å®šé•¿åº¦ï¼Œå°±ä½œä¸ºä¸€ä¸ªæ–‡æœ¬å—
                                if paragraph and len(paragraph) > 10:  # æœ€å°é•¿åº¦è¦æ±‚é™ä½
                                    text_chunks.append(paragraph)
                                    
                    except Exception as e:
                        logger.warning(f"æå–ç¬¬{page_num+1}é¡µæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
                        continue
                
            # å¦‚æœPyPDF2æå–æ•ˆæœä¸å¥½ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if len(text_chunks) < 5:  # å¦‚æœæå–çš„æ–‡æœ¬å—å¤ªå°‘
                logger.warning(f"PyPDF2æå–æ•ˆæœä¸ä½³ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                text_chunks = self._try_alternative_pdf_extraction(pdf_path)
                
                logger.info(f"ä»PDFæ–‡ä»¶ {pdf_path} æå–äº† {len(text_chunks)} ä¸ªæ–‡æœ¬å—")
                return text_chunks
                
        except Exception as e:
            logger.error(f"è¯»å–PDFæ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def _try_alternative_pdf_extraction(self, pdf_path: str) -> List[str]:
        """
        å°è¯•å…¶ä»–PDFæ–‡æœ¬æå–æ–¹æ³•
        """
        text_chunks = []
        
        try:
            # å°è¯•ä½¿ç”¨pdfplumberï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import pdfplumber
                with pdfplumber.open(pdf_path) as pdf:
                    for page in pdf.pages:
                        text = page.extract_text()
                        if text:
                            # æŒ‰ç”¨æˆ·è¦æ±‚ï¼šä»¥\nä¸ºåˆ†æ®µæ ‡å‡†
                            paragraphs = text.split('\n')
                            for para in paragraphs:
                                para = para.strip()
                                if para and len(para) > 10:
                                    text_chunks.append(para)
                logger.info(f"ä½¿ç”¨pdfplumberæå–äº† {len(text_chunks)} ä¸ªæ–‡æœ¬å—")
                return text_chunks
            except ImportError:
                logger.info("pdfplumberæœªå®‰è£…ï¼Œè·³è¿‡")
            
            # å°è¯•ä½¿ç”¨pymupdfï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import fitz  # pymupdf
                doc = fitz.open(pdf_path)
                for page_num in range(doc.page_count):
                    page = doc[page_num]
                    text = page.get_text()
                    if text:
                        # æŒ‰ç”¨æˆ·è¦æ±‚ï¼šä»¥\nä¸ºåˆ†æ®µæ ‡å‡†
                        paragraphs = text.split('\n')
                        for para in paragraphs:
                            para = para.strip()
                            if para and len(para) > 10:
                                text_chunks.append(para)
                doc.close()
                logger.info(f"ä½¿ç”¨pymupdfæå–äº† {len(text_chunks)} ä¸ªæ–‡æœ¬å—")
                return text_chunks
            except ImportError:
                logger.info("pymupdfæœªå®‰è£…ï¼Œè·³è¿‡")
                
        except Exception as e:
            logger.warning(f"å¤‡ç”¨PDFæå–æ–¹æ³•å¤±è´¥: {str(e)}")
        
        return text_chunks
    
    def extract_text_from_txt(self, file_path: str) -> List[str]:
        """ä»TXTæ–‡ä»¶ä¸­æå–æ–‡æœ¬å—"""
        text_chunks = []
        
        try:
            # å°è¯•ä¸åŒç¼–ç è¯»å–æ–‡ä»¶
            encodings = ['utf-8', 'gbk', 'gb2312']
            content = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as file:
                        content = file.read().strip()
                        break
                except UnicodeDecodeError:
                    continue
            
            if content is None:
                logger.error(f"æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è¯»å–æ–‡ä»¶: {file_path}")
                return []
            
            if not content:
                return []
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»ªè¡¨å®‰è£…è§„èŒƒæ–‡æ¡£ï¼ˆé€šè¿‡æ–‡ä»¶åå’Œå†…å®¹ç‰¹å¾åˆ¤æ–­ï¼‰
            is_instrument_standard = (
                'ä»ªè¡¨å®‰è£…è§„èŒƒ' in os.path.basename(file_path) or 
                'ç¬¬ 1.0.1 æ¡æœ¬è§„èŒƒé€‚ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–ä»ªè¡¨' in content[:200]
            )
            
            if is_instrument_standard:
                # å¯¹äºä»ªè¡¨å®‰è£…è§„èŒƒï¼Œä½¿ç”¨å¤šçº§æ ‡é¢˜æ™ºèƒ½åˆ†å‰²
                import re
                
                # å®šä¹‰å¤šçº§æ ‡é¢˜çš„æ­£åˆ™æ¨¡å¼ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
                hierarchical_patterns = [
                    (r'ç¬¬\s*\d+\.\d+\.\d+\s*æ¡', 'æ¡æ¬¾'),          # ç¬¬ 1.0.1 æ¡ (æœ€é‡è¦)
                    (r'ç¬¬\s*\d+\.\d+\s*æ¡', 'æ¡æ¬¾'),             # ç¬¬ 1.1 æ¡
                    (r'ç¬¬\s*\d+\s*æ¡', 'æ¡æ¬¾'),                  # ç¬¬ 1 æ¡
                    (r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,2}\.', 'ä¸€çº§æ ‡é¢˜'),   # ä¸€. äºŒ. ç­‰
                    (r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,2}ã€', 'ä¸€çº§æ ‡é¢˜'),   # ä¸€ã€äºŒã€ç­‰  
                    (r'\d+\.', 'äºŒçº§æ ‡é¢˜'),                     # 1. 2. ç­‰
                    (r'\d+ã€', 'äºŒçº§æ ‡é¢˜'),                     # 1ã€2ã€ç­‰
                    (r'\(\d+\)', 'ä¸‰çº§æ ‡é¢˜'),                   # (1) (2) ç­‰
                    (r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,2}\)', 'ä¸‰çº§æ ‡é¢˜'), # ä¸€) äºŒ) ç­‰
                    (r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]', 'å››çº§æ ‡é¢˜'), # â‘  â‘¡ ç­‰
                ]
                
                # åŒæ—¶åº”ç”¨æ‰€æœ‰æ­£åˆ™ï¼Œæ‰¾å‡ºæ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹
                all_splits = []
                
                for pattern, level_name in hierarchical_patterns:
                    try:
                        # æ‰¾å‡ºè¯¥æ¨¡å¼çš„æ‰€æœ‰åŒ¹é…ä½ç½®
                        for match in re.finditer(pattern, content):
                            all_splits.append({
                                'start': match.start(),
                                'end': match.end(),
                                'title': match.group(),
                                'level': level_name,
                                'pattern': pattern
                            })
                    except Exception as e:
                        logger.warning(f"æ­£åˆ™æ¨¡å¼ {pattern} æ‰§è¡Œå¤±è´¥: {e}")
                        continue
                
                if all_splits:
                    # æŒ‰ä½ç½®æ’åºæ‰€æœ‰åˆ†å‰²ç‚¹
                    all_splits.sort(key=lambda x: x['start'])
                    
                    logger.info(f"ğŸ” æ‰¾åˆ° {len(all_splits)} ä¸ªå¤šçº§æ ‡é¢˜åˆ†å‰²ç‚¹")
                    
                    # æå–å„ä¸ªæ®µè½
                    for i, split_info in enumerate(all_splits):
                        # ç¡®å®šæ®µè½å†…å®¹çš„èŒƒå›´
                        content_start = split_info['end']
                        
                        # æ‰¾åˆ°ä¸‹ä¸€ä¸ªåˆ†å‰²ç‚¹
                        if i + 1 < len(all_splits):
                            content_end = all_splits[i + 1]['start']
                        else:
                            content_end = len(content)
                        
                        # æå–æ®µè½å†…å®¹
                        section_content = content[content_start:content_end].strip()
                        
                        if section_content:
                            # ç»„åˆå®Œæ•´æ®µè½
                            full_section = f"[{split_info['level']}] {split_info['title']} {section_content}"
                            
                            # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ®µè½
                            if len(section_content) > 20:
                                text_chunks.append(full_section)
                    
                    logger.info(f"âœ… å¤šçº§æ ‡é¢˜åˆ†å‰²å®Œæˆï¼Œæå–äº† {len(text_chunks)} ä¸ªç»“æ„åŒ–æ®µè½")
                
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æ„åŒ–æ ‡é¢˜ï¼Œå›é€€åˆ°æ®µè½åˆ†å‰²
                    logger.info("æœªæ‰¾åˆ°ç»“æ„åŒ–æ ‡é¢˜ï¼Œä½¿ç”¨æ®µè½åˆ†å‰²")
                    paragraphs = content.split('\n\n')
                    for paragraph in paragraphs:
                        paragraph = paragraph.strip()
                        if paragraph and len(paragraph) > 30:
                            text_chunks.append(paragraph)
                    
                    logger.info(f"æ®µè½åˆ†å‰²æå–äº† {len(text_chunks)} ä¸ªæ®µè½")
            else:
                # å¯¹äºå…¶ä»–æ–‡æ¡£ï¼Œä¿æŒåŸæœ‰çš„æŒ‰æ®µè½åˆ†å‰²é€»è¾‘
                paragraphs = content.split('\n')
                for paragraph in paragraphs:
                    paragraph = paragraph.strip()
                    if paragraph and len(paragraph) > 10:
                        text_chunks.append(paragraph)
                        
                logger.info(f"ä»æ–‡æœ¬æ–‡ä»¶ {file_path} æå–äº† {len(text_chunks)} ä¸ªæ–‡æœ¬å—")
            
            return text_chunks
            
        except Exception as e:
            logger.error(f"è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def process_documents(self, file_paths: List[str]) -> List[Dict]:
        """
        å¤„ç†å¤šä¸ªæ–‡æ¡£æ–‡ä»¶
        
        Args:
            file_paths: æ–‡æ¡£æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
        Returns:
            å¤„ç†åçš„æ–‡æ¡£ä¿¡æ¯åˆ—è¡¨
        """
        all_documents = []
        
        for file_path in file_paths:
            if not os.path.exists(file_path):
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext == '.pdf':
                text_chunks = self.extract_text_from_pdf(file_path)
            elif file_ext in ['.txt', '.md']:
                text_chunks = self.extract_text_from_txt(file_path)
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                continue
            
            # ä¸ºæ¯ä¸ªæ–‡æœ¬å—æ·»åŠ å…ƒæ•°æ®
            for i, chunk in enumerate(text_chunks):
                doc_info = {
                    'content': chunk,
                    'source_file': file_path,
                    'chunk_id': i,
                    'file_type': file_ext
                }
                all_documents.append(doc_info)
        
        logger.info(f"æ€»å…±å¤„ç†äº† {len(all_documents)} ä¸ªæ–‡æ¡£å—")
        return all_documents
    
    def build_index(self, file_paths: List[str], index_path: str = None) -> bool:
        """
        æ„å»ºå‘é‡ç´¢å¼•ï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«LLMä»ªè¡¨ç±»å‹è¯†åˆ«ï¼‰
        
        Args:
            file_paths: æ–‡æ¡£æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            index_path: ç´¢å¼•ä¿å­˜è·¯å¾„
        
        Returns:
            æ˜¯å¦æ„å»ºæˆåŠŸ
        """
        try:
            # åŠ è½½æ¨¡å‹
            self._load_model()
            
            # å¤„ç†æ–‡æ¡£
            documents = self.process_documents(file_paths)
            if not documents:
                logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹")
                return False
            
            # æå–æ–‡æœ¬å†…å®¹
            texts = [doc['content'] for doc in documents]
            
            # ğŸ¤– æ–°å¢ï¼šLLMæ™ºèƒ½è¯†åˆ«ä»ªè¡¨ç±»å‹
            logger.info("ğŸ¤– å¯åŠ¨LLMæ™ºèƒ½è¯†åˆ«ä»ªè¡¨ç±»å‹...")
            instrument_types = self.instrument_classifier.analyze_documents_with_llm(texts)
            
            if instrument_types:
                # ä¿å­˜è¯†åˆ«ç»“æœ
                self.instrument_classifier.save_classification_results(instrument_types)
                logger.info(f"âœ… LLMæˆåŠŸè¯†åˆ«äº† {len(instrument_types)} ç§å…·ä½“ä»ªè¡¨ç±»å‹")
                
                # æ˜¾ç¤ºè¯†åˆ«ç»“æœ
                print(f"\nğŸ¯ LLMè¯†åˆ«çš„ä»ªè¡¨ç±»å‹:")
                for instrument, info in instrument_types.items():
                    print(f"   â€¢ {instrument} (ç±»åˆ«: {info['category']}, é¢‘æ¬¡: {info['frequency']})")
            else:
                logger.warning("âš ï¸ æœªèƒ½è¯†åˆ«å‡ºä»ªè¡¨ç±»å‹")
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            logger.info("å¼€å§‹ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡...")
            embeddings = self.model.encode(texts, show_progress_bar=True)
            
            # åˆ›å»ºFAISSç´¢å¼•
            logger.info("åˆ›å»ºFAISSç´¢å¼•...")
            self.index = faiss.IndexFlatIP(embeddings.shape[1])  # ä½¿ç”¨å†…ç§¯ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡
            
            # æ ‡å‡†åŒ–å‘é‡ä»¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            faiss.normalize_L2(embeddings)
            
            # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
            self.index.add(embeddings.astype(np.float32))
            
            # ä¿å­˜æ–‡æ¡£å’Œå…ƒæ•°æ®
            self.documents = texts
            self.metadata = documents
            
            # ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶
            if index_path is None:
                index_path = FAISS_INDEX_PATH
            
            self.save_index(index_path)
            
            logger.info(f"æˆåŠŸæ„å»ºå‘é‡ç´¢å¼•ï¼ŒåŒ…å« {len(texts)} ä¸ªæ–‡æ¡£å—")
            return True
            
        except Exception as e:
            logger.error(f"æ„å»ºå‘é‡ç´¢å¼•å¤±è´¥: {str(e)}")
            return False
    
    def save_index(self, index_path: str):
        """
        ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶
        
        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(index_path), exist_ok=True)
            
            # ä¿å­˜FAISSç´¢å¼•
            faiss.write_index(self.index, index_path)
            
            # ä¿å­˜æ–‡æ¡£å’Œå…ƒæ•°æ®
            metadata_path = index_path.replace('.index', '_metadata.pkl')
            with open(metadata_path, 'wb') as f:
                pickle.dump({
                    'documents': self.documents,
                    'metadata': self.metadata,
                    'model_name': self.model_name
                }, f)
            
            logger.info(f"ç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {str(e)}")
            raise
    
    def load_index(self, index_path: str) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•
        
        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            if not os.path.exists(index_path):
                logger.error(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
                return False
            
            # åŠ è½½FAISSç´¢å¼•
            self.index = faiss.read_index(index_path)
            
            # åŠ è½½æ–‡æ¡£å’Œå…ƒæ•°æ®
            metadata_path = index_path.replace('.index', '_metadata.pkl')
            if os.path.exists(metadata_path):
                with open(metadata_path, 'rb') as f:
                    data = pickle.load(f)
                    self.documents = data['documents']
                    self.metadata = data['metadata']
                    self.model_name = data.get('model_name', self.model_name)
            
            # åŠ è½½æ¨¡å‹
            self._load_model()
            
            logger.info(f"æˆåŠŸåŠ è½½ç´¢å¼•ï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£å—")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½ç´¢å¼•å¤±è´¥: {str(e)}")
            return False

def build_index_from_files(file_paths: List[str], index_path: str = None) -> bool:
    """
    ä»æ–‡ä»¶åˆ—è¡¨æ„å»ºå‘é‡ç´¢å¼•çš„ä¾¿æ·å‡½æ•°
    
    Args:
        file_paths: æ–‡æ¡£æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        index_path: ç´¢å¼•ä¿å­˜è·¯å¾„
    
    Returns:
        æ˜¯å¦æ„å»ºæˆåŠŸ
    """
    indexer = DocumentIndexer()
    return indexer.build_index(file_paths, index_path)

def scan_and_list_documents():
    """
    æ‰«æå¹¶åˆ—å‡ºdata/standardsç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
    ä¸åˆ›å»ºä»»ä½•ç¡¬ç¼–ç çš„ç¤ºä¾‹æ–‡ä»¶
    """
    import glob
    
    # åŠ¨æ€ç¡®å®šé¡¹ç›®æ ¹ç›®å½•å’Œdata/standardsè·¯å¾„
    current_file = os.path.abspath(__file__)
    current_dir = os.path.dirname(current_file)  # toolsç›®å½•
    
    # ä»toolsç›®å½•å‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
    project_root = os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•
    standards_dir = os.path.join(project_root, "data", "standards")
    
    # å¦‚æœä¸Šé¢çš„è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„ä½ç½®
    if not os.path.exists(standards_dir):
        # å°è¯•å½“å‰å·¥ä½œç›®å½•ä¸‹çš„data/standards
        cwd_standards = os.path.join(os.getcwd(), "data", "standards")
        if os.path.exists(cwd_standards):
            standards_dir = cwd_standards
        else:
            # å‘ä¸Šæœç´¢åŒ…å«dataç›®å½•çš„ç›®å½•
            search_dir = current_dir
            while search_dir != os.path.dirname(search_dir):
                test_data_dir = os.path.join(search_dir, "data", "standards")
                if os.path.exists(test_data_dir):
                    standards_dir = test_data_dir
                    break
                search_dir = os.path.dirname(search_dir)
    
    print(f"æ‰«æç›®å½•: {standards_dir}")
    
    if not os.path.exists(standards_dir):
        print(f"ç›®å½•ä¸å­˜åœ¨: {standards_dir}")
        return []
    
    # æ‰«ææ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶
    supported_extensions = ['*.pdf', '*.PDF', '*.txt', '*.TXT', '*.md', '*.MD']
    all_files = []
    
    for extension in supported_extensions:
        pattern = os.path.join(standards_dir, extension)
        found_files = glob.glob(pattern)
        all_files.extend(found_files)
    
    all_files = sorted(list(set(all_files)))
    
    if all_files:
        print(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶:")
        for file_path in all_files:
            try:
                file_size = os.path.getsize(file_path) / 1024
                print(f"  - {os.path.basename(file_path)} ({file_size:.1f} KB)")
            except Exception as e:
                print(f"  - {os.path.basename(file_path)} (æ— æ³•è·å–å¤§å°)")
    else:
        print("æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£æ–‡ä»¶")
        
        # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        try:
            all_items = os.listdir(standards_dir)
            if all_items:
                print("ç›®å½•ä¸­ç°æœ‰çš„æ–‡ä»¶:")
                for item in all_items:
                    print(f"  - {item}")
            else:
                print("ç›®å½•ä¸ºç©º")
        except Exception as e:
            print(f"æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {str(e)}")
    
    return all_files

def rebuild_rag_index():
    """é‡æ–°æ„å»ºRAGå‘é‡ç´¢å¼•çš„ä¾¿æ·å‡½æ•° - åŠ¨æ€æœç´¢æ‰€æœ‰PDFæ–‡ä»¶"""
    import glob
    
    # åŠ¨æ€ç¡®å®šé¡¹ç›®æ ¹ç›®å½•å’Œdata/standardsè·¯å¾„
    current_file = os.path.abspath(__file__)
    current_dir = os.path.dirname(current_file)  # toolsç›®å½•
    
    # ä»toolsç›®å½•å‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŒ…å«contest1ç­‰ï¼‰
    project_root = os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•
    standards_dir = os.path.join(project_root, "data", "standards")
    
    # å¦‚æœä¸Šé¢çš„è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„ä½ç½®
    if not os.path.exists(standards_dir):
        # å°è¯•å½“å‰å·¥ä½œç›®å½•ä¸‹çš„data/standards
        cwd_standards = os.path.join(os.getcwd(), "data", "standards")
        if os.path.exists(cwd_standards):
            standards_dir = cwd_standards
        else:
            # å‘ä¸Šæœç´¢åŒ…å«dataç›®å½•çš„ç›®å½•
            search_dir = current_dir
            while search_dir != os.path.dirname(search_dir):
                test_data_dir = os.path.join(search_dir, "data", "standards")
                if os.path.exists(test_data_dir):
                    standards_dir = test_data_dir
                    break
                search_dir = os.path.dirname(search_dir)
    
    print(f"æ‰«æç›®å½•: {standards_dir}")
    
    if not os.path.exists(standards_dir):
        print(f"é”™è¯¯: æ ‡å‡†æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {standards_dir}")
        print(f"è¯·ç¡®ä¿ data/standards/ ç›®å½•å­˜åœ¨ä¸”åŒ…å«PDFæ–‡ä»¶")
        return False
    
    # åŠ¨æ€æœç´¢æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶ï¼ˆç»ä¸ç¡¬ç¼–ç æ–‡ä»¶åï¼‰
    supported_extensions = ['*.pdf', '*.PDF', '*.txt', '*.TXT', '*.md', '*.MD']
    all_files = []
    
    print("æ­£åœ¨æ‰«ææ‰€æœ‰æ–‡æ¡£æ–‡ä»¶...")
    for extension in supported_extensions:
        pattern = os.path.join(standards_dir, extension)
        found_files = glob.glob(pattern)
        all_files.extend(found_files)
    
    # å»é‡å¹¶æ’åº
    all_files = sorted(list(set(all_files)))
    
    if not all_files:
        print(f"é”™è¯¯: åœ¨ç›®å½• {standards_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£æ–‡ä»¶")
        print("æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: PDF, TXT, MD")
        
        # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        try:
            all_items = os.listdir(standards_dir)
            if all_items:
                print(f"ç›®å½•ä¸­ç°æœ‰çš„æ–‡ä»¶:")
                for item in all_items:
                    item_path = os.path.join(standards_dir, item)
                    if os.path.isfile(item_path):
                        file_size = os.path.getsize(item_path) / 1024
                        print(f"  - {item} ({file_size:.1f} KB)")
            else:
                print("ç›®å½•ä¸ºç©º")
        except Exception as e:
            print(f"æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {str(e)}")
        
        return False
    
    print(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶:")
    total_size = 0
    for file_path in all_files:
        try:
            file_size = os.path.getsize(file_path) / 1024  # KB
            total_size += file_size
            print(f"  - {os.path.basename(file_path)} ({file_size:.1f} KB)")
        except Exception as e:
            print(f"  - {os.path.basename(file_path)} (æ— æ³•è·å–å¤§å°: {str(e)})")
    
    print(f"æ€»è®¡: {total_size:.1f} KB")
    
    # æ„å»ºå‘é‡ç´¢å¼•
    print(f"\nå¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
    indexer = DocumentIndexer()
    success = indexer.build_index(all_files)
    
    if success:
        print(f"âœ… æˆåŠŸ: å‘é‡ç´¢å¼•æ„å»ºæˆåŠŸï¼")
        print(f"ğŸ“Š ç»Ÿè®¡: ç´¢å¼•åŒ…å« {len(indexer.documents)} ä¸ªæ–‡æ¡£å—")
        print(f"ğŸ’¾ ä¿å­˜: ç´¢å¼•å·²ä¿å­˜åˆ°: data/indexes/instrument_standards.index")
        print(f"\nğŸ‰ å®Œæˆ: æ™ºèƒ½ä½“ç°åœ¨å¯ä»¥è‡ªåŠ¨æ£€ç´¢è¿™äº›æ–‡æ¡£å†…å®¹äº†ï¼")
        return True
    else:
        print(f"âŒ å¤±è´¥: å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥")
        return False

if __name__ == "__main__":
    import argparse
    from datetime import datetime
    
    parser = argparse.ArgumentParser(description='å‘é‡ç´¢å¼•æ„å»ºå·¥å…·ï¼ˆé›†æˆLLMä»ªè¡¨ç±»å‹è¯†åˆ«ï¼‰')
    parser.add_argument('--list', action='store_true', 
                        help='ä»…åˆ—å‡ºdata/standardsç›®å½•ä¸­çš„æ–‡æ¡£æ–‡ä»¶ï¼Œä¸æ„å»ºç´¢å¼•')
    parser.add_argument('--mode', choices=['test', 'rebuild'], default='rebuild',
                        help='è¿è¡Œæ¨¡å¼: test(ä½¿ç”¨ç¤ºä¾‹æ•°æ®) æˆ– rebuild(ä½¿ç”¨ç”¨æˆ·PDF)')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    if args.list:
        # ä»…æ‰«æå¹¶åˆ—å‡ºæ–‡æ¡£æ–‡ä»¶
        print("æ‰«ædata/standardsç›®å½•ä¸­çš„æ–‡æ¡£æ–‡ä»¶...")
        files = scan_and_list_documents()
        if files:
            print(f"\nå¯ç”¨äºæ„å»ºç´¢å¼•çš„æ–‡ä»¶: {len(files)} ä¸ª")
        else:
            print("\næœªæ‰¾åˆ°å¯ç”¨çš„æ–‡æ¡£æ–‡ä»¶")
            print("è¯·å°†PDFã€TXTæˆ–MDæ ¼å¼çš„æ–‡æ¡£æ”¾å…¥data/standardsç›®å½•")
    else:
        # é»˜è®¤æ¨¡å¼ï¼šæ„å»ºå‘é‡ç´¢å¼• + LLMä»ªè¡¨ç±»å‹è¯†åˆ«
        print("ğŸš€ å¯åŠ¨æ™ºèƒ½å‘é‡ç´¢å¼•æ„å»ºï¼ˆé›†æˆLLMä»ªè¡¨ç±»å‹è¯†åˆ«ï¼‰")
        print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now()}")
        success = rebuild_rag_index()
        
        if success:
            print(f"\nğŸ‰ å®Œæˆæ—¶é—´: {datetime.now()}")
            print("âœ… ç´¢å¼•æ„å»ºæˆåŠŸï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½:")
            print("   1. ğŸ“š æ–‡æ¡£å‘é‡åŒ–ç´¢å¼•")
            print("   2. ğŸ¤– LLMæ™ºèƒ½ä»ªè¡¨ç±»å‹è¯†åˆ«")
            print("   3. ğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜")
            print("\nğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨è‡ªé€‚åº”RAGç³»ç»Ÿè¿›è¡Œæ™ºèƒ½æŸ¥è¯¢äº†ï¼")
        else:
            print(f"\nâŒ å®Œæˆæ—¶é—´: {datetime.now()}")
            print("æ„å»ºå¤±è´¥çš„å¯èƒ½åŸå› :")
            print("1. data/standards/ ç›®å½•ä¸å­˜åœ¨")
            print("2. ç›®å½•ä¸­æ²¡æœ‰PDFã€TXTæˆ–MDæ–‡ä»¶")
            print("3. LLMé…ç½®é—®é¢˜ï¼ˆå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼‰")
            sys.exit(1) 