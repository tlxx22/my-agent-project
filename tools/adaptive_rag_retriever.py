"""
è‡ªé€‚åº”RAGæ£€ç´¢å™¨
è‡ªåŠ¨ä»æ–‡æ¡£ä¸­å­¦ä¹ ä»ªè¡¨ç±»å‹å’Œç›¸å…³æœ¯è¯­ï¼Œå®Œå…¨é¿å…ç¡¬ç¼–ç 
å…·å¤‡è‡ªå­¦ä¹ å’ŒåŠ¨æ€æ‰©å±•èƒ½åŠ›
"""
import os
import re
import json
from typing import List, Dict, Optional
import logging
from collections import defaultdict, Counter
import jieba
import jieba.posseg as pseg
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from tools.match_standard_clause import StandardClauseRetriever

logger = logging.getLogger(__name__)

class AdaptiveRAGRetriever:
    """è‡ªé€‚åº”RAGæ£€ç´¢å™¨ - é›¶ç¡¬ç¼–ç è®¾è®¡"""
    
    def __init__(self, index_path: str = None, auto_learn: bool = True):
        """åˆå§‹åŒ–è‡ªé€‚åº”æ£€ç´¢å™¨"""
        self.base_retriever = StandardClauseRetriever(index_path)
        self.knowledge_cache_path = "./data/adaptive_knowledge_cache.json"
        
        # åŠ¨æ€å­¦ä¹ çš„çŸ¥è¯†åº“
        self.instrument_patterns = {}
        self.semantic_clusters = {}
        self.document_statistics = {}
        
        if auto_learn:
            self._auto_learn_from_documents()
        else:
            self._load_cached_knowledge()
    
    def _auto_learn_from_documents(self):
        """è‡ªåŠ¨ä»æ–‡æ¡£ä¸­å­¦ä¹ ä»ªè¡¨ç±»å‹å’Œç›¸å…³æœ¯è¯­"""
        logger.info("å¼€å§‹è‡ªåŠ¨å­¦ä¹ æ–‡æ¡£ä¸­çš„ä»ªè¡¨ç±»å‹å’Œæœ¯è¯­...")
        
        # ç¡®ä¿æ£€ç´¢å™¨å·²åŠ è½½
        if not self.base_retriever.is_loaded:
            self.base_retriever.load_index()
        
        if not self.base_retriever.indexer.documents:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œæ— æ³•è¿›è¡Œè‡ªåŠ¨å­¦ä¹ ")
            return
        
        documents = self.base_retriever.indexer.documents
        
        # 1. æå–ä»ªè¡¨ç±»å‹æ¨¡å¼
        self._extract_instrument_patterns(documents)
        
        # 2. æ„å»ºè¯­ä¹‰èšç±»
        self._build_semantic_clusters(documents)
        
        # 3. ç”Ÿæˆæ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
        self._generate_document_statistics(documents)
        
        # 4. ä¿å­˜å­¦ä¹ ç»“æœ
        self._save_learned_knowledge()
        
        logger.info(f"è‡ªåŠ¨å­¦ä¹ å®Œæˆï¼è¯†åˆ«äº† {len(self.instrument_patterns)} ç§ä»ªè¡¨ç±»å‹")
    
    def _extract_instrument_patterns(self, documents: List[str]):
        """ä»æ–‡æ¡£ä¸­æå–ä»ªè¡¨ç±»å‹æ¨¡å¼"""
        # åŠ¨æ€çš„ä»ªè¡¨ç±»å‹æ¨¡å¼ - åŸºäºæ–‡æ¡£å†…å®¹å­¦ä¹ 
        instrument_suffixes = [
            r'([^ï¼Œã€‚ã€\s]{1,8})(è¡¨|è®¡|å™¨|ä»ª|ä¼ æ„Ÿå™¨|å˜é€å™¨|æ§åˆ¶å™¨|æ£€æµ‹å™¨|åˆ†æä»ª)',
            r'([^ï¼Œã€‚ã€\s]{1,8})(é˜€|é—¨)',
            r'(æ¸©åº¦|å‹åŠ›|æµé‡|æ¶²ä½|ç‰©ä½|ç•Œé¢|æµ“åº¦|å¯†åº¦|ç²˜åº¦|pH|ç”µå¯¼|æ°§|æ°”ä½“)([^ï¼Œã€‚ã€\s]{0,4})(è¡¨|è®¡|å™¨|ä»ª)',
        ]
        
        instrument_counter = Counter()
        
        for doc in documents:
            for pattern in instrument_suffixes:
                matches = re.finditer(pattern, doc)
                for match in matches:
                    if len(match.groups()) == 2:
                        instrument_type = match.group(1) + match.group(2)
                    else:
                        instrument_type = ''.join(match.groups())
                    
                    if 2 <= len(instrument_type) <= 8 and not re.search(r'[0-9]{3,}', instrument_type):
                        instrument_counter[instrument_type] += 1
        
        # åŠ¨æ€é˜ˆå€¼ - åŸºäºæ–‡æ¡£æ•°é‡
        min_frequency = max(2, len(documents) // 100)
        for instrument, count in instrument_counter.items():
            if count >= min_frequency:
                self.instrument_patterns[instrument] = {
                    'frequency': count,
                    'category': self._infer_instrument_category(instrument),
                    'related_terms': set(),
                    'installation_terms': set()
                }
        
        # ä¸ºæ¯ä¸ªè¯†åˆ«çš„ä»ªè¡¨ç±»å‹æå–ç›¸å…³æœ¯è¯­
        self._extract_related_terms(documents)
    
    def _infer_instrument_category(self, instrument_type: str) -> str:
        """åŠ¨æ€æ¨æ–­ä»ªè¡¨ç±»å‹çš„å¤§ç±»"""
        category_keywords = {
            'æ¸©åº¦': ['æ¸©åº¦', 'çƒ­', 'å†·', 'æ¸©'],
            'å‹åŠ›': ['å‹åŠ›', 'å‹å¼º', 'çœŸç©º', 'å·®å‹'],
            'æµé‡': ['æµé‡', 'æµé€Ÿ', 'æµä½“', 'ä»‹è´¨'],
            'æ¶²ä½': ['æ¶²ä½', 'ç‰©ä½', 'ç•Œé¢', 'æ¶²é¢', 'æ–™ä½'],
            'åˆ†æ': ['pH', 'æ°§', 'æµ“åº¦', 'å¯†åº¦', 'ç²˜åº¦', 'ç”µå¯¼', 'åˆ†æ'],
            'æ§åˆ¶': ['é˜€', 'é—¨', 'æ§åˆ¶', 'è°ƒèŠ‚', 'æ‰§è¡Œ']
        }
        
        for category, keywords in category_keywords.items():
            if any(keyword in instrument_type for keyword in keywords):
                return category
        
        return 'å…¶ä»–'
    
    def _extract_related_terms(self, documents: List[str]):
        """ä¸ºæ¯ä¸ªä»ªè¡¨ç±»å‹æå–ç›¸å…³æœ¯è¯­"""
        for instrument_type in self.instrument_patterns.keys():
            related_terms = set()
            installation_terms = set()
            
            for doc in documents:
                if instrument_type in doc:
                    try:
                        words = pseg.cut(doc)
                        word_list = [(word.word, word.flag) for word in words]
                        
                        for i, (word, flag) in enumerate(word_list):
                            if instrument_type in word:
                                context_range = 5
                                start = max(0, i - context_range)
                                end = min(len(word_list), i + context_range + 1)
                                
                                for j in range(start, end):
                                    if j != i:
                                        context_word, context_flag = word_list[j]
                                        if len(context_word) >= 2 and context_flag in ['n', 'vn', 'v']:
                                            if any(keyword in context_word for keyword in ['å®‰è£…', 'ä½ç½®', 'é«˜åº¦']):
                                                installation_terms.add(context_word)
                                            else:
                                                related_terms.add(context_word)
                    except:
                        # å¦‚æœjiebaå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²å¤„ç†
                        words = doc.replace(instrument_type, ' ').split()
                        for word in words[:10]:  # å–å‰10ä¸ªè¯
                            if len(word) >= 2:
                                related_terms.add(word)
            
            if instrument_type in self.instrument_patterns:
                self.instrument_patterns[instrument_type]['related_terms'] = related_terms
                self.instrument_patterns[instrument_type]['installation_terms'] = installation_terms
    
    def _build_semantic_clusters(self, documents: List[str]):
        """æ„å»ºè¯­ä¹‰èšç±»"""
        semantic_seeds = {
            'å®‰è£…': ['å®‰è£…', 'è£…é…', 'è®¾ç½®', 'å›ºå®š'],
            'ææ–™': ['ææ–™', 'æè´¨', 'é‡‘å±', 'é’¢'],
            'è¿æ¥': ['è¿æ¥', 'é…ç®¡', 'é…çº¿', 'æ¥çº¿'],
            'æµ‹é‡': ['æµ‹é‡', 'æ£€æµ‹', 'ç›‘æµ‹'],
            'ç»´æŠ¤': ['ç»´æŠ¤', 'ä¿å…»', 'ç»´ä¿®'],
            'å®‰å…¨': ['å®‰å…¨', 'é˜²æŠ¤', 'ä¿æŠ¤']
        }
        
        for category, seeds in semantic_seeds.items():
            expanded_terms = set(seeds)
            for doc in documents:
                for seed in seeds:
                    if seed in doc:
                        try:
                            words = list(jieba.cut(doc))
                            for i, word in enumerate(words):
                                if seed in word:
                                    for j in range(max(0, i-2), min(len(words), i+3)):
                                        if j != i and len(words[j]) >= 2:
                                            expanded_terms.add(words[j])
                        except:
                            pass
            
            self.semantic_clusters[category] = list(expanded_terms)[:15]
    
    def _generate_document_statistics(self, documents: List[str]):
        """ç”Ÿæˆæ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        self.document_statistics = {
            'total_documents': len(documents),
            'instrument_types_count': len(self.instrument_patterns),
            'semantic_categories': list(self.semantic_clusters.keys())
        }
    
    def _save_learned_knowledge(self):
        """ä¿å­˜å­¦ä¹ åˆ°çš„çŸ¥è¯†"""
        knowledge = {
            'instrument_patterns': {k: {**v, 'related_terms': list(v['related_terms']), 
                                       'installation_terms': list(v['installation_terms'])} 
                                   for k, v in self.instrument_patterns.items()},
            'semantic_clusters': self.semantic_clusters,
            'document_statistics': self.document_statistics
        }
        
        os.makedirs(os.path.dirname(self.knowledge_cache_path), exist_ok=True)
        with open(self.knowledge_cache_path, 'w', encoding='utf-8') as f:
            json.dump(knowledge, f, ensure_ascii=False, indent=2)
    
    def _load_cached_knowledge(self):
        """åŠ è½½ç¼“å­˜çš„çŸ¥è¯†"""
        if os.path.exists(self.knowledge_cache_path):
            try:
                with open(self.knowledge_cache_path, 'r', encoding='utf-8') as f:
                    knowledge = json.load(f)
                
                self.instrument_patterns = {}
                for k, v in knowledge.get('instrument_patterns', {}).items():
                    self.instrument_patterns[k] = {
                        **v,
                        'related_terms': set(v.get('related_terms', [])),
                        'installation_terms': set(v.get('installation_terms', []))
                    }
                
                self.semantic_clusters = knowledge.get('semantic_clusters', {})
                self.document_statistics = knowledge.get('document_statistics', {})
                
                logger.info(f"å·²åŠ è½½ç¼“å­˜çŸ¥è¯†ï¼ŒåŒ…å« {len(self.instrument_patterns)} ç§ä»ªè¡¨ç±»å‹")
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜çŸ¥è¯†å¤±è´¥: {e}ï¼Œå°†é‡æ–°å­¦ä¹ ")
                self._auto_learn_from_documents()
        else:
            self._auto_learn_from_documents()
    
    def auto_identify_instrument_type(self, query: str) -> Optional[str]:
        """è‡ªåŠ¨è¯†åˆ«æŸ¥è¯¢ä¸­çš„ä»ªè¡¨ç±»å‹"""
        best_match = None
        best_score = 0
        
        for instrument_type in self.instrument_patterns.keys():
            if instrument_type in query:
                score = len(instrument_type) / len(query) + 1.0
                if score > best_score:
                    best_score = score
                    best_match = instrument_type
        
        return best_match if best_score > 0.1 else None
    
    def adaptive_query_enhancement(self, query: str, instrument_type: str = None) -> List[str]:
        """è‡ªé€‚åº”æŸ¥è¯¢å¢å¼º"""
        enhanced_queries = [query]
        
        if not instrument_type:
            instrument_type = self.auto_identify_instrument_type(query)
        
        if instrument_type and instrument_type in self.instrument_patterns:
            pattern = self.instrument_patterns[instrument_type]
            
            # åŸºäºå­¦ä¹ åˆ°çš„ç›¸å…³æœ¯è¯­æ‰©å±•æŸ¥è¯¢
            for term in list(pattern['related_terms'])[:3]:
                enhanced_queries.append(f"{instrument_type} {term}")
            
            # åŸºäºå®‰è£…æœ¯è¯­æ‰©å±•
            if any(install_word in query for install_word in ['å®‰è£…', 'ä½ç½®', 'æ–¹æ³•']):
                for term in list(pattern['installation_terms'])[:2]:
                    enhanced_queries.append(f"{instrument_type} {term}")
        
        # å»é‡
        unique_queries = []
        seen = set()
        for q in enhanced_queries:
            if q.lower() not in seen:
                unique_queries.append(q)
                seen.add(q.lower())
        
        return unique_queries[:5]
    
    def get_instrument_types_summary(self) -> Dict:
        """è·å–å­¦ä¹ åˆ°çš„ä»ªè¡¨ç±»å‹æ€»ç»“"""
        summary = {
            'total_types': len(self.instrument_patterns),
            'categories': {},
            'top_types': []
        }
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        for instrument, pattern in self.instrument_patterns.items():
            category = pattern['category']
            if category not in summary['categories']:
                summary['categories'][category] = []
            summary['categories'][category].append({
                'name': instrument,
                'frequency': pattern['frequency']
            })
        
        # æŒ‰é¢‘æ¬¡æ’åº
        sorted_instruments = sorted(
            self.instrument_patterns.items(),
            key=lambda x: x[1]['frequency'],
            reverse=True
        )
        
        summary['top_types'] = [
            {'name': name, 'frequency': pattern['frequency'], 'category': pattern['category']}
            for name, pattern in sorted_instruments[:10]
        ]
        
        return summary

def test_adaptive_retriever():
    """æµ‹è¯•è‡ªé€‚åº”æ£€ç´¢å™¨"""
    print("ğŸš€ æµ‹è¯•è‡ªé€‚åº”RAGæ£€ç´¢å™¨ï¼ˆé›¶ç¡¬ç¼–ç ï¼‰")
    print("=" * 60)
    
    try:
        adaptive_retriever = AdaptiveRAGRetriever()
        
        summary = adaptive_retriever.get_instrument_types_summary()
        print(f"\nğŸ“Š è‡ªåŠ¨å­¦ä¹ ç»“æœ:")
        print(f"   è¯†åˆ«ä»ªè¡¨ç±»å‹æ€»æ•°: {summary['total_types']}ç§")
        print(f"   è¯†åˆ«ç±»åˆ«æ•°: {len(summary['categories'])}ä¸ª")
        
        print(f"\nğŸ† é¢‘æ¬¡æœ€é«˜çš„10ç§ä»ªè¡¨ç±»å‹:")
        for i, instrument in enumerate(summary['top_types'], 1):
            print(f"   {i}. {instrument['name']} (ç±»åˆ«: {instrument['category']}, é¢‘æ¬¡: {instrument['frequency']})")
        
        print(f"\nğŸ“‹ å„ç±»åˆ«åˆ†å¸ƒ:")
        for category, instruments in summary['categories'].items():
            print(f"   {category}: {len(instruments)}ç§")
        
        # æµ‹è¯•è‡ªåŠ¨è¯†åˆ«
        test_queries = [
            "çƒ­ç”µå¶å®‰è£…è¦æ±‚",
            "å‹åŠ›å˜é€å™¨ææ–™", 
            "ç”µç£æµé‡è®¡ä½ç½®",
            "æ–°å‹æ™ºèƒ½ä»ªè¡¨å®‰è£…"
        ]
        
        print(f"\nğŸ” æµ‹è¯•è‡ªåŠ¨è¯†åˆ«å’ŒæŸ¥è¯¢å¢å¼º:")
        for query in test_queries:
            identified_type = adaptive_retriever.auto_identify_instrument_type(query)
            enhanced_queries = adaptive_retriever.adaptive_query_enhancement(query)
            
            print(f"\n   æŸ¥è¯¢: '{query}'")
            print(f"   è¯†åˆ«ç±»å‹: {identified_type or 'æœªè¯†åˆ«'}")
            print(f"   å¢å¼ºæŸ¥è¯¢: {enhanced_queries}")
        
        print("\nâœ… è‡ªé€‚åº”æ£€ç´¢å™¨æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_adaptive_retriever()
 