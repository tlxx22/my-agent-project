"""
æ‰¹é‡è¯„ä¼°æµ‹è¯•è„šæœ¬ - å¯¹15ç¯‡å®‰è£…æ¨èè¿›è¡Œç»¼åˆè¯„ä¼°
"""

import re
import os
from comprehensive_evaluation_metrics import integrate_comprehensive_metrics
from datetime import datetime
import json

def read_test_file_with_encoding(file_path):
    """å°è¯•å¤šç§ç¼–ç æ–¹å¼è¯»å–æ–‡ä»¶"""
    encodings = ['utf-8', 'gbk', 'gb2312', 'cp936', 'unicode_escape']
    
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                content = f.read()
            print(f"âœ… æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶")
            return content
        except UnicodeDecodeError:
            continue
        except Exception as e:
            print(f"âŒ ä½¿ç”¨ {encoding} ç¼–ç æ—¶å‡ºé”™: {e}")
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•å¿½ç•¥é”™è¯¯
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        print("âš ï¸ ä½¿ç”¨ utf-8 ç¼–ç å¹¶å¿½ç•¥é”™è¯¯è¯»å–æ–‡ä»¶")
        return content
    except Exception as e:
        print(f"âŒ å®Œå…¨æ— æ³•è¯»å–æ–‡ä»¶: {e}")
        return None

def parse_documents_from_content(content):
    """ä»æ–‡ä»¶å†…å®¹ä¸­è§£æå‡ºä¸åŒè´¨é‡ç­‰çº§çš„æ–‡æ¡£"""
    if not content:
        return []
    
    # å®šä¹‰è´¨é‡æ ‡è¯†ç¬¦çš„å¤šç§å¯èƒ½æ ¼å¼
    quality_patterns = [
        r'ã€é«˜è´¨é‡ã€‘',
        r'ã€ä¸­è´¨é‡ã€‘', 
        r'ã€ä½è´¨é‡ã€‘',
        r'\[é«˜è´¨é‡\]',
        r'\[ä¸­è´¨é‡\]',
        r'\[ä½è´¨é‡\]',
        r'é«˜è´¨é‡',
        r'ä¸­è´¨é‡', 
        r'ä½è´¨é‡'
    ]
    
    documents = []
    
    # å°è¯•æŒ‰æ®µè½åˆ†å‰²
    paragraphs = re.split(r'\n\s*\n', content)
    
    current_doc = ""
    current_quality = None
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph:
            continue
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è´¨é‡æ ‡è¯†
        quality_found = None
        for pattern in quality_patterns:
            if re.search(pattern, paragraph):
                if 'é«˜è´¨é‡' in paragraph or 'é«˜è´¨é‡' in pattern:
                    quality_found = 'é«˜è´¨é‡'
                elif 'ä¸­è´¨é‡' in paragraph or 'ä¸­è´¨é‡' in pattern:
                    quality_found = 'ä¸­è´¨é‡'
                elif 'ä½è´¨é‡' in paragraph or 'ä½è´¨é‡' in pattern:
                    quality_found = 'ä½è´¨é‡'
                break
        
        if quality_found:
            # ä¿å­˜ä¹‹å‰çš„æ–‡æ¡£
            if current_doc and current_quality:
                documents.append({
                    'quality': current_quality,
                    'content': current_doc.strip()
                })
            
            # å¼€å§‹æ–°æ–‡æ¡£
            current_quality = quality_found
            current_doc = paragraph
        else:
            # ç»§ç»­å½“å‰æ–‡æ¡£
            if current_doc:
                current_doc += "\n\n" + paragraph
            else:
                current_doc = paragraph
    
    # ä¿å­˜æœ€åä¸€ä¸ªæ–‡æ¡£
    if current_doc and current_quality:
        documents.append({
            'quality': current_quality,
            'content': current_doc.strip()
        })
    
    return documents

def create_sample_documents():
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç”¨äºæµ‹è¯•"""
    sample_docs = []
    
    # é«˜è´¨é‡æ–‡æ¡£ç¤ºä¾‹
    for i in range(1, 6):
        high_quality_doc = f"""
ã€é«˜è´¨é‡ã€‘æ¸©åº¦ä»ªè¡¨å®‰è£…æ¨èæ–¹æ¡ˆ {i}

## å®‰è£…ä½ç½®é€‰æ‹©
æ¸©åº¦ä»ªè¡¨åº”å®‰è£…åœ¨èƒ½å¤Ÿå‡†ç¡®åæ˜ å·¥è‰ºæ¸©åº¦çš„ä»£è¡¨æ€§ä½ç½®ã€‚é€‰æ‹©å®‰è£…ä½ç½®æ—¶åº”è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
- æµ‹é‡å‡†ç¡®æ€§ï¼šé€‰æ‹©èƒ½å¤ŸçœŸå®åæ˜ è¢«æµ‹ä»‹è´¨æ¸©åº¦çš„å…¸å‹ä½ç½®ï¼Œé¿å…é è¿‘åŠ çƒ­å™¨ã€å†·å´å™¨ç­‰çƒ­æº
- æ’å…¥æ·±åº¦ï¼šç¡®ä¿ä¼ æ„Ÿå™¨æœ‰è¶³å¤Ÿçš„æ’å…¥æ·±åº¦ï¼Œä¸€èˆ¬ä¸å°äºç®¡å¾„çš„1/3ï¼Œé¿å…ç®¡å£å¯¼çƒ­å½±å“
- å®‰è£…é«˜åº¦ï¼šè·ç¦»åœ°é¢1.2-1.8ç±³ï¼Œä¾¿äºè§‚å¯Ÿå’Œç»´æŠ¤
- ç¯å¢ƒæ¡ä»¶ï¼šé¿å¼€å¼ºè…èš€ã€å¼ºæŒ¯åŠ¨åŒºåŸŸ

## å®‰è£…æ–¹å¼ä¸æ­¥éª¤
1. å‰æœŸå‡†å¤‡ï¼šç¡®è®¤ä»ªè¡¨å‹å·ã€é‡ç¨‹ã€ç²¾åº¦ç­‰çº§ï¼Œå‡†å¤‡å®‰è£…å·¥å…·å’Œææ–™
2. å¼€å­”å®‰è£…ï¼šåœ¨é¢„å®šä½ç½®å¼€å­”ï¼Œå®‰è£…çƒ­ç”µå¶å¥—ç®¡æˆ–ç›´æ¥å®‰è£…ä¼ æ„Ÿå™¨
3. å¯†å°å¤„ç†ï¼šä½¿ç”¨åˆé€‚å¯†å°ææ–™ï¼Œç¡®ä¿æ— æ³„æ¼
4. ç”µæ°”è¿æ¥ï¼šæŒ‰ç…§æ¥çº¿å›¾æ­£ç¡®è¿æ¥ä¿¡å·çº¿å’Œç”µæºçº¿
5. è°ƒè¯•éªŒæ”¶ï¼šè¿›è¡Œé›¶ç‚¹å’Œé‡ç¨‹æ ¡å‡†ï¼ŒéªŒè¯æµ‹é‡ç²¾åº¦

## ææ–™æ¸…å•
- æ¸©åº¦ä¼ æ„Ÿå™¨ï¼šPt100ï¼Œç²¾åº¦ç­‰çº§Açº§ï¼Œ-50~200Â°C
- ä¿æŠ¤å¥—ç®¡ï¼šä¸é”ˆé’¢316Lï¼Œé•¿åº¦200mm
- å¯†å°ä»¶ï¼šPTFEå¯†å°åœˆï¼Œè€æ¸©250Â°C
- ä¿¡å·ç”µç¼†ï¼š4èŠ¯å±è”½ç”µç¼†ï¼Œ2.5mmÂ²
- å®‰è£…é™„ä»¶ï¼šæ³•å…°ã€èºæ “ã€å«ç‰‡ç­‰

## å®‰å…¨è¦æ±‚
- æ–½å·¥å®‰å…¨ï¼šæ–­ç”µæ–½å·¥ï¼Œä½¿ç”¨ç»ç¼˜å·¥å…·
- é˜²çˆ†è¦æ±‚ï¼šé€‰ç”¨é˜²çˆ†å‹ä»ªè¡¨ï¼Œç¬¦åˆç°åœºé˜²çˆ†ç­‰çº§
- äººå‘˜é˜²æŠ¤ï¼šä½©æˆ´å®‰å…¨å¸½ã€ç»ç¼˜æ‰‹å¥—
- éªŒæ”¶æ ‡å‡†ï¼šç²¾åº¦è¯¯å·®â‰¤Â±0.5%ï¼Œç»ç¼˜ç”µé˜»â‰¥20MÎ©
        """
        sample_docs.append({
            'quality': 'é«˜è´¨é‡',
            'content': high_quality_doc.strip()
        })
    
    # ä¸­è´¨é‡æ–‡æ¡£ç¤ºä¾‹
    for i in range(1, 6):
        medium_quality_doc = f"""
ã€ä¸­è´¨é‡ã€‘å‹åŠ›ä»ªè¡¨å®‰è£…æŒ‡å— {i}

## å®‰è£…ä½ç½®
å‹åŠ›ä»ªè¡¨åº”å®‰è£…åœ¨èƒ½å¤Ÿå‡†ç¡®æµ‹é‡å·¥è‰ºå‹åŠ›çš„ä½ç½®ï¼š
- é€‰æ‹©å‹åŠ›ç¨³å®šçš„ç›´ç®¡æ®µ
- è·ç¦»å¼¯å¤´5å€ç®¡å¾„ä»¥ä¸Š
- ä¾¿äºè§‚å¯Ÿå’Œç»´æŠ¤çš„é«˜åº¦
- é¿å¼€æŒ¯åŠ¨å’Œé«˜æ¸©åŒºåŸŸ

## å®‰è£…æ­¥éª¤
1. ç¡®è®¤ä»ªè¡¨è§„æ ¼å’Œå–å‹ç‚¹ä½ç½®
2. å®‰è£…å–å‹é˜€é—¨å’Œå‹åŠ›è¡¨
3. è¿›è¡Œå¯†å°å’Œæ¥çº¿
4. æ ¡éªŒå’Œè°ƒè¯•

## æ‰€éœ€ææ–™
- å‹åŠ›è¡¨ï¼šé‡ç¨‹0-1.6MPaï¼Œç²¾åº¦1.6çº§
- å–å‹é˜€ï¼šçƒé˜€DN15
- å¯†å°ä»¶ï¼šçŸ³å¢¨å«ç‰‡
- è¿æ¥ç®¡ï¼šä¸é”ˆé’¢ç®¡Ï†14Ã—2

## å®‰å…¨æ³¨æ„äº‹é¡¹
- å®‰è£…å‰ç¡®è®¤ç³»ç»Ÿæ— å‹åŠ›
- ä½¿ç”¨åˆé€‚çš„å¯†å°ææ–™
- å®šæœŸæ£€æŸ¥å’Œæ ¡éªŒ
        """
        sample_docs.append({
            'quality': 'ä¸­è´¨é‡', 
            'content': medium_quality_doc.strip()
        })
    
    # ä½è´¨é‡æ–‡æ¡£ç¤ºä¾‹
    for i in range(1, 6):
        low_quality_doc = f"""
ã€ä½è´¨é‡ã€‘ä»ªè¡¨å®‰è£…è¯´æ˜ {i}

å®‰è£…ä½ç½®ï¼šè£…åœ¨ç®¡é“ä¸Š
å®‰è£…æ–¹æ³•ï¼šç”¨èºæ “å›ºå®š
ææ–™ï¼šä»ªè¡¨ã€èºæ “ã€å«ç‰‡
æ³¨æ„å®‰å…¨ã€‚
        """
        sample_docs.append({
            'quality': 'ä½è´¨é‡',
            'content': low_quality_doc.strip()
        })
    
    return sample_docs

def evaluate_documents(documents):
    """å¯¹æ–‡æ¡£åˆ—è¡¨è¿›è¡Œæ‰¹é‡è¯„ä¼°"""
    
    print("ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼°æµ‹è¯•")
    print("=" * 80)
    
    # ç»Ÿè®¡ç»“æœ
    results = []
    quality_stats = {'é«˜è´¨é‡': [], 'ä¸­è´¨é‡': [], 'ä½è´¨é‡': []}
    
    for i, doc in enumerate(documents, 1):
        print(f"\nğŸ“‹ è¯„ä¼°æ–‡æ¡£ {i}: {doc['quality']}")
        print("-" * 60)
        
        try:
            # è¿›è¡Œç»¼åˆè¯„ä¼°
            result = integrate_comprehensive_metrics(doc['content'])
            
            # æå–å…³é”®æŒ‡æ ‡
            comprehensive_score = result['comprehensive_score']
            comprehensive_level = result['comprehensive_level']
            coverage_score = result['content_coverage']['overall_coverage_score']
            usability_score = result['usability_operability']['usability_score']
            quality_score = result['quality_review']['aggregated'].get('overall_quality_score', 0)
            
            # ä¿å­˜ç»“æœ
            doc_result = {
                'doc_index': i,
                'expected_quality': doc['quality'],
                'comprehensive_score': comprehensive_score,
                'comprehensive_level': comprehensive_level,
                'coverage_score': coverage_score,
                'usability_score': usability_score,
                'quality_score': quality_score,
                'content_preview': doc['content'][:100] + "..."
            }
            
            results.append(doc_result)
            quality_stats[doc['quality']].append(doc_result)
            
            # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
            print(f"ğŸ¯ ç»¼åˆå¾—åˆ†: {comprehensive_score:.1f}/100 ({comprehensive_level})")
            print(f"ğŸ“Š å†…å®¹è¦†ç›–: {coverage_score:.1f}/100 (æƒé‡20%)")
            print(f"ğŸ”§ å¯ç”¨æ€§: {usability_score:.1f}/100 (æƒé‡45%)")
            print(f"ğŸ‘¨â€ğŸ”¬ è´¨é‡è¯„å®¡: {quality_score:.1f}/100 (æƒé‡35%)")
            
            # æƒé‡è´¡çŒ®åˆ†æ
            coverage_contrib = coverage_score * 0.20
            usability_contrib = usability_score * 0.45
            quality_contrib = quality_score * 0.35
            
            print(f"ğŸ’¡ æƒé‡è´¡çŒ®:")
            print(f"  ğŸ“Š å†…å®¹è¦†ç›–: {coverage_contrib:.1f}åˆ†")
            print(f"  ğŸ”§ å¯ç”¨æ€§: {usability_contrib:.1f}åˆ†")
            print(f"  ğŸ‘¨â€ğŸ”¬ è´¨é‡è¯„å®¡: {quality_contrib:.1f}åˆ†")
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°æ–‡æ¡£ {i} æ—¶å‡ºé”™: {str(e)}")
            continue
    
    return results, quality_stats

def calculate_quality_level_stats(quality_stats):
    """è®¡ç®—å„è´¨é‡å±‚çº§çš„ç»Ÿè®¡æŒ‡æ ‡"""
    
    print(f"\nğŸ“Š å„è´¨é‡å±‚çº§ç»Ÿè®¡åˆ†æ")
    print("=" * 80)
    
    for quality_level, docs in quality_stats.items():
        if not docs:
            print(f"\nâš ï¸ {quality_level}æ–‡æ¡£ï¼šæ— æœ‰æ•ˆæ•°æ®")
            continue
            
        # è®¡ç®—å„æŒ‡æ ‡çš„å¹³å‡å€¼
        avg_comprehensive = sum(doc['comprehensive_score'] for doc in docs) / len(docs)
        avg_coverage = sum(doc['coverage_score'] for doc in docs) / len(docs)
        avg_usability = sum(doc['usability_score'] for doc in docs) / len(docs)
        avg_quality = sum(doc['quality_score'] for doc in docs) / len(docs)
        
        # åŠ æƒå¹³å‡ç»¼åˆå¾—åˆ†éªŒè¯
        weighted_avg = avg_coverage * 0.20 + avg_usability * 0.45 + avg_quality * 0.35
        
        print(f"\nğŸ“‹ {quality_level}æ–‡æ¡£ (å…±{len(docs)}ç¯‡)")
        print("-" * 40)
        print(f"ğŸ¯ å¹³å‡ç»¼åˆå¾—åˆ†: {avg_comprehensive:.1f}/100")
        print(f"ğŸ“Š å¹³å‡å†…å®¹è¦†ç›–: {avg_coverage:.1f}/100")
        print(f"ğŸ”§ å¹³å‡å¯ç”¨æ€§: {avg_usability:.1f}/100")
        print(f"ğŸ‘¨â€ğŸ”¬ å¹³å‡è´¨é‡è¯„å®¡: {avg_quality:.1f}/100")
        print(f"âœ… åŠ æƒå¹³å‡éªŒè¯: {weighted_avg:.1f}/100")
        
        # æ˜¾ç¤ºå¾—åˆ†åˆ†å¸ƒ
        scores = [doc['comprehensive_score'] for doc in docs]
        print(f"ğŸ“ˆ å¾—åˆ†åˆ†å¸ƒ: æœ€é«˜{max(scores):.1f}, æœ€ä½{min(scores):.1f}, æ ‡å‡†å·®{(sum((s-avg_comprehensive)**2 for s in scores)/len(scores))**0.5:.1f}")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ” æ‰¹é‡è¯„ä¼°æµ‹è¯•è„šæœ¬")
    print("æµ‹è¯•ç›®æ ‡ï¼ševaluation recommendation_test/test.txtä¸­çš„15ç¯‡å®‰è£…æ¨è")
    print("è¯„ä¼°æŒ‡æ ‡ï¼šå†…å®¹è¦†ç›–(20%) + å¯ç”¨æ€§(45%) + è´¨é‡è¯„å®¡(35%)")
    print("=" * 80)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    test_file_path = "recommendation_test/test.txt"
    
    if os.path.exists(test_file_path):
        print(f"ğŸ“ æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶: {test_file_path}")
        
        # å°è¯•è¯»å–æ–‡ä»¶
        content = read_test_file_with_encoding(test_file_path)
        
        if content:
            # è§£ææ–‡æ¡£
            documents = parse_documents_from_content(content)
            print(f"ğŸ“„ è§£æå‡º {len(documents)} ç¯‡æ–‡æ¡£")
            
            if len(documents) < 10:  # å¦‚æœè§£æå‡ºçš„æ–‡æ¡£å¤ªå°‘ï¼Œä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£
                print("âš ï¸ è§£æå‡ºçš„æ–‡æ¡£æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£è¿›è¡Œæµ‹è¯•")
                documents = create_sample_documents()
        else:
            print("âŒ æ— æ³•è¯»å–æµ‹è¯•æ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£")
            documents = create_sample_documents()
    else:
        print(f"âš ï¸ æµ‹è¯•æ–‡ä»¶ {test_file_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£")
        documents = create_sample_documents()
    
    print(f"ğŸ“Š æ€»å…±è¯„ä¼° {len(documents)} ç¯‡æ–‡æ¡£")
    
    # æ˜¾ç¤ºæ–‡æ¡£åˆ†å¸ƒ
    quality_count = {}
    for doc in documents:
        quality = doc['quality']
        quality_count[quality] = quality_count.get(quality, 0) + 1
    
    print("ğŸ“ˆ æ–‡æ¡£è´¨é‡åˆ†å¸ƒ:")
    for quality, count in quality_count.items():
        print(f"  {quality}: {count}ç¯‡")
    
    # æ‰§è¡Œæ‰¹é‡è¯„ä¼°
    results, quality_stats = evaluate_documents(documents)
    
    # è®¡ç®—è´¨é‡å±‚çº§ç»Ÿè®¡
    calculate_quality_level_stats(quality_stats)
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"batch_evaluation_results_{timestamp}.json"
    
    output_data = {
        "evaluation_time": timestamp,
        "total_documents": len(documents),
        "quality_distribution": quality_count,
        "evaluation_results": results,
        "quality_level_stats": {
            quality: {
                "count": len(docs),
                "avg_comprehensive": sum(doc['comprehensive_score'] for doc in docs) / len(docs) if docs else 0,
                "avg_coverage": sum(doc['coverage_score'] for doc in docs) / len(docs) if docs else 0,
                "avg_usability": sum(doc['usability_score'] for doc in docs) / len(docs) if docs else 0,
                "avg_quality": sum(doc['quality_score'] for doc in docs) / len(docs) if docs else 0
            }
            for quality, docs in quality_stats.items()
        }
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("\nğŸ‰ æ‰¹é‡è¯„ä¼°æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main() 