"""
RAGå¢å¼ºæ¨¡å—æ¶ˆèä¸å¯¹æ¯”å®éªŒ
å®éªŒç›®çš„ï¼šæ¯”è¾ƒæ™ºèƒ½é‡æ’åºRAG vs æ ‡å‡†RAG vs æ— RAGçš„æ€§èƒ½å·®å¼‚
"""

import json
import random
import time
from datetime import datetime
from typing import List, Dict, Any
import os
import sys

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from comprehensive_evaluation_metrics import integrate_comprehensive_metrics
from tools.enhanced_rag_retriever import EnhancedRAGRetriever
from tools.generate_installation_recommendation import generate_installation_recommendation
from config.settings import get_openai_config
from langchain_openai import ChatOpenAI

class RAGAblationExperiment:
    """RAGæ¶ˆèå®éªŒç±»"""
    
    def __init__(self):
        # åˆå§‹åŒ–LLM
        openai_config = get_openai_config()
        self.llm = ChatOpenAI(
            model=openai_config["model"],
            openai_api_key=openai_config["api_key"],
            openai_api_base=openai_config["base_url"],
            temperature=0.1
        )
        self.enhanced_rag = EnhancedRAGRetriever()
        
        # å®éªŒé…ç½®
        self.experiment_name = "RAGå¢å¼ºæ¨¡å—æ¶ˆèå¯¹æ¯”å®éªŒ"
        self.total_samples = 10
        
        # å®éªŒç»„é…ç½®
        self.experiment_groups = {
            "enhanced_rag": {
                "name": "æ™ºèƒ½é‡æ’åºRAG",
                "description": "ä½¿ç”¨å¢å¼ºRAGæ£€ç´¢å™¨ï¼ŒåŒ…å«æ™ºèƒ½é‡æ’åºå’Œå¤šæ®µæ‹¼æ¥",
                "use_rag": True,
                "use_enhanced": True
            },
            "standard_rag": {
                "name": "æ ‡å‡†RAG", 
                "description": "ä½¿ç”¨åŸºç¡€RAGæ£€ç´¢ï¼Œæ— é‡æ’åºä¼˜åŒ–",
                "use_rag": True,
                "use_enhanced": False
            },
            "no_rag": {
                "name": "æ— RAG",
                "description": "ä»…ä½¿ç”¨LLMï¼Œä¸è¿›è¡ŒçŸ¥è¯†æ£€ç´¢",
                "use_rag": False,
                "use_enhanced": False
            }
        }
        
        # æµ‹è¯•ä»ªè¡¨æ ·æœ¬
        self.test_instruments = [
            "æ¸©åº¦å˜é€å™¨TT-101",
            "å‹åŠ›è¡¨PI-201", 
            "æµé‡è®¡FT-301",
            "æ¶²ä½è®¡LT-401",
            "pHè®¡AT-501",
            "ç”µå¯¼ç‡è®¡CT-601",
            "æ°§åˆ†æä»ªOT-701",
            "çƒ­ç”µå¶TE-801",
            "å‹åŠ›å˜é€å™¨PT-901",
            "å·®å‹å˜é€å™¨DPT-111",
            "æ¶¡è¡—æµé‡è®¡VFT-121",
            "ç”µç£æµé‡è®¡EFT-131",
            "é›·è¾¾æ¶²ä½è®¡RLT-141",
            "è¶…å£°æ³¢æ¶²ä½è®¡ULT-151",
            "å¯ç‡ƒæ°”ä½“æ£€æµ‹å™¨GT-161",
            "è°ƒèŠ‚é˜€CV-171",
            "ç”µç£é˜€SV-181",
            "å®‰å…¨é˜€PSV-191",
            "æ­¢å›é˜€CV-201",
            "çƒé˜€BV-211"
        ]
        
        # éšæœºé€‰æ‹©æµ‹è¯•æ ·æœ¬
        random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°
        self.selected_instruments = random.sample(self.test_instruments, self.total_samples)
        
        print(f"ğŸ”¬ åˆå§‹åŒ–{self.experiment_name}")
        print(f"ğŸ“Š å®éªŒæ ·æœ¬æ•°: {self.total_samples}")
        print(f"ğŸ¯ å®éªŒç»„æ•°: {len(self.experiment_groups)}")
        print(f"ğŸ“‹ é€‰ä¸­çš„æµ‹è¯•ä»ªè¡¨: {', '.join(self.selected_instruments[:5])}...")

    def generate_recommendation_enhanced_rag(self, instrument_desc: str) -> str:
        """ä½¿ç”¨å¢å¼ºRAGç”Ÿæˆæ¨è"""
        try:
            # ä½¿ç”¨å¢å¼ºRAGæ£€ç´¢å™¨
            retrieval_result = self.enhanced_rag.enhanced_retrieve(
                query=instrument_desc,
                top_k=5
            )
            
            # æ„å»ºå¢å¼ºæç¤º
            context_parts = []
            for doc in retrieval_result['documents']:
                context_parts.append(f"å‚è€ƒèµ„æ–™ï¼š{doc['content']}")
            
            enhanced_context = "\n\n".join(context_parts)
            
            # å¤šæ®µæ‹¼æ¥æç¤ºæ¨¡æ¿
            prompt = f"""
è¯·åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™ï¼Œä¸ºä»ªè¡¨"{instrument_desc}"ç”Ÿæˆä¸“ä¸šçš„å®‰è£…æ¨èæ–¹æ¡ˆã€‚

{enhanced_context}

è¦æ±‚ï¼š
1. ç»“åˆå‚è€ƒèµ„æ–™ä¸­çš„ç›¸å…³å†…å®¹
2. åŒ…å«å®‰è£…ä½ç½®é€‰æ‹©ã€å®‰è£…æ­¥éª¤ã€ææ–™æ¸…å•ã€å®‰å…¨è¦æ±‚ç­‰
3. å†…å®¹ä¸“ä¸šã€è¯¦ç»†ã€å¯æ“ä½œ
4. å­—æ•°æ§åˆ¶åœ¨800-1200å­—

è¯·ç”Ÿæˆå®‰è£…æ¨èï¼š
"""
            
            response = self.llm.invoke(prompt)
            return response.content if hasattr(response, 'content') else str(response)
            
        except Exception as e:
            print(f"âŒ å¢å¼ºRAGç”Ÿæˆå¤±è´¥: {e}")
            return f"å¢å¼ºRAGç”Ÿæˆå¤±è´¥: {str(e)}"

    def generate_recommendation_standard_rag(self, instrument_desc: str) -> str:
        """ä½¿ç”¨æ ‡å‡†RAGç”Ÿæˆæ¨è"""
        try:
            # ä½¿ç”¨åŸºç¡€æ£€ç´¢ï¼ˆä¸å«é‡æ’åºï¼‰
            retrieval_result = self.enhanced_rag.basic_retrieve(
                query=instrument_desc,
                top_k=3  # æ ‡å‡†RAGä½¿ç”¨è¾ƒå°‘çš„æ£€ç´¢ç»“æœ
            )
            
            # ç®€å•æ‹¼æ¥
            if retrieval_result and len(retrieval_result) > 0:
                context = "\n".join([doc.page_content for doc in retrieval_result])
            else:
                context = "æœªæ‰¾åˆ°ç›¸å…³å‚è€ƒèµ„æ–™"
            
            # æ ‡å‡†RAGæç¤ºæ¨¡æ¿
            prompt = f"""
å‚è€ƒèµ„æ–™ï¼š
{context}

è¯·ä¸ºä»ªè¡¨"{instrument_desc}"ç”Ÿæˆå®‰è£…æ¨èæ–¹æ¡ˆï¼ŒåŒ…æ‹¬å®‰è£…ä½ç½®ã€æ­¥éª¤ã€ææ–™å’Œå®‰å…¨è¦æ±‚ã€‚

å®‰è£…æ¨èï¼š
"""
            
            response = self.llm.invoke(prompt)
            return response.content if hasattr(response, 'content') else str(response)
            
        except Exception as e:
            print(f"âŒ æ ‡å‡†RAGç”Ÿæˆå¤±è´¥: {e}")
            return f"æ ‡å‡†RAGç”Ÿæˆå¤±è´¥: {str(e)}"

    def generate_recommendation_no_rag(self, instrument_desc: str) -> str:
        """ä¸ä½¿ç”¨RAGç”Ÿæˆæ¨è"""
        try:
            # çº¯LLMæç¤º
            prompt = f"""
è¯·ä¸ºä»ªè¡¨"{instrument_desc}"ç”Ÿæˆä¸“ä¸šçš„å®‰è£…æ¨èæ–¹æ¡ˆã€‚

è¦æ±‚åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
1. å®‰è£…ä½ç½®é€‰æ‹©åŸåˆ™
2. å…·ä½“å®‰è£…æ­¥éª¤
3. æ‰€éœ€ææ–™æ¸…å•  
4. å®‰å…¨æ³¨æ„äº‹é¡¹

è¯·åŸºäºæ‚¨çš„ä¸“ä¸šçŸ¥è¯†ç”Ÿæˆè¯¦ç»†çš„å®‰è£…æ¨èï¼š
"""
            
            response = self.llm.invoke(prompt)
            return response.content if hasattr(response, 'content') else str(response)
            
        except Exception as e:
            print(f"âŒ æ— RAGç”Ÿæˆå¤±è´¥: {e}")
            return f"æ— RAGç”Ÿæˆå¤±è´¥: {str(e)}"

    def run_single_experiment(self, instrument: str, group_key: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        group_config = self.experiment_groups[group_key]
        
        print(f"  ğŸ”§ è¿è¡Œ {group_config['name']} - {instrument}")
        
        start_time = time.time()
        
        # æ ¹æ®å®éªŒç»„é…ç½®ç”Ÿæˆæ¨è
        if group_key == "enhanced_rag":
            recommendation = self.generate_recommendation_enhanced_rag(instrument)
        elif group_key == "standard_rag":
            recommendation = self.generate_recommendation_standard_rag(instrument)
        else:  # no_rag
            recommendation = self.generate_recommendation_no_rag(instrument)
        
        generation_time = time.time() - start_time
        
        # ä½¿ç”¨ç»¼åˆè¯„ä¼°ç³»ç»Ÿè¯„ä¼°
        try:
            eval_start = time.time()
            evaluation_result = integrate_comprehensive_metrics(recommendation)
            eval_time = time.time() - eval_start
            
            # æå–å…³é”®æŒ‡æ ‡
            result = {
                "instrument": instrument,
                "group": group_key,
                "group_name": group_config["name"],
                "recommendation": recommendation,
                "generation_time": round(generation_time, 2),
                "evaluation_time": round(eval_time, 2),
                "comprehensive_score": evaluation_result["comprehensive_score"],
                "comprehensive_level": evaluation_result["comprehensive_level"],
                "coverage_score": evaluation_result["content_coverage"]["overall_coverage_score"],
                "usability_score": evaluation_result["usability_operability"]["usability_score"],
                "quality_score": evaluation_result["quality_review"]["aggregated"].get("overall_quality_score", 0),
                "evaluation_success": True
            }
            
            print(f"    âœ… å®Œæˆ - ç»¼åˆå¾—åˆ†: {result['comprehensive_score']:.1f}")
            
        except Exception as e:
            print(f"    âŒ è¯„ä¼°å¤±è´¥: {e}")
            result = {
                "instrument": instrument,
                "group": group_key,
                "group_name": group_config["name"],
                "recommendation": recommendation,
                "generation_time": round(generation_time, 2),
                "evaluation_time": 0,
                "comprehensive_score": 0,
                "comprehensive_level": "è¯„ä¼°å¤±è´¥",
                "coverage_score": 0,
                "usability_score": 0,
                "quality_score": 0,
                "evaluation_success": False,
                "error": str(e)
            }
        
        return result

    def run_full_experiment(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print(f"\nğŸš€ å¼€å§‹{self.experiment_name}")
        print("=" * 80)
        
        all_results = []
        group_stats = {group: [] for group in self.experiment_groups.keys()}
        
        # å¯¹æ¯ä¸ªä»ªè¡¨è¿è¡Œæ‰€æœ‰å®éªŒç»„
        for i, instrument in enumerate(self.selected_instruments, 1):
            print(f"\nğŸ“‹ æµ‹è¯•æ ·æœ¬ {i}/{self.total_samples}: {instrument}")
            print("-" * 60)
            
            for group_key in self.experiment_groups.keys():
                result = self.run_single_experiment(instrument, group_key)
                all_results.append(result)
                
                if result["evaluation_success"]:
                    group_stats[group_key].append(result)
        
        # è®¡ç®—å„ç»„ç»Ÿè®¡æŒ‡æ ‡
        print(f"\nğŸ“Š å®éªŒç»“æœç»Ÿè®¡åˆ†æ")
        print("=" * 80)
        
        summary_stats = {}
        for group_key, results in group_stats.items():
            if not results:
                print(f"\nâš ï¸ {self.experiment_groups[group_key]['name']}: æ— æœ‰æ•ˆæ•°æ®")
                continue
            
            # è®¡ç®—å¹³å‡å€¼
            avg_comprehensive = sum(r["comprehensive_score"] for r in results) / len(results)
            avg_coverage = sum(r["coverage_score"] for r in results) / len(results)
            avg_usability = sum(r["usability_score"] for r in results) / len(results)
            avg_quality = sum(r["quality_score"] for r in results) / len(results)
            avg_gen_time = sum(r["generation_time"] for r in results) / len(results)
            
            # è®¡ç®—æ ‡å‡†å·®
            comp_scores = [r["comprehensive_score"] for r in results]
            std_comprehensive = (sum((x - avg_comprehensive) ** 2 for x in comp_scores) / len(comp_scores)) ** 0.5
            
            stats = {
                "group_name": self.experiment_groups[group_key]["name"],
                "sample_count": len(results),
                "avg_comprehensive": round(avg_comprehensive, 2),
                "avg_coverage": round(avg_coverage, 2),
                "avg_usability": round(avg_usability, 2),
                "avg_quality": round(avg_quality, 2),
                "avg_generation_time": round(avg_gen_time, 2),
                "std_comprehensive": round(std_comprehensive, 2),
                "min_score": min(comp_scores),
                "max_score": max(comp_scores)
            }
            
            summary_stats[group_key] = stats
            
            print(f"\nğŸ“ˆ {stats['group_name']} (æ ·æœ¬æ•°: {stats['sample_count']})")
            print(f"  ğŸ¯ å¹³å‡ç»¼åˆå¾—åˆ†: {stats['avg_comprehensive']} Â± {stats['std_comprehensive']}")
            print(f"  ğŸ“Š å¹³å‡å†…å®¹è¦†ç›–: {stats['avg_coverage']}")
            print(f"  ğŸ”§ å¹³å‡å¯ç”¨æ€§: {stats['avg_usability']}")
            print(f"  ğŸ‘¨â€ğŸ”¬ å¹³å‡è´¨é‡è¯„å®¡: {stats['avg_quality']}")
            print(f"  â±ï¸ å¹³å‡ç”Ÿæˆæ—¶é—´: {stats['avg_generation_time']}ç§’")
            print(f"  ğŸ“ˆ å¾—åˆ†èŒƒå›´: {stats['min_score']:.1f} - {stats['max_score']:.1f}")
        
        # ç»„è£…å®Œæ•´ç»“æœ
        experiment_result = {
            "experiment_info": {
                "name": self.experiment_name,
                "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
                "total_samples": self.total_samples,
                "selected_instruments": self.selected_instruments,
                "experiment_groups": self.experiment_groups
            },
            "detailed_results": all_results,
            "summary_statistics": summary_stats,
            "performance_comparison": self.analyze_performance_comparison(summary_stats)
        }
        
        return experiment_result

    def analyze_performance_comparison(self, summary_stats: Dict) -> Dict:
        """åˆ†ææ€§èƒ½å¯¹æ¯”"""
        if len(summary_stats) < 2:
            return {"analysis": "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”åˆ†æ"}
        
        # æå–å„ç»„çš„ç»¼åˆå¾—åˆ†
        scores = {}
        for group_key, stats in summary_stats.items():
            scores[group_key] = stats["avg_comprehensive"]
        
        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®è¡¨ç°
        best_group = max(scores, key=scores.get)
        worst_group = min(scores, key=scores.get)
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        improvements = {}
        if "enhanced_rag" in scores:
            base_score = scores["enhanced_rag"]
            for group_key, score in scores.items():
                if group_key != "enhanced_rag":
                    improvement = ((base_score - score) / score) * 100 if score > 0 else 0
                    improvements[group_key] = round(improvement, 1)
        
        analysis = {
            "best_performing_group": {
                "group": best_group,
                "name": summary_stats[best_group]["group_name"],
                "score": scores[best_group]
            },
            "worst_performing_group": {
                "group": worst_group,
                "name": summary_stats[worst_group]["group_name"], 
                "score": scores[worst_group]
            },
            "performance_gap": round(scores[best_group] - scores[worst_group], 2),
            "improvements_over_baselines": improvements,
            "ranking": sorted(scores.items(), key=lambda x: x[1], reverse=True)
        }
        
        return analysis

    def save_results(self, results: Dict, filename: str = None):
        """ä¿å­˜å®éªŒç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rag_ablation_experiment_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {filename}")
        return filename

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ RAGå¢å¼ºæ¨¡å—æ¶ˆèä¸å¯¹æ¯”å®éªŒ")
    print("å®éªŒç›®çš„ï¼šæ¯”è¾ƒæ™ºèƒ½é‡æ’åºRAG vs æ ‡å‡†RAG vs æ— RAGçš„æ€§èƒ½å·®å¼‚")
    print("=" * 80)
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = RAGAblationExperiment()
    
    # è¿è¡Œå®éªŒ
    results = experiment.run_full_experiment()
    
    # ä¿å­˜ç»“æœ
    experiment.save_results(results)
    
    # æ˜¾ç¤ºæ ¸å¿ƒç»“è®º
    print(f"\nğŸ‰ å®éªŒå®Œæˆï¼")
    print("ğŸ“Š æ ¸å¿ƒå‘ç°:")
    
    perf_analysis = results["performance_comparison"]
    if "best_performing_group" in perf_analysis:
        best = perf_analysis["best_performing_group"]
        worst = perf_analysis["worst_performing_group"]
        gap = perf_analysis["performance_gap"]
        
        print(f"  ğŸ† æœ€ä½³æ–¹æ¡ˆ: {best['name']} ({best['score']:.1f}åˆ†)")
        print(f"  ğŸ“‰ æœ€å·®æ–¹æ¡ˆ: {worst['name']} ({worst['score']:.1f}åˆ†)")
        print(f"  ğŸ“ˆ æ€§èƒ½å·®è·: {gap:.1f}åˆ†")
        
        if "improvements_over_baselines" in perf_analysis:
            print("  ğŸ’¡ å¢å¼ºRAGç›¸å¯¹æ”¹è¿›:")
            for group, improvement in perf_analysis["improvements_over_baselines"].items():
                group_name = results["experiment_info"]["experiment_groups"][group]["name"]
                print(f"    vs {group_name}: +{improvement}%")

if __name__ == "__main__":
    main() 