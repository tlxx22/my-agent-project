"""
RAGæ•ˆæœæµ‹è¯•å·¥å…·
ç”¨äºæµ‹è¯•ä»ªè¡¨å®‰è£…è§„èŒƒRAGç³»ç»Ÿçš„æ£€ç´¢æ•ˆæœã€è´¨é‡å’Œæ€§èƒ½
"""
import os
import sys
import time
from typing import List, Dict, Tuple
import logging
from tabulate import tabulate

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from tools.match_standard_clause import StandardClauseRetriever
from tools.build_index import DocumentIndexer

class RAGPerformanceTester:
    """RAGæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.retriever = StandardClauseRetriever()
        self.test_queries = self._prepare_test_queries()
        self.results = {}
        
    def _prepare_test_queries(self) -> List[Dict]:
        """å‡†å¤‡æµ‹è¯•æŸ¥è¯¢é›†åˆ"""
        return [
            # å…·ä½“ä»ªè¡¨ç±»å‹æŸ¥è¯¢
            {
                "query": "çƒ­ç”µå¶å®‰è£…",
                "type": "instrument_specific",
                "expected_keywords": ["çƒ­ç”µå¶", "å®‰è£…", "ä¿æŠ¤ç®¡", "æ¥çº¿"],
                "instrument_type": "çƒ­ç”µå¶"
            },
            {
                "query": "å‹åŠ›è¡¨å®‰è£…è¦æ±‚",
                "type": "instrument_specific", 
                "expected_keywords": ["å‹åŠ›è¡¨", "å®‰è£…", "å–å‹ç‚¹", "é˜€é—¨"],
                "instrument_type": "å‹åŠ›è¡¨"
            },
            {
                "query": "æµé‡è®¡å®‰è£…è§„èŒƒ",
                "type": "instrument_specific",
                "expected_keywords": ["æµé‡è®¡", "å®‰è£…", "ç›´ç®¡æ®µ", "å‰å"],
                "instrument_type": "æµé‡è®¡"
            },
            {
                "query": "æ¶²ä½è®¡å®‰è£…æ–¹æ³•",
                "type": "instrument_specific",
                "expected_keywords": ["æ¶²ä½", "å®‰è£…", "å–æº", "å¯¼å‹ç®¡"],
                "instrument_type": "æ¶²ä½è®¡"
            },
            {
                "query": "è°ƒèŠ‚é˜€å®‰è£…",
                "type": "instrument_specific",
                "expected_keywords": ["è°ƒèŠ‚é˜€", "å®‰è£…", "æ‰§è¡Œæœºæ„", "æ–¹å‘"],
                "instrument_type": "è°ƒèŠ‚é˜€"
            },
            
            # ææ–™å’Œé…ä»¶æŸ¥è¯¢
            {
                "query": "ç®¡è·¯ææ–™è¦æ±‚",
                "type": "materials",
                "expected_keywords": ["ç®¡è·¯", "ææ–™", "ä¸é”ˆé’¢", "ç¢³é’¢"],
                "instrument_type": None
            },
            {
                "query": "é˜€é—¨é€‰æ‹©æ ‡å‡†",
                "type": "materials",
                "expected_keywords": ["é˜€é—¨", "é€‰æ‹©", "çƒé˜€", "æˆªæ­¢é˜€"],
                "instrument_type": None
            },
            {
                "query": "ç”µç¼†å®‰è£…è¦æ±‚",
                "type": "materials",
                "expected_keywords": ["ç”µç¼†", "å®‰è£…", "æ•·è®¾", "å±è”½"],
                "instrument_type": None
            },
            
            # å®‰å…¨å’Œç»´æŠ¤æŸ¥è¯¢
            {
                "query": "ä»ªè¡¨å®‰å…¨è¦æ±‚",
                "type": "safety",
                "expected_keywords": ["å®‰å…¨", "é˜²æŠ¤", "æ¥åœ°", "é˜²çˆ†"],
                "instrument_type": None
            },
            {
                "query": "ç»´æŠ¤ä¿å…»è§„èŒƒ",
                "type": "maintenance",
                "expected_keywords": ["ç»´æŠ¤", "ä¿å…»", "æ£€ä¿®", "æ ¡å‡†"],
                "instrument_type": None
            },
            
            # å¤æ‚ç»„åˆæŸ¥è¯¢
            {
                "query": "çƒ­ç”µå¶ä¿æŠ¤ç®¡ææ–™å’Œå®‰è£…",
                "type": "complex",
                "expected_keywords": ["çƒ­ç”µå¶", "ä¿æŠ¤ç®¡", "ææ–™", "å®‰è£…"],
                "instrument_type": "çƒ­ç”µå¶"
            },
            {
                "query": "å‹åŠ›æµ‹é‡ç‚¹çš„å®‰è£…ä½ç½®è¦æ±‚",
                "type": "complex",
                "expected_keywords": ["å‹åŠ›", "æµ‹é‡ç‚¹", "å®‰è£…", "ä½ç½®"],
                "instrument_type": "å‹åŠ›è¡¨"
            },
            
            # è¾¹ç•Œæ¡ä»¶æµ‹è¯•
            {
                "query": "ä¸å­˜åœ¨çš„ä»ªè¡¨ç±»å‹",
                "type": "boundary",
                "expected_keywords": [],
                "instrument_type": None
            },
            {
                "query": "ä»ªè¡¨",  # è¿‡äºå®½æ³›çš„æŸ¥è¯¢
                "type": "boundary",
                "expected_keywords": ["ä»ªè¡¨"],
                "instrument_type": None
            }
        ]
    
    def test_index_loading(self) -> Dict:
        """æµ‹è¯•ç´¢å¼•åŠ è½½"""
        print("="*50)
        print("æµ‹è¯•ç´¢å¼•åŠ è½½")
        print("="*50)
        
        results = {}
        index_files = [
            "./data/indexes/instrument_standards.index",
            "./data/indexes/instrument_standards_improved.index",
            "./data/indexes/test_standards.index"
        ]
        
        for index_path in index_files:
            start_time = time.time()
            
            if os.path.exists(index_path):
                try:
                    retriever = StandardClauseRetriever(index_path)
                    success = retriever.load_index()
                    load_time = time.time() - start_time
                    
                    if success:
                        doc_count = len(retriever.indexer.documents) if retriever.indexer.documents else 0
                        results[index_path] = {
                            "status": "æˆåŠŸ",
                            "load_time": f"{load_time:.3f}ç§’",
                            "document_count": doc_count
                        }
                        print(f"âœ“ {os.path.basename(index_path)}: åŠ è½½æˆåŠŸï¼Œ{doc_count}ä¸ªæ–‡æ¡£ï¼Œè€—æ—¶{load_time:.3f}ç§’")
                    else:
                        results[index_path] = {
                            "status": "å¤±è´¥",
                            "load_time": f"{load_time:.3f}ç§’",
                            "document_count": 0
                        }
                        print(f"âœ— {os.path.basename(index_path)}: åŠ è½½å¤±è´¥")
                        
                except Exception as e:
                    results[index_path] = {
                        "status": f"é”™è¯¯: {str(e)}",
                        "load_time": "N/A",
                        "document_count": 0
                    }
                    print(f"âœ— {os.path.basename(index_path)}: åŠ è½½å‡ºé”™ - {str(e)}")
            else:
                results[index_path] = {
                    "status": "æ–‡ä»¶ä¸å­˜åœ¨",
                    "load_time": "N/A", 
                    "document_count": 0
                }
                print(f"âœ— {os.path.basename(index_path)}: æ–‡ä»¶ä¸å­˜åœ¨")
        
        return results
    
    def test_basic_retrieval(self) -> Dict:
        """æµ‹è¯•åŸºæœ¬æ£€ç´¢åŠŸèƒ½"""
        print("\n" + "="*50)
        print("æµ‹è¯•åŸºæœ¬æ£€ç´¢åŠŸèƒ½")
        print("="*50)
        
        results = {}
        
        # ç¡®ä¿retrieverå·²åŠ è½½
        if not self.retriever.is_loaded:
            if not self.retriever.load_index():
                print("âŒ æ— æ³•åŠ è½½ç´¢å¼•ï¼Œè·³è¿‡åŸºæœ¬æ£€ç´¢æµ‹è¯•")
                return {"error": "æ— æ³•åŠ è½½ç´¢å¼•"}
        
        for i, test_case in enumerate(self.test_queries[:5], 1):  # åªæµ‹è¯•å‰5ä¸ªæŸ¥è¯¢
            query = test_case["query"]
            expected_keywords = test_case["expected_keywords"]
            
            print(f"\n{i}. æŸ¥è¯¢: '{query}'")
            
            start_time = time.time()
            search_results = self.retriever.search_related_clauses(query, top_k=3)
            search_time = time.time() - start_time
            
            # åˆ†æç»“æœè´¨é‡
            quality_score = self._evaluate_result_quality(search_results, expected_keywords)
            
            results[query] = {
                "result_count": len(search_results),
                "search_time": f"{search_time:.3f}ç§’",
                "quality_score": quality_score,
                "results": search_results[:2]  # åªä¿å­˜å‰2ä¸ªç»“æœç”¨äºå±•ç¤º
            }
            
            print(f"   ç»“æœæ•°é‡: {len(search_results)}")
            print(f"   æœç´¢è€—æ—¶: {search_time:.3f}ç§’")
            print(f"   è´¨é‡è¯„åˆ†: {quality_score:.2f}/1.0")
            
            # æ˜¾ç¤ºå‰2ä¸ªç»“æœ
            for j, result in enumerate(search_results[:2], 1):
                content_preview = result['content'][:100] + "..." if len(result['content']) > 100 else result['content']
                print(f"   {j}. ç›¸ä¼¼åº¦: {result['score']:.3f}")
                print(f"      å†…å®¹: {content_preview}")
        
        return results
    
    def test_instrument_specific_search(self) -> Dict:
        """æµ‹è¯•æŒ‰ä»ªè¡¨ç±»å‹çš„ä¸“é¡¹æœç´¢"""
        print("\n" + "="*50)
        print("æµ‹è¯•ä»ªè¡¨ç±»å‹ä¸“é¡¹æœç´¢")
        print("="*50)
        
        results = {}
        instrument_types = ["çƒ­ç”µå¶", "å‹åŠ›è¡¨", "æµé‡è®¡", "æ¶²ä½è®¡", "è°ƒèŠ‚é˜€"]
        
        if not self.retriever.is_loaded:
            if not self.retriever.load_index():
                print("âŒ æ— æ³•åŠ è½½ç´¢å¼•ï¼Œè·³è¿‡ä»ªè¡¨ä¸“é¡¹æœç´¢æµ‹è¯•")
                return {"error": "æ— æ³•åŠ è½½ç´¢å¼•"}
        
        for instrument_type in instrument_types:
            print(f"\næµ‹è¯•ä»ªè¡¨ç±»å‹: {instrument_type}")
            
            start_time = time.time()
            search_results = self.retriever.search_by_instrument_type(instrument_type, top_k=3)
            search_time = time.time() - start_time
            
            # æ£€æŸ¥ç»“æœçš„ä»ªè¡¨ç±»å‹åŒ¹é…åº¦
            type_match_score = self._evaluate_instrument_type_match(search_results, instrument_type)
            
            results[instrument_type] = {
                "result_count": len(search_results),
                "search_time": f"{search_time:.3f}ç§’",
                "type_match_score": type_match_score,
                "avg_similarity": sum(r['score'] for r in search_results) / len(search_results) if search_results else 0
            }
            
            print(f"   ç»“æœæ•°é‡: {len(search_results)}")
            print(f"   æœç´¢è€—æ—¶: {search_time:.3f}ç§’")
            print(f"   ç±»å‹åŒ¹é…åº¦: {type_match_score:.2f}/1.0")
            print(f"   å¹³å‡ç›¸ä¼¼åº¦: {results[instrument_type]['avg_similarity']:.3f}")
            
            # æ˜¾ç¤ºæœ€ä½³åŒ¹é…ç»“æœ
            if search_results:
                best_result = search_results[0]
                content_preview = best_result['content'][:120] + "..." if len(best_result['content']) > 120 else best_result['content']
                print(f"   æœ€ä½³åŒ¹é…: {content_preview}")
        
        return results
    
    def test_comprehensive_search(self) -> Dict:
        """æµ‹è¯•ç»¼åˆä¿¡æ¯æœç´¢"""
        print("\n" + "="*50)
        print("æµ‹è¯•ç»¼åˆä¿¡æ¯æœç´¢")
        print("="*50)
        
        results = {}
        test_instruments = ["çƒ­ç”µå¶", "å‹åŠ›è¡¨"]
        
        if not self.retriever.is_loaded:
            if not self.retriever.load_index():
                print("âŒ æ— æ³•åŠ è½½ç´¢å¼•ï¼Œè·³è¿‡ç»¼åˆæœç´¢æµ‹è¯•")
                return {"error": "æ— æ³•åŠ è½½ç´¢å¼•"}
        
        for instrument_type in test_instruments:
            print(f"\næµ‹è¯•ç»¼åˆä¿¡æ¯: {instrument_type}")
            
            start_time = time.time()
            comprehensive_info = self.retriever.get_comprehensive_standards(instrument_type)
            search_time = time.time() - start_time
            
            # ç»Ÿè®¡å„ç±»ä¿¡æ¯æ•°é‡
            installation_count = len(comprehensive_info['installation_methods'])
            material_count = len(comprehensive_info['material_requirements'])
            safety_count = len(comprehensive_info['safety_requirements'])
            maintenance_count = len(comprehensive_info['maintenance_requirements'])
            
            results[instrument_type] = {
                "search_time": f"{search_time:.3f}ç§’",
                "installation_methods": installation_count,
                "material_requirements": material_count,
                "safety_requirements": safety_count,
                "maintenance_requirements": maintenance_count,
                "total_items": installation_count + material_count + safety_count + maintenance_count
            }
            
            print(f"   æœç´¢è€—æ—¶: {search_time:.3f}ç§’")
            print(f"   å®‰è£…æ–¹æ³•: {installation_count}æ¡")
            print(f"   ææ–™è¦æ±‚: {material_count}æ¡")
            print(f"   å®‰å…¨è¦æ±‚: {safety_count}æ¡")
            print(f"   ç»´æŠ¤è¦æ±‚: {maintenance_count}æ¡")
            print(f"   æ€»è®¡ä¿¡æ¯: {results[instrument_type]['total_items']}æ¡")
        
        return results
    
    def test_search_quality_analysis(self) -> Dict:
        """æµ‹è¯•æœç´¢è´¨é‡åˆ†æ"""
        print("\n" + "="*50)
        print("æœç´¢è´¨é‡æ·±åº¦åˆ†æ")
        print("="*50)
        
        if not self.retriever.is_loaded:
            if not self.retriever.load_index():
                print("âŒ æ— æ³•åŠ è½½ç´¢å¼•ï¼Œè·³è¿‡è´¨é‡åˆ†ææµ‹è¯•")
                return {"error": "æ— æ³•åŠ è½½ç´¢å¼•"}
        
        quality_metrics = {
            "high_quality_results": 0,
            "medium_quality_results": 0, 
            "low_quality_results": 0,
            "total_queries": len(self.test_queries),
            "avg_response_time": 0,
            "coverage_analysis": {}
        }
        
        total_time = 0
        
        for test_case in self.test_queries:
            query = test_case["query"]
            expected_keywords = test_case["expected_keywords"]
            
            start_time = time.time()
            results = self.retriever.search_related_clauses(query, top_k=5)
            query_time = time.time() - start_time
            total_time += query_time
            
            # è¯„ä¼°æŸ¥è¯¢è´¨é‡
            if results:
                quality_score = self._evaluate_result_quality(results, expected_keywords)
                
                if quality_score >= 0.7:
                    quality_metrics["high_quality_results"] += 1
                elif quality_score >= 0.4:
                    quality_metrics["medium_quality_results"] += 1
                else:
                    quality_metrics["low_quality_results"] += 1
            else:
                quality_metrics["low_quality_results"] += 1
        
        quality_metrics["avg_response_time"] = f"{total_time / len(self.test_queries):.3f}ç§’"
        
        # è®¡ç®—è¦†ç›–ç‡
        coverage = self._analyze_coverage()
        quality_metrics["coverage_analysis"] = coverage
        
        # è¾“å‡ºç»“æœ
        print(f"é«˜è´¨é‡ç»“æœ: {quality_metrics['high_quality_results']}/{quality_metrics['total_queries']}")
        print(f"ä¸­ç­‰è´¨é‡ç»“æœ: {quality_metrics['medium_quality_results']}/{quality_metrics['total_queries']}")
        print(f"ä½è´¨é‡ç»“æœ: {quality_metrics['low_quality_results']}/{quality_metrics['total_queries']}")
        print(f"å¹³å‡å“åº”æ—¶é—´: {quality_metrics['avg_response_time']}")
        print(f"æ–‡æ¡£è¦†ç›–ç‡: {coverage['document_coverage']:.1%}")
        print(f"ä¸»é¢˜è¦†ç›–æƒ…å†µ: {coverage['topic_coverage']}")
        
        return quality_metrics
    
    def test_performance_benchmark(self) -> Dict:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\n" + "="*50)
        print("æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*50)
        
        if not self.retriever.is_loaded:
            if not self.retriever.load_index():
                print("âŒ æ— æ³•åŠ è½½ç´¢å¼•ï¼Œè·³è¿‡æ€§èƒ½åŸºå‡†æµ‹è¯•")
                return {"error": "æ— æ³•åŠ è½½ç´¢å¼•"}
        
        # æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆçš„æ€§èƒ½
        test_configs = [
            {"top_k": 1, "min_similarity": 0.5},
            {"top_k": 3, "min_similarity": 0.5},
            {"top_k": 5, "min_similarity": 0.5},
            {"top_k": 10, "min_similarity": 0.3},
            {"top_k": 5, "min_similarity": 0.7}
        ]
        
        benchmark_results = {}
        test_query = "çƒ­ç”µå¶å®‰è£…è§„èŒƒ"
        
        for config in test_configs:
            config_name = f"top_k={config['top_k']}, min_sim={config['min_similarity']}"
            print(f"\næµ‹è¯•é…ç½®: {config_name}")
            
            # å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
            times = []
            result_counts = []
            
            for _ in range(5):
                start_time = time.time()
                results = self.retriever.search_related_clauses(
                    test_query, 
                    top_k=config['top_k'], 
                    min_similarity=config['min_similarity']
                )
                times.append(time.time() - start_time)
                result_counts.append(len(results))
            
            avg_time = sum(times) / len(times)
            avg_results = sum(result_counts) / len(result_counts)
            
            benchmark_results[config_name] = {
                "avg_time": f"{avg_time:.4f}ç§’",
                "avg_results": f"{avg_results:.1f}æ¡",
                "min_time": f"{min(times):.4f}ç§’",
                "max_time": f"{max(times):.4f}ç§’"
            }
            
            print(f"   å¹³å‡è€—æ—¶: {avg_time:.4f}ç§’")
            print(f"   å¹³å‡ç»“æœæ•°: {avg_results:.1f}æ¡")
            print(f"   æ—¶é—´èŒƒå›´: {min(times):.4f}ç§’ - {max(times):.4f}ç§’")
        
        return benchmark_results
    
    def _evaluate_result_quality(self, results: List[Dict], expected_keywords: List[str]) -> float:
        """è¯„ä¼°æœç´¢ç»“æœè´¨é‡"""
        if not results or not expected_keywords:
            return 0.0
        
        total_score = 0
        for result in results:
            content = result['content'].lower()
            similarity_score = result['score']
            
            # å…³é”®è¯åŒ¹é…å¾—åˆ†
            keyword_matches = sum(1 for keyword in expected_keywords if keyword.lower() in content)
            keyword_score = keyword_matches / len(expected_keywords) if expected_keywords else 0
            
            # å†…å®¹é•¿åº¦å¾—åˆ†ï¼ˆé¿å…è¿‡çŸ­çš„ç»“æœï¼‰
            length_score = min(len(result['content']) / 100, 1.0)
            
            # ç»¼åˆå¾—åˆ†
            result_score = (similarity_score * 0.4 + keyword_score * 0.4 + length_score * 0.2)
            total_score += result_score
        
        return total_score / len(results)
    
    def _evaluate_instrument_type_match(self, results: List[Dict], instrument_type: str) -> float:
        """è¯„ä¼°ç»“æœä¸ä»ªè¡¨ç±»å‹çš„åŒ¹é…åº¦"""
        if not results:
            return 0.0
        
        matches = 0
        for result in results:
            content = result['content'].lower()
            if instrument_type.lower() in content:
                matches += 1
        
        return matches / len(results)
    
    def _analyze_coverage(self) -> Dict:
        """åˆ†ææ–‡æ¡£è¦†ç›–ç‡"""
        coverage_info = {
            "document_coverage": 0.8,  # ç¤ºä¾‹æ•°æ®
            "topic_coverage": {
                "å®‰è£…æ–¹æ³•": "è‰¯å¥½",
                "ææ–™è¦æ±‚": "ä¸­ç­‰", 
                "å®‰å…¨è§„èŒƒ": "ä¸€èˆ¬",
                "ç»´æŠ¤ä¿å…»": "å¾…æ”¹è¿›"
            }
        }
        return coverage_info
    
    def run_full_test_suite(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶"""
        print("ğŸ” RAGæ€§èƒ½æµ‹è¯•å¼€å§‹")
        print("=" * 80)
        
        # å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
        all_results = {}
        
        # 1. ç´¢å¼•åŠ è½½æµ‹è¯•
        all_results["index_loading"] = self.test_index_loading()
        
        # 2. åŸºæœ¬æ£€ç´¢æµ‹è¯•
        all_results["basic_retrieval"] = self.test_basic_retrieval()
        
        # 3. ä»ªè¡¨ä¸“é¡¹æœç´¢æµ‹è¯•
        all_results["instrument_search"] = self.test_instrument_specific_search()
        
        # 4. ç»¼åˆæœç´¢æµ‹è¯•
        all_results["comprehensive_search"] = self.test_comprehensive_search()
        
        # 5. è´¨é‡åˆ†æ
        all_results["quality_analysis"] = self.test_search_quality_analysis()
        
        # 6. æ€§èƒ½åŸºå‡†æµ‹è¯•
        all_results["performance_benchmark"] = self.test_performance_benchmark()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self._generate_test_report(all_results)
        
        return all_results
    
    def _generate_test_report(self, results: Dict):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š RAGç³»ç»Ÿæµ‹è¯•æŠ¥å‘Šæ€»ç»“")
        print("="*80)
        
        # ç´¢å¼•çŠ¶æ€æ±‡æ€»
        if "index_loading" in results and not isinstance(results["index_loading"], dict) or "error" not in results["index_loading"]:
            loaded_indexes = sum(1 for v in results["index_loading"].values() if v.get("status") == "æˆåŠŸ")
            total_indexes = len(results["index_loading"])
            print(f"ğŸ“ ç´¢å¼•åŠ è½½: {loaded_indexes}/{total_indexes} ä¸ªç´¢å¼•æˆåŠŸåŠ è½½")
        
        # æ£€ç´¢æ€§èƒ½æ±‡æ€»
        if "basic_retrieval" in results and "error" not in results["basic_retrieval"]:
            avg_search_time = sum(float(v["search_time"].replace("ç§’", "")) for v in results["basic_retrieval"].values()) / len(results["basic_retrieval"])
            avg_quality = sum(v["quality_score"] for v in results["basic_retrieval"].values()) / len(results["basic_retrieval"])
            print(f"âš¡ æ£€ç´¢æ€§èƒ½: å¹³å‡æœç´¢æ—¶é—´ {avg_search_time:.3f}ç§’ï¼Œå¹³å‡è´¨é‡è¯„åˆ† {avg_quality:.2f}/1.0")
        
        # ä»ªè¡¨ä¸“é¡¹æœç´¢æ±‡æ€»
        if "instrument_search" in results and "error" not in results["instrument_search"]:
            avg_type_match = sum(v["type_match_score"] for v in results["instrument_search"].values()) / len(results["instrument_search"])
            avg_similarity = sum(v["avg_similarity"] for v in results["instrument_search"].values()) / len(results["instrument_search"])
            print(f"ğŸ”§ ä»ªè¡¨æœç´¢: å¹³å‡ç±»å‹åŒ¹é…åº¦ {avg_type_match:.2f}/1.0ï¼Œå¹³å‡ç›¸ä¼¼åº¦ {avg_similarity:.3f}")
        
        # è´¨é‡åˆ†ææ±‡æ€»
        if "quality_analysis" in results and "error" not in results["quality_analysis"]:
            qa = results["quality_analysis"]
            total = qa["total_queries"]
            high_quality_rate = qa["high_quality_results"] / total
            print(f"ğŸ“ˆ è´¨é‡åˆ†æ: {qa['high_quality_results']}/{total} ({high_quality_rate:.1%}) æŸ¥è¯¢è¾¾åˆ°é«˜è´¨é‡")
        
        print("\nğŸ¯ æ€»ä½“è¯„ä¼°:")
        
        # æ ¹æ®æµ‹è¯•ç»“æœç»™å‡ºè¯„ä¼°
        if "error" in str(results):
            print("âŒ ç³»ç»Ÿå­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥ç´¢å¼•æ–‡ä»¶å’Œé…ç½®")
        else:
            print("âœ… RAGç³»ç»ŸåŸºæœ¬åŠŸèƒ½æ­£å¸¸")
            print("ğŸ“ è¯¦ç»†æµ‹è¯•æ•°æ®å·²è®°å½•ï¼Œå¯æ ¹æ®éœ€è¦è¿›è¡Œç³»ç»Ÿä¼˜åŒ–")
        
        print("="*80)


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = RAGPerformanceTester()
        
        # è¿è¡Œå®Œæ•´æµ‹è¯•
        results = tester.run_full_test_suite()
        
        print(f"\nâœ¨ æµ‹è¯•å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°æµ‹è¯•å™¨å¯¹è±¡ä¸­ã€‚")
        
        # å¯é€‰ï¼šå°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
        save_results = input("\næ˜¯å¦å°†æµ‹è¯•ç»“æœä¿å­˜åˆ°æ–‡ä»¶ï¼Ÿ(y/n): ").strip().lower()
        if save_results == 'y':
            import json
            import datetime
            
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rag_test_results_{timestamp}.json"
            
            # è½¬æ¢ç»“æœä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_results = {}
            for key, value in results.items():
                if isinstance(value, dict):
                    serializable_results[key] = {k: str(v) for k, v in value.items()}
                else:
                    serializable_results[key] = str(value)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 