# 错误反思测试套件说明

## 概述

本测试套件包含8个不同的错误场景测试，用于验证智能体的LLM错误反思和决策能力。每个测试都会模拟特定的错误情况，并触发`enhanced_error_handler`进行智能分析和决策。

## 测试文件列表

| 文件名 | 错误场景 | 错误节点 | 预期决策 |
|--------|----------|----------|----------|
| `test_error_reflection_1_file_not_found.py` | 文件不存在错误 | `enter_upload_file` | retry/terminate |
| `test_error_reflection_2_file_format.py` | 文件格式错误 | `enter_upload_file` | terminate |
| `test_error_reflection_3_table_extraction.py` | 表格提取失败 | `extract_excel_tables` | retry/terminate |
| `test_error_reflection_4_data_parsing.py` | 数据解析错误 | `parse_instrument_table` | skip/terminate |
| `test_error_reflection_5_classification.py` | 分类失败错误 | `classify_instrument_type` | retry/skip |
| `test_error_reflection_6_statistics.py` | 统计计算错误 | `summarize_statistics` | retry/skip |
| `test_error_reflection_7_standards_matching.py` | 标准匹配失败 | `match_standard_clause` | skip/retry |
| `test_error_reflection_8_recommendation.py` | 推荐生成错误 | `generate_installation_reco` | retry/skip |

## 使用方法

### 1. 运行单个测试

```bash
# 运行文件不存在错误测试
python test_error_reflection_1_file_not_found.py

# 运行文件格式错误测试
python test_error_reflection_2_file_format.py

# 以此类推...
```

### 2. 批量运行所有测试

```bash
# 运行所有8个测试
python run_all_error_reflection_tests.py
```

## 测试输出说明

每个测试都会输出以下信息：

1. **错误场景描述** - 当前测试的错误类型
2. **错误详情** - 具体的错误信息和状态
3. **LLM反思分析** - AI对错误的理解和分析
4. **决策结果** - retry/skip/terminate 三种决策之一
5. **决策分析** - 对AI决策合理性的评估

### 示例输出

```
🧪 错误反思测试 1: 文件不存在错误
==================================================
📁 测试文件路径: /path/to/nonexistent/file.xlsx
❌ 错误来源节点: enter_upload_file
💬 错误信息: 文件不存在: /path/to/nonexistent/file.xlsx
🔢 当前重试次数: 0

🤖 启动LLM错误反思分析...
------------------------------
💭 AI反思：嗯，看起来用户遇到了文件不存在的错误，我应该分析这是临时问题还是真正的文件缺失...
🎯 AI决策：terminate - 文件确实不存在，无法继续处理
🔄 重试标志: 无
⏭️ 跳过标志: False
❌ 错误状态: True

📝 决策分析:
🛑 LLM决定终止 - 合理，文件不存在无法继续
```

## LLM决策类型

- **retry** - 重试当前节点，适用于临时性错误
- **skip** - 跳过当前步骤，继续后续流程
- **terminate** - 终止整个流程，适用于严重错误

## 环境要求

- Python 3.8+
- 已配置的 OpenAI API 或兼容接口
- 智能体项目的完整环境

## 注意事项

1. 测试会自动创建和清理临时文件
2. 每个测试都是独立的，可以单独运行
3. 测试不会影响实际的智能体运行
4. 建议先运行单个测试确认环境正常

## 故障排除

如果测试失败，请检查：

1. **环境配置** - 确保 conda 环境已激活
2. **API 配置** - 检查 OpenAI API 密钥是否有效
3. **依赖包** - 确保所有必要的包都已安装
4. **路径问题** - 确保在项目根目录运行测试

## 贡献

如需添加新的错误场景测试：

1. 创建新的测试文件，命名格式：`test_error_reflection_N_scenario.py`
2. 实现测试函数，返回智能体状态
3. 在批量运行脚本中添加新测试的映射
4. 更新本说明文档 