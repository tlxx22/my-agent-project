"""
ç»¼åˆè¯„ä»·æŒ‡æ ‡ä½“ç³»
åŒ…å«å†…å®¹è¦†ç›–ç±»ã€å¯è¡Œæ€§å¯æ“ä½œæ€§ç±»ã€è´¨é‡è¯„å®¡ç±»ä¸‰å¤§æŒ‡æ ‡ä½“ç³»
ä¸¥æ ¼æŒ‰ç…§å›¾ç‰‡è¦æ±‚è®¾è®¡
"""

import re
import json
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime
from config.settings import OPENAI_API_KEY, LLM_MODEL

logger = logging.getLogger(__name__)

# ==================== 1. å†…å®¹è¦†ç›–ç±»æŒ‡æ ‡ (Content Coverage Metrics) ====================
# ç›®æ ‡ï¼šè‡ªåŠ¨åˆ¤æ–­ç”Ÿæˆæ–‡æ¡£æ˜¯å¦"æŠŠè¯¥è¯´çš„éƒ½è¯´åˆ°äº†"

class ContentCoverageMetrics:
    """å†…å®¹è¦†ç›–ç±»æŒ‡æ ‡ - è‡ªåŠ¨åˆ¤æ–­ç”Ÿæˆæ–‡æ¡£æ˜¯å¦"æŠŠè¯¥è¯´çš„éƒ½è¯´åˆ°äº†" """
    
    def __init__(self):
        # 3-1 è°ƒæ•´è¯è¡¨ & æ¬ ç¼ºæƒé‡ - åŒºåˆ†æ ¸å¿ƒè¯ä¸å¯é€‰è¯
        self.core_words = {
            "å®‰è£…è¦ç´ ": ["æ­¥éª¤", "ä½ç½®", "å·¥å…·", "äººå‘˜"],  # æ ¸å¿ƒè¯Ã—5åˆ†
            "ææ–™è¦ç´ ": ["è§„æ ¼", "æ•°é‡", "å‹å·"], 
            "å®‰å…¨è¦ç´ ": ["é£é™©", "é˜²æŠ¤", "åº”æ€¥"],
            "æŠ€æœ¯è¦ç‚¹": ["æ¸©åº¦", "å‹åŠ›", "ç²¾åº¦", "è¿æ¥"]  # æ”¹ä¸ºæ›´é€‚åˆå®‰è£…æ–‡æ¡£çš„æŠ€æœ¯è¦ç‚¹
        }
        
        self.optional_words = {
            "å®‰è£…è¦ç´ ": ["é«˜åº¦", "æ³¨æ„", "å·¥æœŸ", "èºä¸åˆ€", "æ“ä½œ", "æµç¨‹"],  # å¯é€‰è¯Ã—2åˆ†
            "ææ–™è¦ç´ ": ["DN", "PN", "å“ç‰Œ", "æ›¿ä»£", "è§„èŒƒ", "æ ‡å‡†"],
            "å®‰å…¨è¦ç´ ": ["å±é™©", "éšæ‚£", "è”é”", "æŠ¥è­¦", "æªæ–½", "é¢„æ¡ˆ"], 
            "æŠ€æœ¯è¦ç‚¹": ["èŒƒå›´", "ç­‰çº§", "æ·±åº¦", "è·ç¦»", "æµ‹é‡", "æ ¡å‡†", "éªŒæ”¶", "æµ‹è¯•"]  # æ›´é€‚åˆæŠ€æœ¯æ–‡æ¡£
        }
        
        # STEP 1: åˆ†é—¨åˆ«ç±»ç»´æŠ¤ï¼Œæ”¯æŒæ­£åˆ™ï¼Œæ§åˆ¶åœ¨100-150æ¡åˆç†èŒƒå›´
        # Gç±»ä¼˜åŒ–ï¼šå¢åŠ ç« èŠ‚å‘½ä¸­æ¨¡å¼ï¼Œæ›´è´´åˆå®é™…æ–‡æ¡£ç»“æ„
        self.alias_map = {
            "è§„æ ¼": ["è§„æ ¼", "å°ºå¯¸", r"\bSize\b", r"\bDN\d+", r"\bPN\b", "ç›´å¾„", "å£å¾„", 
                   r"\b(Ï†|Î¦)\d+mm", "æŠ€æœ¯å‚æ•°", "å‚æ•°"],  # Bç±»ä¼˜åŒ–ï¼šç§»é™¤"æè´¨"é¿å…è¯¯åˆ¤
            "æ•°é‡": ["æ•°é‡", r"åˆè®¡\d+å°", r"å…±è®¡\d+", r"å„\d+ä»¶", "ä¸ªæ•°", "æ”¯", "å¥—", "å°", "ä¸ª", "åª", "æ ¹"],
            "å‹å·": ["å‹å·", "ç±»å‹", "è§„æ ¼å‹å·", "äº§å“å‹å·", "å‹", "æ¬¾", "Kå‹", "PT100", "WRN", "model"],
            "å“ç‰Œ": ["å“ç‰Œ", "å‚å®¶", "åˆ¶é€ å•†", "äº§åœ°", "ä¾›åº”å•†", "ç”Ÿäº§å•†"],
            "æè´¨": ["æè´¨", "Material", "SS304", "SS316", "ç¢³é’¢", "ä¸é”ˆé’¢", "304ä¸é”ˆé’¢", "316ä¸é”ˆé’¢", 
                   "NBR", "PTFE", "çŸ³å¢¨", "åˆé‡‘é’¢"],
            
            # Gç±»ä¼˜åŒ–ï¼šæ­¥éª¤ç« èŠ‚æ¨¡å¼åŒ¹é…
            "æ­¥éª¤": ["æ­¥éª¤", "æµç¨‹", "æ–¹å¼", "è¿‡ç¨‹", "ç¨‹åº", "å·¥åº", 
                   r"å®‰è£….{0,4}(æ–¹å¼|æ­¥éª¤|æµç¨‹)", r"æ“ä½œ.{0,4}æ­¥éª¤", r"æ–½å·¥.{0,4}(æµç¨‹|æ–¹æ³•)"],
            "ä½ç½®": ["ä½ç½®", "åœ°æ–¹", "éƒ¨ä½", "åœºæ‰€", "å®‰è£…ä½ç½®", "é€‰æ‹©", "å¸ƒç½®",
                   r"(å®‰è£…|é€‰æ‹©).{0,4}ä½ç½®", r"ä½ç½®.{0,4}è¦æ±‚"],
            "å·¥å…·": ["å·¥å…·", "è®¾å¤‡", "å™¨å…·", "ä¸“ç”¨å·¥å…·", "ä»ªå™¨", "æ‰³æ‰‹", "èºä¸åˆ€", "æµ‹é‡", "æ ¡å‡†"],
            "äººå‘˜": ["äººå‘˜", "æ“ä½œäººå‘˜", "æŠ€å·¥", "å·¥ä½œäººå‘˜", "æ–½å·¥äººå‘˜", "å®‰å…¨"],
            
            # Gç±»ä¼˜åŒ–ï¼šå®‰å…¨ç« èŠ‚æ¨¡å¼åŒ¹é…
            "é£é™©": ["é£é™©", "éšæ‚£", "å±é™©æ€§", "å®‰å…¨é£é™©", "å±é™©", r"æ½œåœ¨.{0,4}å±é™©", r"æ¼å¼±.{0,4}(ç¯èŠ‚|ç‚¹)",
                   r"å®‰å…¨.{0,4}(é£é™©|éšæ‚£)", r"(å±é™©|é£é™©).{0,4}(è¯†åˆ«|åˆ†æ)"],
            "é˜²æŠ¤": ["é˜²æŠ¤", "ä¿æŠ¤", "é˜²æŠ¤æªæ–½", "å®‰å…¨é˜²æŠ¤", "ä½©æˆ´", "å®‰å…¨å¸½", "æ‰‹å¥—",
                   r"å®‰å…¨.{0,4}(é˜²æŠ¤|æªæ–½)", r"é˜²æŠ¤.{0,4}(è¦æ±‚|æªæ–½)", r"ä¸ªäºº.{0,4}é˜²æŠ¤"],
            "åº”æ€¥": ["åº”æ€¥", "ç´§æ€¥", "åº”æ€¥å¤„ç†", "æ€¥æ•‘", "é¢„æ¡ˆ", "æªæ–½",
                   r"åº”æ€¥.{0,4}(é¢„æ¡ˆ|å¤„ç†|æªæ–½)", r"ç´§æ€¥.{0,4}æƒ…å†µ"],
            
            "æ¸©åº¦": ["æ¸©åº¦", "æ¸©", "çƒ­", "å†·", "â„ƒ", "åº¦", "é«˜æ¸©", "ä½æ¸©"],
            "å‹åŠ›": ["å‹åŠ›", "å‹å¼º", "MPa", "bar", "kPa", "PN", "å‹åŠ›è¡¨"],
            "ç²¾åº¦": ["ç²¾åº¦", "ç²¾ç¡®", "è¯¯å·®", "å‡†ç¡®", "%", "ç­‰çº§", "çº§åˆ«"],
            "è¿æ¥": ["è¿æ¥", "æ¥å¤´", "æ¥å£", "èºçº¹", "æ³•å…°", "èºæ “", "èºæ¯"],
            
            "æŒ¯åŠ¨": ["æŒ¯åŠ¨", "éœ‡åŠ¨", "Vibration", "æŠ–åŠ¨", "æ³¢åŠ¨", "æ‘†åŠ¨"]
        }
        
        # STEP 2: è¯­ä¹‰ç›¸ä¼¼åº¦å…œåº•æœºåˆ¶çš„æŸ¥è¯¢ç¤ºä¾‹
        self.semantic_probes = {
            "è§„æ ¼": "å†™æ˜äº†è®¾å¤‡å°ºå¯¸æˆ–å£å¾„",
            "æ•°é‡": "åˆ—å‡ºäº†æ•°é‡å¤šå°‘", 
            "å‹å·": "ç»™å‡ºäº†åˆ¶é€ å•†ä¿¡æ¯",
            "å“ç‰Œ": "æåˆ°äº†å“ç‰Œå‚å®¶",
            "æè´¨": "è¯´æ˜äº†ææ–™æè´¨",
            
            "æ­¥éª¤": "æè¿°äº†å®‰è£…æµç¨‹æ­¥éª¤",
            "ä½ç½®": "æŒ‡æ˜äº†å®‰è£…ä½ç½®", 
            "å·¥å…·": "åˆ—å‡ºäº†æ‰€éœ€å·¥å…·è®¾å¤‡",
            "äººå‘˜": "æåŠäº†æ“ä½œäººå‘˜è¦æ±‚",
            
            "é£é™©": "è¯†åˆ«äº†å®‰å…¨é£é™©éšæ‚£",
            "é˜²æŠ¤": "è¯´æ˜äº†é˜²æŠ¤æªæ–½",
            "åº”æ€¥": "åˆ¶å®šäº†åº”æ€¥é¢„æ¡ˆ",
            
            "æ¸©åº¦": "æ¶‰åŠæ¸©åº¦å‚æ•°è¦æ±‚",
            "å‹åŠ›": "æåˆ°å‹åŠ›ç­‰çº§è§„æ ¼", 
            "ç²¾åº¦": "è¯´æ˜äº†ç²¾åº¦ç­‰çº§",
            "è¿æ¥": "æè¿°äº†è¿æ¥æ–¹å¼",
            
            "æŒ¯åŠ¨": "è€ƒè™‘äº†æŒ¯åŠ¨å› ç´ "
        }
        
    def keyword_hit(self, word, text):
        """å¢å¼ºå…³é”®è¯åŒ¹é… - æ”¯æŒåˆ«åå’Œæ­£åˆ™"""
        import re
        
        # ç›´æ¥åŒ¹é…
        if word in text:
            return True
            
        # åˆ«ååŒ¹é…ï¼ˆæ”¯æŒæ­£åˆ™ï¼‰
        for alias in self.alias_map.get(word, []):
            if alias.startswith(r'\b') or '\\' in alias or '?' in alias or '+' in alias:
                # Eç±»ä¼˜åŒ–ï¼šæ­£åˆ™æ¨¡å¼ç»Ÿä¸€æ·»åŠ re.IGNORECASEï¼Œè§£å†³å›½é™…å•ä½å¤§å°å†™æ··ç”¨é—®é¢˜
                try:
                    if re.search(alias, text, re.IGNORECASE):
                        return True
                except:
                    pass  # æ­£åˆ™é”™è¯¯åˆ™è·³è¿‡
            else:
                # æ™®é€šå­—ç¬¦ä¸²åŒ¹é…
                if alias in text:
                    return True
        return False
        
    def semantic_hit(self, category: str, text: str, thresh=0.55):
        """è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é… - ä½œä¸ºå…³é”®è¯åŒ¹é…çš„å…œåº•æœºåˆ¶"""
        try:
            # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
            from sentence_transformers import SentenceTransformer, util
            
            # ä½¿ç”¨ç±»å±æ€§ç¼“å­˜æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½
            if not hasattr(self, '_semantic_model'):
                self._semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            
            # Aç±»ä¼˜åŒ–ï¼šç¼“å­˜æ–‡æ¡£å‘é‡ï¼Œå¹³å‡çœ80% CPU/GPUæ—¶å»¶
            if not hasattr(self, '_doc_emb_cache'):
                self._doc_emb_cache = {}
            
            if category not in self.semantic_probes:
                return False
                
            probe_text = self.semantic_probes[category]
            
            # ç¼“å­˜æ–‡æ¡£å‘é‡ï¼šå¦‚æœæ–‡æ¡£å¾ˆé•¿ï¼Œåªä½¿ç”¨å‰500å­—ç¬¦ä½œä¸ºç¼“å­˜key
            doc_key = text[:500] if len(text) > 500 else text
            if doc_key not in self._doc_emb_cache:
                self._doc_emb_cache[doc_key] = self._semantic_model.encode(text)
            
            emb_doc = self._doc_emb_cache[doc_key]
            emb_probe = self._semantic_model.encode(probe_text)
            similarity = util.cos_sim(emb_doc, emb_probe).item()
            
            return similarity > thresh
            
        except Exception as e:
            # å¦‚æœè¯­ä¹‰åŒ¹é…å¤±è´¥ï¼Œé™é»˜è¿”å›Falseï¼Œä¸å½±å“å…¶ä»–åŠŸèƒ½
            return False
        
    def calc_coverage(self, text: str) -> Tuple[float, dict]:
        """
        STEP 2: ä¸‰å±‚åˆ¤å®šæœºåˆ¶ + æŸ”æ€§è¯„åˆ†
        L1: å…³é”®è¯ç›´æ¥åŒ¹é… (100%æƒé‡)
        L2: åˆ«å/æ­£åˆ™åŒ¹é… (100%æƒé‡) 
        L3: è¯­ä¹‰ç›¸ä¼¼åº¦å…œåº• (50%æƒé‡ï¼Œé¿å…è¯¯åˆ¤)
        è¯„åˆ†æœºåˆ¶ï¼š100 - len(missing) * 10ï¼Œæ›´æŸ”æ€§
        """
        details, total_score = {}, 0
        
        for cat in self.core_words.keys():
            # æ ¸å¿ƒè¯ä¸‰å±‚åˆ¤å®š
            core_hits = 0
            core_semantic_hits = 0
            for word in self.core_words[cat]:
                if self.keyword_hit(word, text):
                    core_hits += 1  # L1/L2å‘½ä¸­ï¼Œ100%æƒé‡
                elif self.semantic_hit(word, text):
                    core_semantic_hits += 1  # L3è¯­ä¹‰å‘½ä¸­ï¼Œ50%æƒé‡
            
            # å¯é€‰è¯ä¸‰å±‚åˆ¤å®š
            optional_hits = 0
            optional_semantic_hits = 0
            for word in self.optional_words[cat]:
                if self.keyword_hit(word, text):
                    optional_hits += 1
                elif self.semantic_hit(word, text):
                    optional_semantic_hits += 1
            
            # STEP 2: æŸ”æ€§è¯„åˆ† - å†™å¾—è¶Šå¤šåˆ†è¶Šé«˜ï¼Œæ¼2-3ä¸ªä¹Ÿä¸è‡³äºè…°æ–©
            # æ ¸å¿ƒè¯ï¼šæ¯ç¼ºå¤±ä¸€ä¸ªæ‰£10åˆ†ï¼Œè¯­ä¹‰å‘½ä¸­æŒ‰50%è®¡ç®—
            core_total = len(self.core_words[cat])
            core_effective = core_hits + core_semantic_hits * 0.5
            core_missing = max(0, core_total - core_effective)
            core_score = max(0, 100 - core_missing * 10)
            
            # å¯é€‰è¯ï¼šæ¯ç¼ºå¤±ä¸€ä¸ªæ‰£5åˆ†
            optional_total = len(self.optional_words[cat])
            optional_effective = optional_hits + optional_semantic_hits * 0.5
            optional_missing = max(0, optional_total - optional_effective)  
            optional_score = max(0, 100 - optional_missing * 5)
            
            # Fç±»ä¼˜åŒ–ï¼šæé«˜ææ–™bonusåŠ åˆ† - ä»+3æå‡åˆ°+5ï¼Œæ¿€åŠ±ä¸“ä¸šææ–™è¡¨è¾¾
            bonus_score = 0
            if cat == "ææ–™è¦ç´ ":
                professional_materials = ["304ä¸é”ˆé’¢", "316ä¸é”ˆé’¢", "ç¢³é’¢", "åˆé‡‘é’¢", "NBR", "PTFE", "çŸ³å¢¨"]
                if any(material in text for material in professional_materials):
                    bonus_score += 5  # Fç±»ä¼˜åŒ–ï¼šä»+3æå‡åˆ°+5
                
                standards = ["GB/T", "HG/T", "JB/T", "API", "ASME", "DIN", "ISO"]
                if any(std in text for std in standards):
                    bonus_score += 5  # Fç±»ä¼˜åŒ–ï¼šä»+3æå‡åˆ°+5
            
            # ç±»åˆ«æ€»åˆ†ï¼šæ ¸å¿ƒè¯æƒé‡70%ï¼Œå¯é€‰è¯æƒé‡30%
            category_score = min(100, core_score * 0.7 + optional_score * 0.3 + bonus_score)
            
            details[cat] = {
                "core_hits": core_hits,
                "core_semantic_hits": core_semantic_hits,
                "optional_hits": optional_hits,
                "optional_semantic_hits": optional_semantic_hits,
                "bonus_score": bonus_score,
                "score": category_score,
                "missing_core": [w for w in self.core_words[cat] 
                               if not self.keyword_hit(w, text) and not self.semantic_hit(w, text)],
                "missing_optional": [w for w in self.optional_words[cat] 
                                   if not self.keyword_hit(w, text) and not self.semantic_hit(w, text)]
            }
            total_score += category_score
        
        total_score /= len(self.core_words)
        return total_score, details
    
    def extract_missing_items(self, text: str) -> List[str]:
        """
        Cç±»ä¼˜åŒ–ï¼šç¼ºé¡¹è¾“å‡ºä½¿ç”¨è¯­ä¹‰èƒ½åŠ›ï¼Œä¿æŒä¸calc_coverageä¸€è‡´æ€§
        ç°åœ¨alias/è¯­ä¹‰åŒ¹é…åˆ°ä¹Ÿç®—"ä¸ç¼ºå¤±"
        """
        missing_items = []
        
        # ä¼˜å…ˆæ·»åŠ æ ¸å¿ƒè¯ç¼ºå¤±
        for category, words in self.core_words.items():
            for word in words:
                # Cç±»ä¼˜åŒ–ï¼šä½¿ç”¨å·²æœ‰çš„è¯­ä¹‰åˆ¤å®šèƒ½åŠ›ï¼Œè€Œä¸æ˜¯ç®€å•çš„word in text
                if not self.keyword_hit(word, text) and not self.semantic_hit(word, text):
                    missing_items.append(f"{category}-{word}")
        
        # å†æ·»åŠ éƒ¨åˆ†å¯é€‰è¯ç¼ºå¤±
        for category, words in self.optional_words.items():
            for word in words[:3]:  # åªå–å‰3ä¸ªå¯é€‰è¯é¿å…åˆ—è¡¨è¿‡é•¿
                if not self.keyword_hit(word, text) and not self.semantic_hit(word, text):
                    missing_items.append(f"{category}-{word}")
        
        return missing_items
    
    def evaluate_content_coverage(self, recommendation_text: str) -> Dict[str, Any]:
        """è¯„ä¼°å†…å®¹è¦†ç›–åº¦ - 3-2ä¸‹è°ƒè¦†ç›–å æ¯”ï¼Œå¼•å…¥ç« èŠ‚å‘½ä¸­"""
        
        # ä½¿ç”¨æ–°çš„å·®å¼‚åŒ–æƒé‡è®¡ç®—è¦†ç›–ç‡
        coverage_score, coverage_details = self.calc_coverage(recommendation_text)
        
        # ç¼ºé¡¹è¾“å‡º
        missing_items = self.extract_missing_items(recommendation_text)
        
        # 3-2 ç« èŠ‚å®Œæ•´åº¦æ£€æŸ¥ï¼ˆæé«˜æƒé‡ï¼‰
        required_sections = [
            "å®‰è£…ä½ç½®", "å®‰è£…æ–¹å¼", "ææ–™æ¸…å•", "å®‰å…¨è¦æ±‚", 
            "è´¨é‡æ§åˆ¶", "æŠ€æœ¯å‚æ•°", "å®‰è£…æ­¥éª¤", "æ³¨æ„äº‹é¡¹"
        ]
        
        section_hits = sum(1 for section in required_sections if section in recommendation_text)
        section_score = (section_hits / len(required_sections)) * 100
        
        # 3-2 ç»¼åˆè¯„åˆ†ï¼šé™ä½çº¯è¯é¢‘æƒé‡0.4â†’0.25ï¼Œæé«˜ç« èŠ‚ç»“æ„æƒé‡0.6â†’0.75
        final_coverage_score = coverage_score * 0.25 + section_score * 0.75
        
        return {
            "overall_coverage_score": final_coverage_score,
            "category_coverage": coverage_details,
            "missing_items": missing_items,
            "section_completeness": {
                "score": section_score,
                "hits": section_hits,
                "total": len(required_sections),
                "missing": [s for s in required_sections if s not in recommendation_text]
            },
            "feedback_for_llm": f"å»ºè®®è¡¥å……ä»¥ä¸‹å†…å®¹ï¼š{', '.join(missing_items[:5])}" if missing_items else "å†…å®¹è¦†ç›–è¾ƒä¸ºå®Œæ•´"
        }

# ==================== 2. å¯è¡Œæ€§-å¯æ“ä½œæ€§ç±»æŒ‡æ ‡ (Usability / Operability Metrics) ====================
# ç›®æ ‡ï¼šæ‰“åˆ†  "æ–½å·¥ç°åœºå¥½ä¸å¥½ç”¨"

class UsabilityOperabilityMetrics:
    """å¯è¡Œæ€§-å¯æ“ä½œæ€§ç±»æŒ‡æ ‡ - æ‰“åˆ†ç°åœºå¥½ä¸å¥½ç”¨"""
    
    def __init__(self):
        # åŸºæœ¬æ¡†æ¶ - å·²æœ‰çš„è¯„ä¼°å‡½æ•°
        pass
        
    def evaluate_operability(self, text: str) -> Dict[str, Any]:
        """è¯„ä¼°å¯æ“ä½œæ€§"""
        operability_scores = {}
        
        # æ“ä½œæ­¥éª¤è¯¦ç»†ç¨‹åº¦
        step_indicators = ["æ­¥éª¤", "æµç¨‹", "é¦–å…ˆ", "ç„¶å", "æ¥ä¸‹æ¥", "æœ€å"]
        step_score = min(25, sum(5 for indicator in step_indicators if indicator in text))
        operability_scores["æ“ä½œæ­¥éª¤è¯¦ç»†ç¨‹åº¦"] = step_score
        
        # å·¥å…·éœ€æ±‚æ˜ç¡®æ€§
        tools = ["æ‰³æ‰‹", "èºä¸åˆ€", "åŠè£…", "èµ·é‡", "æµ‹é‡", "æ ¡å‡†", "å·¥å…·"]
        tool_score = min(20, sum(3 for tool in tools if tool in text))
        operability_scores["å·¥å…·éœ€æ±‚æ˜ç¡®æ€§"] = tool_score
        
        # æ—¶é—´ä¼°ç®—
        time_keywords = ["æ—¶é—´", "å·¥æœŸ", "å°æ—¶", "å¤©", "å·¥æ—¥", "å·¥æ—¶"]
        time_score = min(15, sum(5 for keyword in time_keywords if keyword in text))
        operability_scores["æ—¶é—´ä¼°ç®—"] = time_score
        
        # äººå‘˜é…ç½®
        personnel_keywords = ["äººå‘˜", "æŠ€å·¥", "ç”µå·¥", "ç„Šå·¥", "æ“ä½œäººå‘˜", "ä¸“ä¸š"]
        personnel_score = min(20, sum(4 for keyword in personnel_keywords if keyword in text))
        operability_scores["äººå‘˜é…ç½®"] = personnel_score
        
        # è´¨é‡æ£€æŸ¥ç‚¹
        qc_keywords = ["æ£€æŸ¥", "éªŒæ”¶", "æµ‹è¯•", "æ ¡å‡†", "æ£€éªŒ", "ç¡®è®¤"]
        qc_score = min(20, sum(4 for keyword in qc_keywords if keyword in text))
        operability_scores["è´¨é‡æ£€æŸ¥ç‚¹"] = qc_score
        
        total_operability = sum(operability_scores.values())
        
        return {
            "operability_score": total_operability,
            "operability_details": operability_scores
        }
    
    def evaluate_field_applicability(self, text: str) -> Dict[str, Any]:
        """è¯„ä¼°ç°åœºé€‚ç”¨æ€§"""
        field_scores = {}
        
        # ç¯å¢ƒé€‚åº”æ€§
        env_keywords = ["ç¯å¢ƒ", "æ¸©åº¦", "æ¹¿åº¦", "è…èš€", "æŒ¯åŠ¨", "ç°å°˜", "é˜²æŠ¤"]
        env_score = min(30, sum(5 for kw in env_keywords if kw in text))
        field_scores["ç¯å¢ƒé€‚åº”æ€§"] = env_score
        
        # ç©ºé—´è¦æ±‚
        space_keywords = ["ç©ºé—´", "ä½ç½®", "è·ç¦»", "é«˜åº¦", "å®‰è£…ä½ç½®", "å¸ƒç½®"]
        space_score = min(25, sum(5 for kw in space_keywords if kw in text))
        field_scores["ç©ºé—´è¦æ±‚"] = space_score
        
        # æ¥å£å…¼å®¹æ€§
        interface_keywords = ["æ¥å£", "å…¼å®¹", "è¿æ¥", "é…å¥—", "åŒ¹é…", "é€‚é…"]
        interface_score = min(25, sum(5 for kw in interface_keywords if kw in text))
        field_scores["æ¥å£å…¼å®¹æ€§"] = interface_score
        
        # ç»´æŠ¤ä¾¿åˆ©æ€§
        maintenance_keywords = ["ç»´æŠ¤", "æ£€ä¿®", "æ‹†å¸", "æ›´æ¢", "æ¸…æ´", "ä¾¿åˆ©"]
        maintenance_score = min(20, sum(4 for kw in maintenance_keywords if kw in text))
        field_scores["ç»´æŠ¤ä¾¿åˆ©æ€§"] = maintenance_score
        
        total_field = sum(field_scores.values())
        
        return {
            "field_applicability_score": total_field,
            "field_details": field_scores
        }
    
    def simulate_engineer_decision(self, text: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå·¥ç¨‹å¸ˆå†³ç­–"""
        decision_factors = {
            "å¯å®æ–½æ€§": 0,
            "é£é™©å¯æ§æ€§": 0,
            "ç»æµåˆç†æ€§": 0,
            "æŠ€æœ¯æˆç†Ÿåº¦": 0
        }
        
        # å¯å®æ–½æ€§è¯„ä¼°
        impl_keywords = ["æ­¥éª¤", "ææ–™", "å·¥å…·", "äººå‘˜", "å¯è¡Œ"]
        decision_factors["å¯å®æ–½æ€§"] = min(25, sum(5 for kw in impl_keywords if kw in text))
        
        # é£é™©å¯æ§æ€§
        risk_keywords = ["é£é™©", "å®‰å…¨", "é˜²æŠ¤", "æªæ–½", "åº”æ€¥", "é¢„æ¡ˆ"]
        decision_factors["é£é™©å¯æ§æ€§"] = min(25, sum(4 for kw in risk_keywords if kw in text))
        
        # ç»æµåˆç†æ€§
        cost_keywords = ["æˆæœ¬", "è´¹ç”¨", "é¢„ç®—", "ç»æµ", "æŠ•èµ„"]
        decision_factors["ç»æµåˆç†æ€§"] = min(25, sum(5 for kw in cost_keywords if kw in text))
        
        # æŠ€æœ¯æˆç†Ÿåº¦
        tech_keywords = ["æ ‡å‡†", "è§„èŒƒ", "éªŒè¯", "æˆç†Ÿ", "å¯é "]
        decision_factors["æŠ€æœ¯æˆç†Ÿåº¦"] = min(25, sum(5 for kw in tech_keywords if kw in text))
        
        # ç»¼åˆæ¨èåº¦
        overall_recommendation = sum(decision_factors.values())
        
        return {
            "engineer_recommendation_score": overall_recommendation,
            "decision_factors": decision_factors
        }
    
    # 2.2 å¯æ‰©å±•é¡¹ï¼ˆæ— éœ€æ ‡æ³¨ï¼‰
    def check_sequence_consistency(self, text: str) -> int:
        """æ—¶åºä¸€è‡´æ€§ï¼šå¦‚æœæ–‡æ¡£è¾“å‡ºäº†é¡ºåºè¯ï¼Œä¸”indexé€»è¾‘ï¼Œåˆ™+10åˆ†"""
        sequence_words = ["é¦–å…ˆ", "ç„¶å", "æ¥ä¸‹æ¥", "æœ€å", "ç¬¬ä¸€æ­¥", "ç¬¬äºŒæ­¥"]
        sequence_found = sum(1 for word in sequence_words if word in text)
        return 10 if sequence_found >= 2 else 0
    
    def check_tool_step_alignment(self, text: str) -> int:
        """3-3 å·¥å…·-æ­¥éª¤å¯¹åº”ï¼šæ‰©å±•å¸¸è§ç»„åˆï¼Œæ›´æ˜“è§¦å‘"""
        tool_verb_combinations = [
            # åŸæœ‰ç»„åˆ
            ("æ‰³æ‰‹", "æ‹§ç´§"), ("èºä¸åˆ€", "æ‹§ç´§"), ("åŠè£…", "èµ·é‡"),
            ("ç„Šæ¥", "ç„Šæœº"), ("åˆ‡å‰²", "åˆ‡å‰²æœº"), ("æµ‹é‡", "é‡å…·"),
            # 3-3 æ–°å¢å¸¸è§ç»„åˆ
            ("èšæ°¯", "æ‰­çŸ©"), ("æ ¡å‡†", "ä¸‡ç”¨è¡¨"), ("å®šä½", "æµ‹é‡"),
            ("å›ºå®š", "èºæ “"), ("è¿æ¥", "ç®¡é“"), ("å¯†å°", "å«ç‰‡"),
            ("å®‰è£…", "æ”¯æ¶"), ("è°ƒè¯•", "ä»ªè¡¨"), ("æ£€æŸ¥", "è¿æ¥"),
            ("æ¸…æ´", "è¡¨é¢"), ("æ ‡è®°", "ä½ç½®"), ("éªŒæ”¶", "æµ‹è¯•")
        ]
        
        alignment_score = 0
        found_combinations = []
        
        for tool, verb in tool_verb_combinations:
            if tool in text and verb in text:
                alignment_score += 3  # é™ä½å•ä¸ªç»„åˆå¾—åˆ†ï¼Œä½†å¢åŠ ç»„åˆæ•°é‡
                found_combinations.append(f"{tool}-{verb}")
        
        return min(15, alignment_score)
    
    def check_dimension_reasonableness(self, text: str) -> int:
        """STEP 3: å°ºå¯¸åˆç†æ€§æŸ”æ€§è¯„åˆ† - å‘½ä¸­1æ¡ç»™5åˆ†ã€2æ¡ç»™8åˆ†ã€â‰¥3æ¡ç»™æ»¡10"""
        import re
        
        # ç»Ÿè®¡æ‰€æœ‰å‘½ä¸­çš„è§„æ ¼æ¡ç›®
        total_hits = 0
        
        # 1. DNæ·±åº¦æ£€æŸ¥ - æ”¾å®½èŒƒå›´
        m_dn = re.search(r'DN(\d+)', text)
        m_depth = re.search(r'æ’å…¥æ·±åº¦.*?(\d+)mm', text)
        
        if m_dn and m_depth:
            dn = int(m_dn.group(1))
            depth = float(m_depth.group(1))
            # è¿›ä¸€æ­¥æ”¾å®½åˆç†æ€§èŒƒå›´ï¼š0.15-1.0å€
            if 0.15 * dn <= depth <= 1.0 * dn:
                total_hits += 1
        
        # 2. å¸¸è§è§„æ ¼è¯†åˆ«
        common_specs = [
            r'50/100',  # æ¸©åº¦èŒƒå›´
            r'PN\d+',   # å‹åŠ›ç­‰çº§  
            r'DN\d+',   # ç®¡å¾„
            r'4-20mA',  # ä¿¡å·èŒƒå›´
            r'IP\d+',   # é˜²æŠ¤ç­‰çº§
            r'0\.25%',  # ç²¾åº¦ç­‰çº§
            r'M\d+Ã—\d+', # èºæ “è§„æ ¼
            r'Â½-NPT',   # èºçº¹è§„æ ¼
            r'Â¾-NPT',   # èºçº¹è§„æ ¼  
            r'G\d+/\d+', # ç®¡èºçº¹
            r'304ä¸é”ˆé’¢', # æè´¨
            r'316ä¸é”ˆé’¢',
            r'NBRæè´¨',
            r'PTFEæè´¨'
        ]
        
        spec_found = sum(1 for pattern in common_specs if re.search(pattern, text))
        total_hits += min(spec_found, 3)  # æœ€å¤šè®¡ç®—3ä¸ªè§„æ ¼
        
        # 3. æ¸©åº¦å‹åŠ›èŒƒå›´åˆç†æ€§
        temp_match = re.search(r'(-?\d+)â„ƒ.*?(-?\d+)â„ƒ', text)
        if temp_match:
            temp_min, temp_max = int(temp_match.group(1)), int(temp_match.group(2))
            # æ”¾å®½å·¥ä¸šæ¸©åº¦èŒƒå›´
            if -300 <= temp_min < temp_max <= 1500:
                total_hits += 1
        
        # 4. å‹åŠ›èŒƒå›´æ£€æŸ¥
        pressure_patterns = [r'PN(\d+)', r'(\d+)MPa', r'(\d+)bar']
        for pattern in pressure_patterns:
            match = re.search(pattern, text)
            if match:
                pressure_val = int(match.group(1))
                if 0 < pressure_val <= 600:  # åˆç†å‹åŠ›èŒƒå›´
                    total_hits += 1
                    break
        
        # STEP 3: æŸ”æ€§è¯„åˆ†æœºåˆ¶ - é¿å…"ä¸€æ¡ä¸åˆæ ¼å³0"
        if total_hits >= 3:
            return 10  # â‰¥3æ¡ç»™æ»¡10åˆ†
        elif total_hits == 2:
            return 8   # 2æ¡ç»™8åˆ†
        elif total_hits == 1:
            return 5   # 1æ¡ç»™5åˆ†
        else:
            return 0   # 0æ¡ç»™0åˆ†
    
    def calc_usability(self, text: str) -> Dict[str, Any]:
        """æ•´ä½“å¯ç”¨æ€§è®¡ç®— - æƒé‡å¯åç»­ç”¨ç½‘æ ¼æœç´¢/ç»éªŒè°ƒä¼˜"""
        
        # åŸºæœ¬æ¡†æ¶è¯„ä¼°
        operability_result = self.evaluate_operability(text)
        field_result = self.evaluate_field_applicability(text)
        engineer_result = self.simulate_engineer_decision(text)
        
        # å¯æ‰©å±•é¡¹æ£€æµ‹
        sequence_score = self.check_sequence_consistency(text)
        tool_step_score = self.check_tool_step_alignment(text)
        dimension_score = self.check_dimension_reasonableness(text)
        
        # åŸºæœ¬æ¡†æ¶æƒé‡
        base_usability = (
            operability_result["operability_score"] * 0.4 +
            field_result["field_applicability_score"] * 0.4 +
            engineer_result["engineer_recommendation_score"] * 0.2
        )
        
        # åŠ ä¸Šå¯æ‰©å±•é¡¹
        total_usability = base_usability + sequence_score + tool_step_score + dimension_score
        
        return {
            "usability_score": min(100, total_usability),  # ç¡®ä¿ä¸è¶…è¿‡100åˆ†
            "operability": operability_result,
            "field_applicability": field_result,
            "engineer_simulation": engineer_result,
            "advanced_checks": {
                "sequence_consistency": sequence_score,
                "tool_step_alignment": tool_step_score,
                "dimension_reasonableness": dimension_score
            }
        }

# ==================== 3. è´¨é‡è¯„å®¡ç±»æŒ‡æ ‡ (Quality Review Metrics) ====================
# ç›®æ ‡ï¼šä»£æ›¿"ä¸“å®¶æ‰“1-5åˆ†"

class QualityReviewMetrics:
    """è´¨é‡è¯„å®¡ç±»æŒ‡æ ‡ - ä»£æ›¿ä¸“å®¶æ‰“1-5åˆ†"""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or LLM_MODEL
    
    # 3.1 æ²¡æœ‰äººå·¥æ ‡æ³¨æ€ä¹ˆåŠï¼Ÿ
    def llm_as_judge_evaluation(self, content: str) -> Dict[str, Any]:
        """1. LLM-as-Judge (G-Eval/GPTScore) - å‡†å¤‡ä¸€æ®µRubricæç¤º"""
        
        rubric_prompt = f"""
ä½ æ˜¯èµ„æ·±çš„ä»ªè¡¨å®‰è£…å·¥ç¨‹å¸ˆï¼Œè¯·ä»ä¸“ä¸šè§’åº¦è¯„ä¼°ä»¥ä¸‹å®‰è£…æ¨èæ–‡æ¡£çš„è´¨é‡ã€‚

è¯„ä¼°æ–‡æ¡£ï¼š
{content}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¯„åˆ†æ ‡å‡†ï¼ˆ1-5åˆ†ï¼‰è¿›è¡Œè¯„ä¼°ï¼š

**ä¸“ä¸šæ€§ (1-5åˆ†)ï¼š**
- 5åˆ†ï¼šæŠ€æœ¯æœ¯è¯­ä½¿ç”¨å‡†ç¡®ï¼Œæ ‡å‡†è§„èŒƒå¼•ç”¨æ­£ç¡®ï¼Œä½“ç°æ·±åšä¸“ä¸šåŠŸåº•
- 4åˆ†ï¼šæŠ€æœ¯æœ¯è¯­åŸºæœ¬å‡†ç¡®ï¼Œæ ‡å‡†å¼•ç”¨è¾ƒä¸ºè§„èŒƒ
- 3åˆ†ï¼šæŠ€æœ¯è¡¨è¿°åˆç†ï¼Œæœ‰ä¸€å®šä¸“ä¸šæ€§
- 2åˆ†ï¼šæŠ€æœ¯å†…å®¹è¾ƒä¸ºç®€å•ï¼Œä¸“ä¸šæ€§ä¸€èˆ¬
- 1åˆ†ï¼šæŠ€æœ¯é”™è¯¯è¾ƒå¤šï¼Œä¸“ä¸šæ€§ä¸è¶³

**å®Œæ•´æ€§ (1-5åˆ†)ï¼š**
- 5åˆ†ï¼šæ¶µç›–å®‰è£…ä½ç½®ã€æ–¹å¼ã€ææ–™ã€å®‰å…¨ç­‰æ‰€æœ‰å…³é”®è¦ç´ 
- 4åˆ†ï¼šæ¶µç›–å¤§éƒ¨åˆ†å…³é”®è¦ç´ ï¼Œå°‘é‡é—æ¼
- 3åˆ†ï¼šæ¶µç›–ä¸»è¦è¦ç´ ï¼Œæœ‰ä¸€å®šé—æ¼
- 2åˆ†ï¼šè¦ç´ è¦†ç›–ä¸å…¨ï¼Œæœ‰æ˜æ˜¾ç¼ºå¤±
- 1åˆ†ï¼šè¦ç´ ç¼ºå¤±ä¸¥é‡ï¼Œä¸å¤Ÿå®Œæ•´

**å®ç”¨æ€§ (1-5åˆ†)ï¼š**
- 5åˆ†ï¼šç°åœºå·¥ç¨‹å¸ˆå¯ç›´æ¥æŒ‰æ–‡æ¡£æ“ä½œï¼ŒæŒ‡å¯¼æ€§å¼º
- 4åˆ†ï¼šå…·æœ‰è¾ƒå¥½çš„å®ç”¨æŒ‡å¯¼ä»·å€¼
- 3åˆ†ï¼šæœ‰ä¸€å®šå®ç”¨æ€§ï¼Œéœ€è¦è¡¥å……ç»†èŠ‚
- 2åˆ†ï¼šå®ç”¨æ€§ä¸€èˆ¬ï¼Œæ“ä½œæŒ‡å¯¼ä¸å¤Ÿæ˜ç¡®
- 1åˆ†ï¼šå®ç”¨æ€§å·®ï¼Œéš¾ä»¥æŒ‡å¯¼å®é™…æ“ä½œ

**å®‰å…¨æ€§ (1-5åˆ†)ï¼š**
- 5åˆ†ï¼šå®‰å…¨é£é™©è¯†åˆ«å…¨é¢ï¼Œé˜²æŠ¤æªæ–½å…·ä½“æœ‰æ•ˆ
- 4åˆ†ï¼šå®‰å…¨è€ƒè™‘è¾ƒä¸ºå‘¨å…¨ï¼Œé˜²æŠ¤æªæ–½è¾ƒä¸ºå®Œå–„
- 3åˆ†ï¼šæœ‰åŸºæœ¬çš„å®‰å…¨è€ƒè™‘
- 2åˆ†ï¼šå®‰å…¨è€ƒè™‘ä¸å¤Ÿå……åˆ†
- 1åˆ†ï¼šå®‰å…¨è€ƒè™‘ä¸¥é‡ä¸è¶³

è¯·è¾“å‡ºJSONæ ¼å¼ï¼š
{{"ä¸“ä¸šæ€§": 4, "å®Œæ•´æ€§": 3, "å®ç”¨æ€§": 2, "å®‰å…¨æ€§": 5, "ç»¼åˆè¯„åˆ†": 3.5, "è¯„ä»·ç†ç”±": "..."}}
        """
        
        try:
            from openai import OpenAI
            
            if not OPENAI_API_KEY:
                return {"error": "æœªé…ç½®OpenAI API Key"}
            
            client = OpenAI(api_key=OPENAI_API_KEY)
            
            response = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„ä»ªè¡¨å·¥ç¨‹å¸ˆè¯„å®¡ä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†è¿›è¡Œè¯„ä¼°ã€‚"},
                    {"role": "user", "content": rubric_prompt}
                ],
                temperature=0.1  # ä½æ¸©åº¦ä¿è¯è¯„ä»·ç¨³å®šæ€§
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # å°è¯•è§£æJSON
            try:
                # å…ˆæ¸…ç†markdownä»£ç å—æ ¼å¼
                cleaned_text = result_text
                if "```json" in result_text:
                    # æå–```json å’Œ ```ä¹‹é—´çš„å†…å®¹
                    import re
                    json_match = re.search(r'```json\s*\n(.*?)\n```', result_text, re.DOTALL)
                    if json_match:
                        cleaned_text = json_match.group(1).strip()
                elif "```" in result_text:
                    # æå–æ™®é€šä»£ç å—
                    json_match = re.search(r'```\s*\n(.*?)\n```', result_text, re.DOTALL)
                    if json_match:
                        cleaned_text = json_match.group(1).strip()
                
                result_json = json.loads(cleaned_text)
                return {"llm_judge_scores": result_json}
            except:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ›´å¥å£®çš„æ­£åˆ™æå–åˆ†æ•°
                scores = {}
                score_patterns = [
                    (r'"ä¸“ä¸šæ€§":\s*(\d+)', 'ä¸“ä¸šæ€§'),
                    (r'"å®Œæ•´æ€§":\s*(\d+)', 'å®Œæ•´æ€§'), 
                    (r'"å®ç”¨æ€§":\s*(\d+)', 'å®ç”¨æ€§'),
                    (r'"å®‰å…¨æ€§":\s*(\d+)', 'å®‰å…¨æ€§'),
                    (r'"ç»¼åˆè¯„åˆ†":\s*(\d+\.?\d*)', 'ç»¼åˆè¯„åˆ†'),
                    # å¤‡ç”¨æ¨¡å¼
                    (r'ä¸“ä¸šæ€§[ï¼š:]\s*(\d+)', 'ä¸“ä¸šæ€§'),
                    (r'å®Œæ•´æ€§[ï¼š:]\s*(\d+)', 'å®Œæ•´æ€§'),
                    (r'å®ç”¨æ€§[ï¼š:]\s*(\d+)', 'å®ç”¨æ€§'),
                    (r'å®‰å…¨æ€§[ï¼š:]\s*(\d+)', 'å®‰å…¨æ€§'),
                    (r'ç»¼åˆè¯„åˆ†[ï¼š:]\s*(\d+\.?\d*)', 'ç»¼åˆè¯„åˆ†')
                ]
                
                for pattern, key in score_patterns:
                    if key not in scores:  # é¿å…é‡å¤æå–
                        match = re.search(pattern, result_text)
                        if match:
                            scores[key] = float(match.group(1))
                
                return {"llm_judge_scores": scores, "raw_response": result_text}
            
        except Exception as e:
            logger.error(f"LLMè¯„ä¼°å‡ºé”™: {str(e)}")
            return {"error": f"LLMè¯„ä¼°å¤±è´¥: {str(e)}"}
    
    def self_consistency_evaluation(self, content: str, num_samples: int = 3) -> Dict[str, Any]:
        """2. Self-Consistency / Majority Voting - åŒä¸€æ–‡æ¡£é—®å¤šæ¬¡ï¼ˆæˆ–å¤šæ¨¡å‹ï¼‰ï¼Œå–å¹³å‡åˆ†ï¼Œå¯é™ä½LLMéšæœºæ€§"""
        
        evaluations = []
        
        for i in range(num_samples):
            # ç¨å¾®å˜åŒ–æç¤ºè¯ä»¥è·å¾—å¤šæ ·æ€§
            variations = [
                "ä»å·¥ç¨‹å®è·µè§’åº¦",
                "ä»å®‰å…¨ç®¡ç†è§’åº¦", 
                "ä»é¡¹ç›®å®æ–½è§’åº¦"
            ]
            
            perspective = variations[i % len(variations)]
            
            prompt = f"""
è¯·{perspective}è¯„ä¼°ä»¥ä¸‹ä»ªè¡¨å®‰è£…æ¨èæ–‡æ¡£ï¼ˆ1-5åˆ†åˆ¶ï¼‰ï¼š

{content}

è¯·ç»™å‡ºæ€»ä½“è´¨é‡è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰å¹¶è¯´æ˜ç†ç”±ã€‚
            """
            
            try:
                from openai import OpenAI
                client = OpenAI(api_key=OPENAI_API_KEY)
                
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3  # é€‚ä¸­æ¸©åº¦è·å¾—ä¸€å®šéšæœºæ€§
                )
                
                response_text = response.choices[0].message.content.strip()
                
                # æå–åˆ†æ•°
                score_match = re.search(r'(\d+\.?\d*)[åˆ†]?', response_text)
                if score_match:
                    score = float(score_match.group(1))
                    evaluations.append({
                        "score": score,
                        "perspective": perspective,
                        "reasoning": response_text
                    })
                
            except Exception as e:
                logger.error(f"ä¸€è‡´æ€§è¯„ä¼°ç¬¬{i+1}æ¬¡å¤±è´¥: {str(e)}")
        
        if evaluations:
            # è®¡ç®—å¹³å‡åˆ†å’Œä¸€è‡´æ€§
            scores = [eval["score"] for eval in evaluations]
            avg_score = sum(scores) / len(scores)
            consistency = 1.0 - (max(scores) - min(scores)) / 4.0  # ä¸€è‡´æ€§æŒ‡æ ‡
            
            return {
                "self_consistency_score": avg_score,
                "consistency_metric": consistency,
                "individual_evaluations": evaluations,
                "score_variance": max(scores) - min(scores)
            }
        else:
            return {"error": "æ‰€æœ‰ä¸€è‡´æ€§è¯„ä¼°éƒ½å¤±è´¥äº†"}
    
    def pairwise_ranking(self, content_a: str, content_b: str) -> Dict[str, Any]:
        """3. Pairwise Relative Ranking - è‹¥åŒæ—¶ç”Ÿæˆå¤šä»½æ–‡æ¡£ï¼Œè¯·LLMåˆ¤æ–­"A vs Bè°ä»½å¥½ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ" """
        
        prompt = f"""
è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªä»ªè¡¨å®‰è£…æ¨èæ–‡æ¡£çš„è´¨é‡ï¼Œåˆ¤æ–­å“ªä¸ªæ›´å¥½ï¼š

æ–‡æ¡£Aï¼š
{content_a[:500]}...

æ–‡æ¡£Bï¼š
{content_b[:500]}...

è¯·é€‰æ‹©å¹¶è¯´æ˜ç†ç”±ï¼š
- Aæ›´å¥½
- Bæ›´å¥½  
- åŸºæœ¬ç›¸å½“
        """
        
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            
            response = client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            result = response.choices[0].message.content.strip()
            
            # è§£ææ¯”è¾ƒç»“æœ
            if "Aæ›´å¥½" in result or "Aå¥½" in result:
                preference = "A"
            elif "Bæ›´å¥½" in result or "Bå¥½" in result:
                preference = "B"
            elif "ç›¸å½“" in result or "ç›¸ä¼¼" in result:
                preference = "tie"
            else:
                preference = "unclear"
            
            return {
                "preference": preference,
                "reasoning": result
            }
            
        except Exception as e:
            return {"error": f"æˆå¯¹æ¯”è¾ƒå¤±è´¥: {str(e)}"}
    
    def aggregate_quality_scores(self, llm_judge: Dict, self_consistency: Dict) -> Dict[str, Any]:
        """3.3 ç»“æœèåˆä¸ºå•åˆ† - èšåˆå¤šç§è´¨é‡è¯„åˆ†"""
        
        aggregated_scores = {}
        
        # LLM-as-Judgeåˆ†æ•° (è½¬æ¢ä¸º0-100åˆ†åˆ¶)
        if "llm_judge_scores" in llm_judge:
            judge_scores = llm_judge["llm_judge_scores"]
            if "ç»¼åˆè¯„åˆ†" in judge_scores:
                aggregated_scores["llm_judge"] = judge_scores["ç»¼åˆè¯„åˆ†"] * 20  # 5åˆ†åˆ¶è½¬100åˆ†åˆ¶
        
        # Self-Consistencyåˆ†æ•°
        if "self_consistency_score" in self_consistency:
            aggregated_scores["self_consistency"] = self_consistency["self_consistency_score"] * 20
        
        # ä¸€è‡´æ€§æƒé‡è°ƒæ•´
        consistency_weight = self_consistency.get("consistency_metric", 1.0)
        
        # ç»¼åˆè´¨é‡åˆ†æ•°
        if aggregated_scores:
            # åŠ æƒå¹³å‡å…¬å¼ç¤ºä¾‹ï¼šæƒé‡Ã—å¾—åˆ†ç›¸åŠ 
            weights = {"llm_judge": 0.6, "self_consistency": 0.4}
            
            quality_score = 0
            total_weight = 0
            
            for method, score in aggregated_scores.items():
                weight = weights.get(method, 1.0) * consistency_weight
                quality_score += score * weight
                total_weight += weight
            
            if total_weight > 0:
                quality_score /= total_weight
            
            return {
                "overall_quality_score": quality_score,
                "component_scores": aggregated_scores,
                "consistency_weight": consistency_weight,
                "quality_level": self._get_quality_level(quality_score)
            }
        else:
            return {"error": "æ— æœ‰æ•ˆçš„è´¨é‡è¯„åˆ†"}
    
    def _get_quality_level(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è·å–è´¨é‡ç­‰çº§"""
        if score >= 90:
            return "ä¼˜ç§€"
        elif score >= 80:
            return "è‰¯å¥½"  
        elif score >= 70:
            return "ä¸­ç­‰"
        elif score >= 60:
            return "åŠæ ¼"
        else:
            return "ä¸åˆæ ¼"

# ==================== 4. æŠŠä¸‰ç±»æŒ‡æ ‡æ•´åˆåˆ°ç°æœ‰è„šæœ¬ ====================

def integrate_comprehensive_metrics(recommendation_text: str) -> Dict[str, Any]:
    """æ•´åˆä¸‰ç±»æŒ‡æ ‡çš„ç»¼åˆè¯„ä¼°"""
    
    print("ğŸ” å¯åŠ¨ä¸‰ç±»æŒ‡æ ‡ç»¼åˆè¯„ä¼°...")
    
    # 1. å†…å®¹è¦†ç›–ç±»æŒ‡æ ‡
    print("ğŸ“Š è¯„ä¼°å†…å®¹è¦†ç›–ç±»æŒ‡æ ‡...")
    coverage_evaluator = ContentCoverageMetrics()
    coverage_result = coverage_evaluator.evaluate_content_coverage(recommendation_text)
    
    # 2. å¯è¡Œæ€§-å¯æ“ä½œæ€§ç±»æŒ‡æ ‡  
    print("ğŸ”§ è¯„ä¼°å¯è¡Œæ€§-å¯æ“ä½œæ€§ç±»æŒ‡æ ‡...")
    usability_evaluator = UsabilityOperabilityMetrics()
    usability_result = usability_evaluator.calc_usability(recommendation_text)
    
    # 3. è´¨é‡è¯„å®¡ç±»æŒ‡æ ‡
    print("ğŸ‘¨â€ğŸ”¬ è¯„ä¼°è´¨é‡è¯„å®¡ç±»æŒ‡æ ‡...")
    quality_evaluator = QualityReviewMetrics()
    llm_judge_result = quality_evaluator.llm_as_judge_evaluation(recommendation_text)
    consistency_result = quality_evaluator.self_consistency_evaluation(recommendation_text)
    quality_aggregate = quality_evaluator.aggregate_quality_scores(llm_judge_result, consistency_result)
    
    # STEP 2: æŠŠå†…å®¹è¦†ç›–æƒé‡é”å®š30-35%ï¼Œé¿å…å…³é”®è¯æ²¡è¦†ç›–å°±è·Œç©¿åŠæ ¼çº¿
    weights = {
        "coverage": 0.30,     # å†…å®¹è¦†ç›– 30% (ä»35%å†æ¬¡é™ä½ï¼Œå®Œå…¨é¿å…å¡æ­»)
        "usability": 0.50,    # å¯ç”¨æ€§(â‰ˆè½åœ°æ€§) 50% (ä»45%æé«˜ï¼Œçªå‡ºå®ç”¨æ€§)
        "quality": 0.20       # è´¨é‡è¯„å®¡(LLM) 20% (ä¿æŒä¸å˜)
    }
    
    comprehensive_score = (
        coverage_result["overall_coverage_score"] * weights["coverage"] +
        usability_result["usability_score"] * weights["usability"] +
        quality_aggregate.get("overall_quality_score", 70) * weights["quality"]
    )
    
    return {
        "comprehensive_score": comprehensive_score,
        "comprehensive_level": quality_evaluator._get_quality_level(comprehensive_score),
        "content_coverage": coverage_result,
        "usability_operability": usability_result,
        "quality_review": {
            "llm_judge": llm_judge_result,
            "self_consistency": consistency_result,
            "aggregated": quality_aggregate
        },
        "evaluation_weights": weights,
        "evaluation_timestamp": datetime.now().isoformat()
    }

# ==================== 5. ä¸ºä»€ä¹ˆ"ä¸éœ€è¦äººå·¥æ ‡æ³¨"ä¹Ÿè¡Œï¼Ÿ ====================

def why_no_human_annotation_needed():
    """
    è§£é‡Šä¸ºä»€ä¹ˆä¸éœ€è¦äººå·¥æ ‡æ³¨çš„åŸå› ï¼š
    
    1. å†…å®¹è¦†ç›–ç±»ï¼šåŸºäºè§„åˆ™å’Œè¯è¡¨ï¼Œå®¢è§‚å¯éªŒè¯
    2. å¯è¡Œæ€§ç±»ï¼šåŸºäºå·¥ç¨‹å¸¸è¯†å’Œä¸“å®¶è§„åˆ™ï¼Œå¯ç¨‹åºåŒ–
    3. è´¨é‡è¯„å®¡ç±»ï¼šLLM-as-Judgeå·²è¢«è¯æ˜ä¸äººç±»è¯„ä»·é«˜åº¦ç›¸å…³
    
    è¿™æ ·çš„æ— æ ‡æ³¨è¯„ä¼°å¯ä»¥ï¼š
    - å¿«é€Ÿè¿­ä»£å’Œä¼˜åŒ–
    - å¤§è§„æ¨¡è‡ªåŠ¨åŒ–è¯„ä¼°
    - æŒç»­ç›‘æ§ç³»ç»Ÿè´¨é‡
    """
    return {
        "æ— éœ€äººå·¥æ ‡æ³¨çš„ä¼˜åŠ¿": [
            "å¿«é€Ÿè¿­ä»£ï¼šå¯ä»¥å®æ—¶è¯„ä¼°å’Œä¼˜åŒ–",
            "è§„æ¨¡åŒ–ï¼šæ”¯æŒå¤§æ‰¹é‡æ–‡æ¡£è¯„ä¼°",
            "ä¸€è‡´æ€§ï¼šé¿å…äººå·¥è¯„ä¼°çš„ä¸»è§‚æ€§å·®å¼‚",
            "æˆæœ¬æ•ˆç›Šï¼šæ˜¾è‘—é™ä½è¯„ä¼°æˆæœ¬"
        ],
        "è¯„ä¼°å¯é æ€§ä¿è¯": [
            "å¤šç»´åº¦äº¤å‰éªŒè¯",
            "Self-Consistencyé™ä½éšæœºæ€§",
            "åŸºäºä¸“å®¶è§„åˆ™çš„å®¢è§‚è¯„ä¼°",
            "å¯æŒç»­æ”¹è¿›çš„è¯„ä¼°æ ‡å‡†"
        ]
    }

def test_comprehensive_metrics():
    """æµ‹è¯•ç»¼åˆè¯„ä»·æŒ‡æ ‡"""
    
    sample_text = """
    # å‹åŠ›ä»ªè¡¨å®‰è£…æ¨èæ–¹æ¡ˆ
    
    ## å®‰è£…ä½ç½®é€‰æ‹©
    å‹åŠ›ä»ªè¡¨åº”å®‰è£…åœ¨ç›´ç®¡æ®µï¼Œè·ç¦»å¼¯å¤´5å€ç®¡å¾„ï¼Œç¡®ä¿æµ‹é‡å‡†ç¡®æ€§ã€‚
    ç¯å¢ƒæ¸©åº¦åº”åœ¨-40Â°Cè‡³+85Â°CèŒƒå›´å†…ï¼Œæ¹¿åº¦ä¸è¶…è¿‡85%ã€‚
    
    ## å®‰è£…æ–¹å¼ä¸æ­¥éª¤
    1. é¦–å…ˆæ£€æŸ¥å®‰è£…ä½ç½®å’Œå·¥å…·å‡†å¤‡
    2. ç„¶åä½¿ç”¨æ‰³æ‰‹æ‹§ç´§è¿æ¥èºæ “
    3. æ¥ä¸‹æ¥è¿›è¡Œæ°”å¯†æ€§æµ‹è¯•
    4. æœ€åéªŒæ”¶ç¡®è®¤
    
    ## ææ–™æ¸…å•
    - å‹åŠ›è¡¨ï¼šDN50ï¼ŒPN16ï¼Œæè´¨ä¸é”ˆé’¢304
    - èºæ “ï¼šM12Ã—50ï¼Œæ•°é‡8ä¸ª  
    - å«ç‰‡ï¼šçŸ³å¢¨å«ç‰‡ï¼Œè€æ¸©200Â°C
    
    ## å®‰å…¨è¦æ±‚
    å®‰è£…è¿‡ç¨‹ä¸­æ³¨æ„é˜²æŠ¤ï¼Œå­˜åœ¨é«˜å‹é£é™©ï¼Œåº”ä½©æˆ´å®‰å…¨å¸½å’Œæ‰‹å¥—ã€‚
    è®¾ç½®å‹åŠ›æŠ¥è­¦å’Œå®‰å…¨è”é”ç³»ç»Ÿï¼Œé˜²æ­¢è¶…å‹å±é™©ã€‚
    åˆ¶å®šåº”æ€¥é¢„æ¡ˆï¼Œç¡®ä¿äººå‘˜å®‰å…¨ã€‚
    """
    
    print("ğŸ§ª æµ‹è¯•ä¸‰ç±»æŒ‡æ ‡ç»¼åˆè¯„ä¼°...")
    result = integrate_comprehensive_metrics(sample_text)
    
    print("\nğŸ“Š ç»¼åˆè¯„ä»·ç»“æœï¼š")
    print(f"ğŸ¯ ç»¼åˆå¾—åˆ†ï¼š{result['comprehensive_score']:.1f}/100 ({result['comprehensive_level']})")
    print(f"ğŸ“‹ å†…å®¹è¦†ç›–ï¼š{result['content_coverage']['overall_coverage_score']:.1f}/100")
    print(f"ğŸ”§ å¯ç”¨æ€§ï¼š{result['usability_operability']['usability_score']:.1f}/100")
    
    if 'quality_review' in result and 'aggregated' in result['quality_review']:
        quality_score = result['quality_review']['aggregated'].get('overall_quality_score', 0)
        print(f"ğŸ‘¨â€ğŸ”¬ è´¨é‡è¯„å®¡ï¼š{quality_score:.1f}/100")
    
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®ï¼š{result['content_coverage']['feedback_for_llm']}")
    
    return result

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_comprehensive_metrics() 
 